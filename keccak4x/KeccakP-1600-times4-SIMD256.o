# 1 "keccak4x/KeccakP-1600-times4-SIMD256.c"
# 1 "P:\\QCO\\SPMSource//"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "keccak4x/KeccakP-1600-times4-SIMD256.c"
# 16 "keccak4x/KeccakP-1600-times4-SIMD256.c"
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 1 3
# 9 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt_stdio_config.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt_stdio_config.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_mac.h" 1 3
# 98 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_mac.h" 3
             
# 107 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_mac.h" 3
             
# 11 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 2 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_secapi.h" 1 3
# 12 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 2 3
# 283 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 1 3
# 9 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 1 3
# 617 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sdks/_mingw_ddk.h" 1 3
# 618 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 2 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 2 3




#pragma pack(push,_CRT_PACKING)
# 24 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 3
  
# 24 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 3
 typedef __builtin_va_list __gnuc_va_list;






  typedef __gnuc_va_list va_list;
# 103 "C:/msys64/mingw64/x86_64-w64-mingw32/include/vadefs.h" 3
#pragma pack(pop)
# 284 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 2 3
# 580 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw.h" 3
void __attribute__((__cdecl__)) __debugbreak(void);
extern __inline__ __attribute__((__always_inline__,__gnu_inline__)) void __attribute__((__cdecl__)) __debugbreak(void)
{

  __asm__ __volatile__("int {$}3":);







}




const char *__mingw_get_crt_info (void);
# 11 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 2 3




#pragma pack(push,_CRT_PACKING)
# 40 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
__extension__ typedef unsigned long long size_t;
# 50 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
__extension__ typedef long long ssize_t;






typedef size_t rsize_t;
# 67 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
__extension__ typedef long long intptr_t;
# 80 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
__extension__ typedef unsigned long long uintptr_t;
# 93 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
__extension__ typedef long long ptrdiff_t;
# 103 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
typedef unsigned short wchar_t;







typedef unsigned short wint_t;
typedef unsigned short wctype_t;





typedef int errno_t;




typedef long __time32_t;




__extension__ typedef long long __time64_t;
# 143 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
typedef __time64_t time_t;
# 435 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
struct threadlocaleinfostruct;
struct threadmbcinfostruct;
typedef struct threadlocaleinfostruct *pthreadlocinfo;
typedef struct threadmbcinfostruct *pthreadmbcinfo;
struct __lc_time_data;

typedef struct localeinfo_struct {
  pthreadlocinfo locinfo;
  pthreadmbcinfo mbcinfo;
} _locale_tstruct,*_locale_t;



typedef struct tagLC_ID {
  unsigned short wLanguage;
  unsigned short wCountry;
  unsigned short wCodePage;
} LC_ID,*LPLC_ID;




typedef struct threadlocaleinfostruct {





  int refcount;
  unsigned int lc_codepage;
  unsigned int lc_collate_cp;
  unsigned long lc_handle[6];
  LC_ID lc_id[6];
  struct {
    char *locale;
    wchar_t *wlocale;
    int *refcount;
    int *wrefcount;
  } lc_category[6];
  int lc_clike;
  int mb_cur_max;
  int *lconv_intl_refcount;
  int *lconv_num_refcount;
  int *lconv_mon_refcount;
  struct lconv *lconv;
  int *ctype1_refcount;
  unsigned short *ctype1;
  const unsigned short *pctype;
  const unsigned char *pclmap;
  const unsigned char *pcumap;
  struct __lc_time_data *lc_time_curr;

} threadlocinfo;
# 506 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt.h" 3
#pragma pack(pop)
# 11 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt_stdio_config.h" 2 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 2 3

#pragma pack(push,_CRT_PACKING)
# 24 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  struct _iobuf {



    char *_ptr;
    int _cnt;
    char *_base;
    int _flag;
    int _file;
    int _charbuf;
    int _bufsiz;
    char *_tmpfname;

  };
  typedef struct _iobuf FILE;
# 82 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_off_t.h" 1 3




  typedef long _off_t;

  typedef long off32_t;





  __extension__ typedef long long _off64_t;

  __extension__ typedef long long off64_t;
# 26 "C:/msys64/mingw64/x86_64-w64-mingw32/include/_mingw_off_t.h" 3
typedef off32_t off_t;
# 83 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 2 3

__attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) __acrt_iob_func(unsigned index);


  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) __iob_func(void);
# 106 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __extension__ typedef long long fpos_t;
# 147 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
extern
  __attribute__((__format__ (gnu_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_sscanf(const char * __restrict__ _Src,const char * __restrict__ _Format,...);
extern
  __attribute__((__format__ (gnu_scanf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vsscanf (const char * __restrict__ _Str,const char * __restrict__ Format,va_list argp);
extern
  __attribute__((__format__ (gnu_scanf, 1, 2))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_scanf(const char * __restrict__ _Format,...);
extern
  __attribute__((__format__ (gnu_scanf, 1, 0))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_vscanf(const char * __restrict__ Format, va_list argp);
extern
  __attribute__((__format__ (gnu_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_fscanf(FILE * __restrict__ _File,const char * __restrict__ _Format,...);
extern
  __attribute__((__format__ (gnu_scanf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vfscanf (FILE * __restrict__ fp, const char * __restrict__ Format,va_list argp);

extern
  __attribute__((__format__ (gnu_printf, 3, 0))) __attribute__ ((__nonnull__ (3)))
  int __attribute__((__cdecl__)) __mingw_vsnprintf(char * __restrict__ _DstBuf,size_t _MaxCount,const char * __restrict__ _Format,
                               va_list _ArgList);
extern
  __attribute__((__format__ (gnu_printf, 3, 4))) __attribute__ ((__nonnull__ (3)))
  int __attribute__((__cdecl__)) __mingw_snprintf(char * __restrict__ s, size_t n, const char * __restrict__ format, ...);
extern
  __attribute__((__format__ (gnu_printf, 1, 2))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_printf(const char * __restrict__ , ... ) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 1, 0))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_vprintf (const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_fprintf (FILE * __restrict__ , const char * __restrict__ , ...) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vfprintf (FILE * __restrict__ , const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_sprintf (char * __restrict__ , const char * __restrict__ , ...) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vsprintf (char * __restrict__ , const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 3))) __attribute__((nonnull (1,2)))
  int __attribute__((__cdecl__)) __mingw_asprintf(char ** __restrict__ , const char * __restrict__ , ...) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (gnu_printf, 2, 0))) __attribute__((nonnull (1,2)))
  int __attribute__((__cdecl__)) __mingw_vasprintf(char ** __restrict__ , const char * __restrict__ , va_list) __attribute__ ((__nothrow__));

extern
  __attribute__((__format__ (ms_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_sscanf(const char * __restrict__ _Src,const char * __restrict__ _Format,...);
extern
  __attribute__((__format__ (ms_scanf, 1, 2))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_scanf(const char * __restrict__ _Format,...);
extern
  __attribute__((__format__ (ms_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_fscanf(FILE * __restrict__ _File,const char * __restrict__ _Format,...);

extern
  __attribute__((__format__ (ms_printf, 1, 2))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_printf(const char * __restrict__ , ... ) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (ms_printf, 1, 0))) __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_vprintf (const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (ms_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_fprintf (FILE * __restrict__ , const char * __restrict__ , ...) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (ms_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_vfprintf (FILE * __restrict__ , const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (ms_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_sprintf (char * __restrict__ , const char * __restrict__ , ...) __attribute__ ((__nothrow__));
extern
  __attribute__((__format__ (ms_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_vsprintf (char * __restrict__ , const char * __restrict__ , va_list) __attribute__ ((__nothrow__));
# 279 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
int sscanf(const char *__source, const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vsscanf( __source, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 1, 2))) __attribute__ ((__nonnull__ (1)))
int scanf(const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfscanf( (__acrt_iob_func(0)), __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 2, 3))) __attribute__ ((__nonnull__ (2)))
int fscanf(FILE *__stream, const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfscanf( __stream, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}



#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wshadow"


static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 2, 0))) __attribute__ ((__nonnull__ (2)))
int vsscanf (const char *__source, const char *__format, __builtin_va_list __local_argv)
{
  return __mingw_vsscanf( __source, __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 1, 0))) __attribute__ ((__nonnull__ (1)))
int vscanf(const char *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfscanf( (__acrt_iob_func(0)), __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_scanf, 2, 0))) __attribute__ ((__nonnull__ (2)))
int vfscanf (FILE *__stream, const char *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfscanf( __stream, __format, __local_argv );
}


#pragma GCC diagnostic pop





static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
int fprintf (FILE *__stream, const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfprintf( __stream, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 1, 2))) __attribute__ ((__nonnull__ (1)))
int printf (const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfprintf( (__acrt_iob_func(1)), __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}
# 385 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 2, 3))) __attribute__ ((__nonnull__ (2)))
int sprintf (char *__stream, const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vsprintf( __stream, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}



static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
int vfprintf (FILE *__stream, const char *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfprintf( __stream, __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 1, 0))) __attribute__ ((__nonnull__ (1)))
int vprintf (const char *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfprintf( (__acrt_iob_func(1)), __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 2, 0))) __attribute__ ((__nonnull__ (2)))
int vsprintf (char *__stream, const char *__format, __builtin_va_list __local_argv)
{
# 424 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  return __mingw_vsprintf( __stream, __format, __local_argv );
}
# 440 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 3, 4))) __attribute__ ((__nonnull__ (3)))
int snprintf (char *__stream, size_t __n, const char *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vsnprintf( __stream, __n, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}



static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
__attribute__((__format__ (gnu_printf, 3, 0))) __attribute__ ((__nonnull__ (3)))
int vsnprintf (char *__stream, size_t __n, const char *__format, __builtin_va_list __local_argv)
{



  return __mingw_vsnprintf( __stream, __n, __format, __local_argv );
}
# 594 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _filbuf(FILE *_File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _flsbuf(int _Ch,FILE *_File);



  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _fsopen(const char *_Filename,const char *_Mode,int _ShFlag);

  void __attribute__((__cdecl__)) clearerr(FILE *_File);
  int __attribute__((__cdecl__)) fclose(FILE *_File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fcloseall(void);



  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _fdopen(int _FileHandle,const char *_Mode);

  int __attribute__((__cdecl__)) feof(FILE *_File);
  int __attribute__((__cdecl__)) ferror(FILE *_File);
  int __attribute__((__cdecl__)) fflush(FILE *_File);
  int __attribute__((__cdecl__)) fgetc(FILE *_File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fgetchar(void);
  int __attribute__((__cdecl__)) fgetpos(FILE * __restrict__ _File ,fpos_t * __restrict__ _Pos);
  int __attribute__((__cdecl__)) fgetpos64(FILE * __restrict__ _File ,fpos_t * __restrict__ _Pos);
  char *__attribute__((__cdecl__)) fgets(char * __restrict__ _Buf,int _MaxCount,FILE * __restrict__ _File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fileno(FILE *_File);



  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _tempnam(const char *_DirName,const char *_FilePrefix);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _flushall(void);
  FILE *__attribute__((__cdecl__)) fopen(const char * __restrict__ _Filename,const char * __restrict__ _Mode) ;
  FILE *fopen64(const char * __restrict__ filename,const char * __restrict__ mode);
  int __attribute__((__cdecl__)) fputc(int _Ch,FILE *_File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fputchar(int _Ch);
  int __attribute__((__cdecl__)) fputs(const char * __restrict__ _Str,FILE * __restrict__ _File);
  size_t __attribute__((__cdecl__)) fread(void * __restrict__ _DstBuf,size_t _ElementSize,size_t _Count,FILE * __restrict__ _File);
  FILE *__attribute__((__cdecl__)) freopen(const char * __restrict__ _Filename,const char * __restrict__ _Mode,FILE * __restrict__ _File) ;
  int __attribute__((__cdecl__)) fsetpos(FILE *_File,const fpos_t *_Pos);
  int __attribute__((__cdecl__)) fsetpos64(FILE *_File,const fpos_t *_Pos);
  int __attribute__((__cdecl__)) fseek(FILE *_File,long _Offset,int _Origin);
  long __attribute__((__cdecl__)) ftell(FILE *_File);



  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fseeki64(FILE *_File,long long _Offset,int _Origin);
  __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _ftelli64(FILE *_File);
# 653 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  int fseeko64(FILE* stream, _off64_t offset, int whence);
  int fseeko(FILE* stream, _off_t offset, int whence);

  _off_t ftello(FILE * stream);
  _off64_t ftello64(FILE * stream);
# 674 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  size_t __attribute__((__cdecl__)) fwrite(const void * __restrict__ _Str,size_t _Size,size_t _Count,FILE * __restrict__ _File);
  int __attribute__((__cdecl__)) getc(FILE *_File);
  int __attribute__((__cdecl__)) getchar(void);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _getmaxstdio(void);
  char *__attribute__((__cdecl__)) gets(char *_Buffer) ;
  int __attribute__((__cdecl__)) _getw(FILE *_File);


  void __attribute__((__cdecl__)) perror(const char *_ErrMsg);


  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _pclose(FILE *_File);
  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _popen(const char *_Command,const char *_Mode);





  int __attribute__((__cdecl__)) putc(int _Ch,FILE *_File);
  int __attribute__((__cdecl__)) putchar(int _Ch);
  int __attribute__((__cdecl__)) puts(const char *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _putw(int _Word,FILE *_File);


  int __attribute__((__cdecl__)) remove(const char *_Filename);
  int __attribute__((__cdecl__)) rename(const char *_OldFilename,const char *_NewFilename);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _unlink(const char *_Filename);

  int __attribute__((__cdecl__)) unlink(const char *_Filename) ;


  void __attribute__((__cdecl__)) rewind(FILE *_File);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _rmtmp(void);
  void __attribute__((__cdecl__)) setbuf(FILE * __restrict__ _File,char * __restrict__ _Buffer) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _setmaxstdio(int _Max);
  __attribute__ ((__dllimport__)) unsigned int __attribute__((__cdecl__)) _set_output_format(unsigned int _Format);
  __attribute__ ((__dllimport__)) unsigned int __attribute__((__cdecl__)) _get_output_format(void);
  int __attribute__((__cdecl__)) setvbuf(FILE * __restrict__ _File,char * __restrict__ _Buf,int _Mode,size_t _Size);
# 734 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scprintf(const char * __restrict__ _Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snscanf(const char * __restrict__ _Src,size_t _MaxCount,const char * __restrict__ _Format,...) ;

  FILE *__attribute__((__cdecl__)) tmpfile(void) ;
  char *__attribute__((__cdecl__)) tmpnam(char *_Buffer);
  int __attribute__((__cdecl__)) ungetc(int _Ch,FILE *_File);
# 756 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__((__format__ (ms_printf, 3, 4))) __attribute__ ((__nonnull__ (3)))
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf(char * __restrict__ _Dest,size_t _Count,const char * __restrict__ _Format,...) ;
  __attribute__((__format__ (ms_printf, 3, 0))) __attribute__ ((__nonnull__ (3)))
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf(char * __restrict__ _Dest,size_t _Count,const char * __restrict__ _Format,va_list _Args) ;
# 921 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscprintf(const char * __restrict__ _Format,va_list _ArgList);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _set_printf_count_output(int _Value);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _get_printf_count_output(void);




                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_swscanf(const wchar_t * __restrict__ _Src,const wchar_t * __restrict__ _Format,...);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vswscanf (const wchar_t * __restrict__ _Str,const wchar_t * __restrict__ Format,va_list argp);
                                                     __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_wscanf(const wchar_t * __restrict__ _Format,...);
                                                     __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_vwscanf(const wchar_t * __restrict__ Format, va_list argp);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_fwscanf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,...);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vfwscanf (FILE * __restrict__ fp, const wchar_t * __restrict__ Format,va_list argp);

                                                      __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_fwprintf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,...);
                                                      __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_wprintf(const wchar_t * __restrict__ _Format,...);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vfwprintf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,va_list _ArgList);
                                                     __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __mingw_vwprintf(const wchar_t * __restrict__ _Format,va_list _ArgList);
                                                      __attribute__ ((__nonnull__ (3)))
  int __attribute__((__cdecl__)) __mingw_snwprintf (wchar_t * __restrict__ s, size_t n, const wchar_t * __restrict__ format, ...);
                                                      __attribute__ ((__nonnull__ (3)))
  int __attribute__((__cdecl__)) __mingw_vsnwprintf (wchar_t * __restrict__ , size_t, const wchar_t * __restrict__ , va_list);
                                                      __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_swprintf(wchar_t * __restrict__ , const wchar_t * __restrict__ , ...);
                                                      __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __mingw_vswprintf(wchar_t * __restrict__ , const wchar_t * __restrict__ ,va_list);

                                                    __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_swscanf(const wchar_t * __restrict__ _Src,const wchar_t * __restrict__ _Format,...);
                                                    __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_wscanf(const wchar_t * __restrict__ _Format,...);
                                                    __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_fwscanf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,...);

                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_fwprintf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,...);
                                                     __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_wprintf(const wchar_t * __restrict__ _Format,...);
                                                    __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_vfwprintf(FILE * __restrict__ _File,const wchar_t * __restrict__ _Format,va_list _ArgList);
                                                    __attribute__ ((__nonnull__ (1)))
  int __attribute__((__cdecl__)) __ms_vwprintf(const wchar_t * __restrict__ _Format,va_list _ArgList);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_swprintf(wchar_t * __restrict__ , const wchar_t * __restrict__ , ...);
                                                     __attribute__ ((__nonnull__ (2)))
  int __attribute__((__cdecl__)) __ms_vswprintf(wchar_t * __restrict__ , const wchar_t * __restrict__ ,va_list);
# 991 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (2)))
int swscanf(const wchar_t *__source, const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vswscanf( __source, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (1)))
int wscanf(const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfwscanf( (__acrt_iob_func(0)), __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (2)))
int fwscanf(FILE *__stream, const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfwscanf( __stream, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}


static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (2)))
int vswscanf (const wchar_t * __restrict__ __source, const wchar_t * __restrict__ __format, __builtin_va_list __local_argv)
{
  return __mingw_vswscanf( __source, __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (1)))
int vwscanf(const wchar_t *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfwscanf( (__acrt_iob_func(0)), __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                     __attribute__ ((__nonnull__ (2)))
int vfwscanf (FILE *__stream, const wchar_t *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfwscanf( __stream, __format, __local_argv );
}




static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (2)))
int fwprintf (FILE *__stream, const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfwprintf( __stream, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (1)))
int wprintf (const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vfwprintf( (__acrt_iob_func(1)), __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (2)))
int vfwprintf (FILE *__stream, const wchar_t *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfwprintf( __stream, __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (1)))
int vwprintf (const wchar_t *__format, __builtin_va_list __local_argv)
{
  return __mingw_vfwprintf( (__acrt_iob_func(1)), __format, __local_argv );
}
# 1099 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (3)))
int snwprintf (wchar_t *__stream, size_t __n, const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv; __builtin_va_start( __local_argv, __format );
  __retval = __mingw_vsnwprintf( __stream, __n, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}



static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (3)))
int vsnwprintf (wchar_t *__stream, size_t __n, const wchar_t *__format, __builtin_va_list __local_argv)
{



  return __mingw_vsnwprintf( __stream, __n, __format, __local_argv );
}
# 1252 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _wfsopen(const wchar_t *_Filename,const wchar_t *_Mode,int _ShFlag);


  wint_t __attribute__((__cdecl__)) fgetwc(FILE *_File);
  __attribute__ ((__dllimport__)) wint_t __attribute__((__cdecl__)) _fgetwchar(void);
  wint_t __attribute__((__cdecl__)) fputwc(wchar_t _Ch,FILE *_File);
  __attribute__ ((__dllimport__)) wint_t __attribute__((__cdecl__)) _fputwchar(wchar_t _Ch);
  wint_t __attribute__((__cdecl__)) getwc(FILE *_File);
  wint_t __attribute__((__cdecl__)) getwchar(void);
  wint_t __attribute__((__cdecl__)) putwc(wchar_t _Ch,FILE *_File);
  wint_t __attribute__((__cdecl__)) putwchar(wchar_t _Ch);
  wint_t __attribute__((__cdecl__)) ungetwc(wint_t _Ch,FILE *_File);
  wchar_t *__attribute__((__cdecl__)) fgetws(wchar_t * __restrict__ _Dst,int _SizeInWords,FILE * __restrict__ _File);
  int __attribute__((__cdecl__)) fputws(const wchar_t * __restrict__ _Str,FILE * __restrict__ _File);
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _getws(wchar_t *_String) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _putws(const wchar_t *_Str);
# 1334 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scwprintf(const wchar_t * __restrict__ _Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf_c(wchar_t * __restrict__ _DstBuf,size_t _SizeInWords,const wchar_t * __restrict__ _Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vswprintf_c(wchar_t * __restrict__ _DstBuf,size_t _SizeInWords,const wchar_t * __restrict__ _Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwprintf(wchar_t * __restrict__ _Dest,size_t _Count,const wchar_t * __restrict__ _Format,...) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnwprintf(wchar_t * __restrict__ _Dest,size_t _Count,const wchar_t * __restrict__ _Format,va_list _Args) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscwprintf(const wchar_t * __restrict__ _Format,va_list _ArgList);
# 1370 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vswprintf(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Format,va_list _Args);



# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/swprintf.inl" 1 3
# 25 "C:/msys64/mingw64/x86_64-w64-mingw32/include/swprintf.inl" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (3)))
int vswprintf (wchar_t *__stream, size_t __count, const wchar_t *__format, __builtin_va_list __local_argv)
{
  return vsnwprintf( __stream, __count, __format, __local_argv );
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
                                                      __attribute__ ((__nonnull__ (3)))
int swprintf (wchar_t *__stream, size_t __count, const wchar_t *__format, ...)
{
  int __retval;
  __builtin_va_list __local_argv;

  __builtin_va_start( __local_argv, __format );
  __retval = vswprintf( __stream, __count, __format, __local_argv );
  __builtin_va_end( __local_argv );
  return __retval;
}
# 1376 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 2 3
# 1385 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wtempnam(const wchar_t *_Directory,const wchar_t *_FilePrefix);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwscanf(const wchar_t * __restrict__ _Src,size_t _MaxCount,const wchar_t * __restrict__ _Format,...);
  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _wfdopen(int _FileHandle ,const wchar_t *_Mode);
  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _wfopen(const wchar_t * __restrict__ _Filename,const wchar_t *__restrict__ _Mode) ;
  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _wfreopen(const wchar_t * __restrict__ _Filename,const wchar_t * __restrict__ _Mode,FILE * __restrict__ _OldFile) ;



  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _wperror(const wchar_t *_ErrMsg);

  __attribute__ ((__dllimport__)) FILE *__attribute__((__cdecl__)) _wpopen(const wchar_t *_Command,const wchar_t *_Mode);




  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wremove(const wchar_t *_Filename);
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wtmpnam(wchar_t *_Buffer);
# 1445 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _lock_file(FILE *_File);
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _unlock_file(FILE *_File);
# 1463 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  char *__attribute__((__cdecl__)) tempnam(const char *_Directory,const char *_FilePrefix) ;
  int __attribute__((__cdecl__)) fcloseall(void) ;
  FILE *__attribute__((__cdecl__)) fdopen(int _FileHandle,const char *_Format) ;
  int __attribute__((__cdecl__)) fgetchar(void) ;
  int __attribute__((__cdecl__)) fileno(FILE *_File) ;
  int __attribute__((__cdecl__)) flushall(void) ;
  int __attribute__((__cdecl__)) fputchar(int _Ch) ;
  int __attribute__((__cdecl__)) getw(FILE *_File) ;
  int __attribute__((__cdecl__)) putw(int _Ch,FILE *_File) ;
  int __attribute__((__cdecl__)) rmtmp(void) ;
# 1489 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
int __attribute__((__cdecl__)) __mingw_str_wide_utf8 (const wchar_t * const wptr, char **mbptr, size_t * buflen);
# 1503 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
int __attribute__((__cdecl__)) __mingw_str_utf8_wide (const char *const mbptr, wchar_t ** wptr, size_t * buflen);
# 1512 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
void __attribute__((__cdecl__)) __mingw_str_free(void *ptr);






  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnl(int _Mode,const wchar_t *_Filename,const wchar_t *_ArgList,...);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnle(int _Mode,const wchar_t *_Filename,const wchar_t *_ArgList,...);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnlp(int _Mode,const wchar_t *_Filename,const wchar_t *_ArgList,...);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnlpe(int _Mode,const wchar_t *_Filename,const wchar_t *_ArgList,...);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnv(int _Mode,const wchar_t *_Filename,const wchar_t *const *_ArgList);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnve(int _Mode,const wchar_t *_Filename,const wchar_t *const *_ArgList,const wchar_t *const *_Env);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnvp(int _Mode,const wchar_t *_Filename,const wchar_t *const *_ArgList);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _wspawnvpe(int _Mode,const wchar_t *_Filename,const wchar_t *const *_ArgList,const wchar_t *const *_Env);
# 1543 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 3
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _spawnv(int _Mode,const char *_Filename,const char *const *_ArgList);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _spawnve(int _Mode,const char *_Filename,const char *const *_ArgList,const char *const *_Env);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _spawnvp(int _Mode,const char *_Filename,const char *const *_ArgList);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _spawnvpe(int _Mode,const char *_Filename,const char *const *_ArgList,const char *const *_Env);







#pragma pack(pop)

# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 1 3
# 9 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 2 3
# 29 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 3
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) clearerr_s(FILE *_File);

  size_t __attribute__((__cdecl__)) fread_s(void *_DstBuf,size_t _DstSize,size_t _ElementSize,size_t _Count,FILE *_File);
# 494 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 3
  int __attribute__((__cdecl__)) fprintf_s(FILE *_File,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fscanf_s_l(FILE *_File,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) fscanf_s(FILE *_File, const char *_Format, ...);
  int __attribute__((__cdecl__)) printf_s(const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scanf_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scanf_s_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) scanf_s(const char *_Format, ...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf_c(char *_DstBuf,size_t _MaxCount,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf_c(char *_DstBuf,size_t _MaxCount,const char *_Format,va_list _ArgList);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fscanf_l(FILE *_File,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sscanf_l(const char *_Src,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sscanf_s_l(const char *_Src,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) sscanf_s(const char *_Src,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snscanf_s(const char *_Src,size_t _MaxCount,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snscanf_l(const char *_Src,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snscanf_s_l(const char *_Src,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  int __attribute__((__cdecl__)) vfprintf_s(FILE *_File,const char *_Format,va_list _ArgList);
  int __attribute__((__cdecl__)) vprintf_s(const char *_Format,va_list _ArgList);

  int __attribute__((__cdecl__)) vsnprintf_s(char *_DstBuf,size_t _DstSize,size_t _MaxCount,const char *_Format,va_list _ArgList);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf_s(char *_DstBuf,size_t _DstSize,size_t _MaxCount,const char *_Format,va_list _ArgList);

  __attribute__((dllimport)) int __attribute__((__cdecl__)) vsprintf_s(char *_DstBuf,size_t _Size,const char *_Format,va_list _ArgList);

  __attribute__((dllimport)) int __attribute__((__cdecl__)) sprintf_s(char *_DstBuf,size_t _DstSize,const char *_Format,...);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf_s(char *_DstBuf,size_t _DstSize,size_t _MaxCount,const char *_Format,...);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fprintf_p(FILE *_File,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _printf_p(const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sprintf_p(char *_Dst,size_t _MaxCount,const char *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfprintf_p(FILE *_File,const char *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vprintf_p(const char *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsprintf_p(char *_Dst,size_t _MaxCount,const char *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scprintf_p(const char *_Format,...);
  __attribute__((dllimport)) int __attribute__((__cdecl__)) _vscprintf_p(const char *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _printf_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _printf_p_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vprintf_l(const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vprintf_p_l(const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fprintf_l(FILE *_File,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fprintf_p_l(FILE *_File,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfprintf_l(FILE *_File,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfprintf_p_l(FILE *_File,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sprintf_l(char *_DstBuf,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sprintf_p_l(char *_DstBuf,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsprintf_l(char *_DstBuf,const char *_Format,_locale_t,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsprintf_p_l(char *_DstBuf,size_t _MaxCount,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scprintf_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scprintf_p_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscprintf_l(const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscprintf_p_l(const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _printf_s_l(const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vprintf_s_l(const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fprintf_s_l(FILE *_File,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfprintf_s_l(FILE *_File,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _sprintf_s_l(char *_DstBuf,size_t _DstSize,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsprintf_s_l(char *_DstBuf,size_t _DstSize,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf_s_l(char *_DstBuf,size_t _DstSize,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf_s_l(char *_DstBuf,size_t _DstSize,size_t _MaxCount,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf_l(char *_DstBuf,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snprintf_c_l(char *_DstBuf,size_t _MaxCount,const char *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf_l(char *_DstBuf,size_t _MaxCount,const char *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnprintf_c_l(char *_DstBuf,size_t _MaxCount,const char *,_locale_t _Locale,va_list _ArgList);


 
 
 
 
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) fopen_s(FILE **_File,const char *_Filename,const char *_Mode);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) freopen_s(FILE** _File, const char *_Filename, const char *_Mode, FILE *_Stream);

  __attribute__ ((__dllimport__)) char* __attribute__((__cdecl__)) gets_s(char*,rsize_t);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) tmpnam_s(char*,rsize_t);
 




  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _getws_s(wchar_t *_Str,size_t _SizeInWords);
 
# 786 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdio_s.h" 3
  int __attribute__((__cdecl__)) fwprintf_s(FILE *_File,const wchar_t *_Format,...);
  int __attribute__((__cdecl__)) wprintf_s(const wchar_t *_Format,...);
  int __attribute__((__cdecl__)) vfwprintf_s(FILE *_File,const wchar_t *_Format,va_list _ArgList);
  int __attribute__((__cdecl__)) vwprintf_s(const wchar_t *_Format,va_list _ArgList);

  int __attribute__((__cdecl__)) vswprintf_s(wchar_t *_Dst,size_t _SizeInWords,const wchar_t *_Format,va_list _ArgList);

  int __attribute__((__cdecl__)) swprintf_s(wchar_t *_Dst,size_t _SizeInWords,const wchar_t *_Format,...);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnwprintf_s(wchar_t *_DstBuf,size_t _DstSizeInWords,size_t _MaxCount,const wchar_t *_Format,va_list _ArgList);

  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwprintf_s(wchar_t *_DstBuf,size_t _DstSizeInWords,size_t _MaxCount,const wchar_t *_Format,...);


  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wprintf_s_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vwprintf_s_l(const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwprintf_s_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfwprintf_s_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf_s_l(wchar_t *_DstBuf,size_t _DstSize,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vswprintf_s_l(wchar_t *_DstBuf,size_t _DstSize,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwprintf_s_l(wchar_t *_DstBuf,size_t _DstSize,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnwprintf_s_l(wchar_t *_DstBuf,size_t _DstSize,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwscanf_s_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) fwscanf_s(FILE *_File, const wchar_t *_Format, ...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swscanf_s_l(const wchar_t *_Src,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) swscanf_s(const wchar_t *_Src,const wchar_t *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwscanf_s(const wchar_t *_Src,size_t _MaxCount,const wchar_t *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwscanf_s_l(const wchar_t *_Src,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wscanf_s_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) wscanf_s(const wchar_t *_Format, ...);


 
 
 
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wfopen_s(FILE **_File,const wchar_t *_Filename,const wchar_t *_Mode);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wfreopen_s(FILE **_File,const wchar_t *_Filename,const wchar_t *_Mode,FILE *_OldFile);

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wtmpnam_s(wchar_t *_DstBuf,size_t _SizeInWords);
 


  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwprintf_p(FILE *_File,const wchar_t *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wprintf_p(const wchar_t *_Format,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfwprintf_p(FILE *_File,const wchar_t *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vwprintf_p(const wchar_t *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf_p(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,...);
  __attribute__((dllimport)) int __attribute__((__cdecl__)) _vswprintf_p(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scwprintf_p(const wchar_t *_Format,...);
  __attribute__((dllimport)) int __attribute__((__cdecl__)) _vscwprintf_p(const wchar_t *_Format,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wprintf_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wprintf_p_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vwprintf_l(const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vwprintf_p_l(const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwprintf_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwprintf_p_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfwprintf_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vfwprintf_p_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf_c_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swprintf_p_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vswprintf_c_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vswprintf_p_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scwprintf_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _scwprintf_p_l(const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscwprintf_p_l(const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwprintf_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vsnwprintf_l(wchar_t *_DstBuf,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) __swprintf_l(wchar_t *_Dest,const wchar_t *_Format,_locale_t _Plocinfo,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) __vswprintf_l(wchar_t *_Dest,const wchar_t *_Format,_locale_t _Plocinfo,va_list _Args);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _vscwprintf_l(const wchar_t *_Format,_locale_t _Locale,va_list _ArgList);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _fwscanf_l(FILE *_File,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _swscanf_l(const wchar_t *_Src,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _snwscanf_l(const wchar_t *_Src,size_t _MaxCount,const wchar_t *_Format,_locale_t _Locale,...);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wscanf_l(const wchar_t *_Format,_locale_t _Locale,...);
# 1557 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdio.h" 2 3
# 17 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt_wstdlib.h" 1 3
# 15 "C:/msys64/mingw64/x86_64-w64-mingw32/include/corecrt_wstdlib.h" 3
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _itow_s (int _Val,wchar_t *_DstBuf,size_t _SizeInWords,int _Radix);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ltow_s (long _Val,wchar_t *_DstBuf,size_t _SizeInWords,int _Radix);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ultow_s (unsigned long _Val,wchar_t *_DstBuf,size_t _SizeInWords,int _Radix);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wgetenv_s(size_t *_ReturnSize,wchar_t *_DstBuf,size_t _DstSizeInWords,const wchar_t *_VarName);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wdupenv_s(wchar_t **_Buffer,size_t *_BufferSizeInWords,const wchar_t *_VarName);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _i64tow_s(long long _Val,wchar_t *_DstBuf,size_t _SizeInWords,int _Radix);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ui64tow_s(unsigned long long _Val,wchar_t *_DstBuf,size_t _SizeInWords,int _Radix);

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wmakepath_s(wchar_t *_PathResult,size_t _SizeInWords,const wchar_t *_Drive,const wchar_t *_Dir,const wchar_t *_Filename,const wchar_t *_Ext);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wputenv_s(const wchar_t *_Name,const wchar_t *_Value);

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wsearchenv_s(const wchar_t *_Filename,const wchar_t *_EnvVar,wchar_t *_ResultPath,size_t _SizeInWords);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wsplitpath_s(const wchar_t *_FullPath,wchar_t *_Drive,size_t _DriveSizeInWords,wchar_t *_Dir,size_t _DirSizeInWords,wchar_t *_Filename,size_t _FilenameSizeInWords,wchar_t *_Ext,size_t _ExtSizeInWords);
 
# 11 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 2 3
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 1 3 4
# 34 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/syslimits.h" 1 3 4






# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 1 3 4
# 195 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 3 4
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/limits.h" 1 3 4





# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/crtdefs.h" 1 3 4
# 7 "C:/msys64/mingw64/x86_64-w64-mingw32/include/limits.h" 2 3 4
# 196 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 2 3 4
# 8 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/syslimits.h" 2 3 4
# 35 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include-fixed/limits.h" 2 3 4
# 12 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 2 3
# 26 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
#pragma pack(push,_CRT_PACKING)
# 50 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  typedef int (__attribute__((__cdecl__)) *_onexit_t)(void);
# 60 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  typedef struct _div_t {
    int quot;
    int rem;
  } div_t;

  typedef struct _ldiv_t {
    long quot;
    long rem;
  } ldiv_t;





#pragma pack(4)
  typedef struct {
    unsigned char ld[10];
  } _LDOUBLE;
#pragma pack()



  typedef struct {
    double x;
  } _CRT_DOUBLE;

  typedef struct {
    float f;
  } _CRT_FLOAT;

       


  typedef struct {
    long double x;
  } _LONGDOUBLE;

       

#pragma pack(4)
  typedef struct {
    unsigned char ld12[12];
  } _LDBL12;
#pragma pack()
# 116 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  extern int * __imp___mb_cur_max;




__attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) ___mb_cur_max_func(void);
# 143 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  typedef void (__attribute__((__cdecl__)) *_purecall_handler)(void);

  __attribute__ ((__dllimport__)) _purecall_handler __attribute__((__cdecl__)) _set_purecall_handler(_purecall_handler _Handler);
  __attribute__ ((__dllimport__)) _purecall_handler __attribute__((__cdecl__)) _get_purecall_handler(void);

  typedef void (__attribute__((__cdecl__)) *_invalid_parameter_handler)(const wchar_t *,const wchar_t *,const wchar_t *,unsigned int,uintptr_t);
  __attribute__ ((__dllimport__)) _invalid_parameter_handler __attribute__((__cdecl__)) _set_invalid_parameter_handler(_invalid_parameter_handler _Handler);
  __attribute__ ((__dllimport__)) _invalid_parameter_handler __attribute__((__cdecl__)) _get_invalid_parameter_handler(void);



  __attribute__ ((__dllimport__)) extern int *__attribute__((__cdecl__)) _errno(void);

  errno_t __attribute__((__cdecl__)) _set_errno(int _Value);
  errno_t __attribute__((__cdecl__)) _get_errno(int *_Value);

  __attribute__ ((__dllimport__)) unsigned long *__attribute__((__cdecl__)) __doserrno(void);

  errno_t __attribute__((__cdecl__)) _set_doserrno(unsigned long _Value);
  errno_t __attribute__((__cdecl__)) _get_doserrno(unsigned long *_Value);
# 173 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  extern __attribute__((dllimport)) char *_sys_errlist[1];
  extern __attribute__((dllimport)) int _sys_nerr;





  __attribute__ ((__dllimport__)) char ***__attribute__((__cdecl__)) __p___argv(void);
  __attribute__ ((__dllimport__)) int *__attribute__((__cdecl__)) __p__fmode(void);
# 191 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  errno_t __attribute__((__cdecl__)) _get_pgmptr(char **_Value);
  errno_t __attribute__((__cdecl__)) _get_wpgmptr(wchar_t **_Value);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _set_fmode(int _Mode);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _get_fmode(int *_PMode);
# 282 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  extern int * __imp___argc;



  extern char *** __imp___argv;



  extern wchar_t *** __imp___wargv;
# 322 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  extern char *** __imp__environ;




  extern wchar_t *** __imp__wenviron;






  extern char ** __imp__pgmptr;




  extern wchar_t ** __imp__wpgmptr;




  extern unsigned int * __imp__osplatform;




  extern unsigned int * __imp__osver;




  extern unsigned int * __imp__winver;




  extern unsigned int * __imp__winmajor;




  extern unsigned int * __imp__winminor;





  errno_t __attribute__((__cdecl__)) _get_osplatform(unsigned int *_Value);
  errno_t __attribute__((__cdecl__)) _get_osver(unsigned int *_Value);
  errno_t __attribute__((__cdecl__)) _get_winver(unsigned int *_Value);
  errno_t __attribute__((__cdecl__)) _get_winmajor(unsigned int *_Value);
  errno_t __attribute__((__cdecl__)) _get_winminor(unsigned int *_Value);
# 388 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) exit(int _Code) __attribute__ ((__noreturn__));
  void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _exit(int _Code) __attribute__ ((__noreturn__));



  void __attribute__((__cdecl__)) _Exit(int) __attribute__ ((__noreturn__));

  extern inline __attribute__((__gnu_inline__)) __attribute__ ((__noreturn__)) void __attribute__((__cdecl__)) _Exit(int status)
  { _exit(status); }



       

  void __attribute__((__cdecl__)) __attribute__ ((__noreturn__)) abort(void);
       



  __attribute__ ((__dllimport__)) unsigned int __attribute__((__cdecl__)) _set_abort_behavior(unsigned int _Flags,unsigned int _Mask);



  int __attribute__((__cdecl__)) abs(int _X);
  long __attribute__((__cdecl__)) labs(long _X);


  __extension__ long long __attribute__((__cdecl__)) _abs64(long long);

  extern __inline__ __attribute__((__always_inline__,__gnu_inline__)) long long __attribute__((__cdecl__)) _abs64(long long x) {
    return __builtin_llabs(x);
  }


  int __attribute__((__cdecl__)) atexit(void (__attribute__((__cdecl__)) *)(void));


  double __attribute__((__cdecl__)) atof(const char *_String);
  double __attribute__((__cdecl__)) _atof_l(const char *_String,_locale_t _Locale);

  int __attribute__((__cdecl__)) atoi(const char *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atoi_l(const char *_Str,_locale_t _Locale);
  long __attribute__((__cdecl__)) atol(const char *_Str);
  __attribute__ ((__dllimport__)) long __attribute__((__cdecl__)) _atol_l(const char *_Str,_locale_t _Locale);


  void *__attribute__((__cdecl__)) bsearch(const void *_Key,const void *_Base,size_t _NumOfElements,size_t _SizeOfElements,int (__attribute__((__cdecl__)) *_PtFuncCompare)(const void *,const void *));
  void __attribute__((__cdecl__)) qsort(void *_Base,size_t _NumOfElements,size_t _SizeOfElements,int (__attribute__((__cdecl__)) *_PtFuncCompare)(const void *,const void *));

  unsigned short __attribute__((__cdecl__)) _byteswap_ushort(unsigned short _Short);
  unsigned long __attribute__((__cdecl__)) _byteswap_ulong (unsigned long _Long);
  __extension__ unsigned long long __attribute__((__cdecl__)) _byteswap_uint64(unsigned long long _Int64);
  div_t __attribute__((__cdecl__)) div(int _Numerator,int _Denominator);
  char *__attribute__((__cdecl__)) getenv(const char *_VarName) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _itoa(int _Value,char *_Dest,int _Radix);
  __extension__ __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _i64toa(long long _Val,char *_DstBuf,int _Radix) ;
  __extension__ __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _ui64toa(unsigned long long _Val,char *_DstBuf,int _Radix) ;
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _atoi64(const char *_String);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _atoi64_l(const char *_String,_locale_t _Locale);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _strtoi64(const char *_String,char **_EndPtr,int _Radix);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _strtoi64_l(const char *_String,char **_EndPtr,int _Radix,_locale_t _Locale);
  __extension__ __attribute__ ((__dllimport__)) unsigned long long __attribute__((__cdecl__)) _strtoui64(const char *_String,char **_EndPtr,int _Radix);
  __extension__ __attribute__ ((__dllimport__)) unsigned long long __attribute__((__cdecl__)) _strtoui64_l(const char *_String,char **_EndPtr,int _Radix,_locale_t _Locale);
  ldiv_t __attribute__((__cdecl__)) ldiv(long _Numerator,long _Denominator);
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _ltoa(long _Value,char *_Dest,int _Radix) ;
  int __attribute__((__cdecl__)) mblen(const char *_Ch,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _mblen_l(const char *_Ch,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _mbstrlen(const char *_Str);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _mbstrlen_l(const char *_Str,_locale_t _Locale);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _mbstrnlen(const char *_Str,size_t _MaxCount);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _mbstrnlen_l(const char *_Str,size_t _MaxCount,_locale_t _Locale);
  int __attribute__((__cdecl__)) mbtowc(wchar_t * __restrict__ _DstCh,const char * __restrict__ _SrcCh,size_t _SrcSizeInBytes);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _mbtowc_l(wchar_t * __restrict__ _DstCh,const char * __restrict__ _SrcCh,size_t _SrcSizeInBytes,_locale_t _Locale);
  size_t __attribute__((__cdecl__)) mbstowcs(wchar_t * __restrict__ _Dest,const char * __restrict__ _Source,size_t _MaxCount);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _mbstowcs_l(wchar_t * __restrict__ _Dest,const char * __restrict__ _Source,size_t _MaxCount,_locale_t _Locale);
  int __attribute__((__cdecl__)) mkstemp(char *template_name);
  int __attribute__((__cdecl__)) rand(void);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _set_error_mode(int _Mode);
  void __attribute__((__cdecl__)) srand(unsigned int _Seed);
# 477 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
double __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) strtod(const char * __restrict__ _Str,char ** __restrict__ _EndPtr)
{
  double __attribute__((__cdecl__)) __mingw_strtod (const char * __restrict__, char ** __restrict__);
  return __mingw_strtod( _Str, _EndPtr);
}

static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
float __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) strtof(const char * __restrict__ _Str,char ** __restrict__ _EndPtr)
{
  float __attribute__((__cdecl__)) __mingw_strtof (const char * __restrict__, char ** __restrict__);
  return __mingw_strtof( _Str, _EndPtr);
}






  long double __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) strtold(const char * __restrict__ , char ** __restrict__ );


  extern double __attribute__((__cdecl__)) __attribute__ ((__nothrow__))
  __strtod (const char * __restrict__ , char ** __restrict__);







  float __attribute__((__cdecl__)) __mingw_strtof (const char * __restrict__, char ** __restrict__);
  double __attribute__((__cdecl__)) __mingw_strtod (const char * __restrict__, char ** __restrict__);
  long double __attribute__((__cdecl__)) __mingw_strtold(const char * __restrict__, char ** __restrict__);

  __attribute__ ((__dllimport__)) double __attribute__((__cdecl__)) _strtod_l(const char * __restrict__ _Str,char ** __restrict__ _EndPtr,_locale_t _Locale);
  long __attribute__((__cdecl__)) strtol(const char * __restrict__ _Str,char ** __restrict__ _EndPtr,int _Radix);
  __attribute__ ((__dllimport__)) long __attribute__((__cdecl__)) _strtol_l(const char * __restrict__ _Str,char ** __restrict__ _EndPtr,int _Radix,_locale_t _Locale);
  unsigned long __attribute__((__cdecl__)) strtoul(const char * __restrict__ _Str,char ** __restrict__ _EndPtr,int _Radix);
  __attribute__ ((__dllimport__)) unsigned long __attribute__((__cdecl__)) _strtoul_l(const char * __restrict__ _Str,char ** __restrict__ _EndPtr,int _Radix,_locale_t _Locale);


  int __attribute__((__cdecl__)) system(const char *_Command);

  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _ultoa(unsigned long _Value,char *_Dest,int _Radix) ;
  int __attribute__((__cdecl__)) wctomb(char *_MbCh,wchar_t _WCh) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wctomb_l(char *_MbCh,wchar_t _WCh,_locale_t _Locale) ;
  size_t __attribute__((__cdecl__)) wcstombs(char * __restrict__ _Dest,const wchar_t * __restrict__ _Source,size_t _MaxCount) ;
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _wcstombs_l(char * __restrict__ _Dest,const wchar_t * __restrict__ _Source,size_t _MaxCount,_locale_t _Locale) ;



  void *__attribute__((__cdecl__)) calloc(size_t _NumOfElements,size_t _SizeOfElements);
  void __attribute__((__cdecl__)) free(void *_Memory);
  void *__attribute__((__cdecl__)) malloc(size_t _Size);
  void *__attribute__((__cdecl__)) realloc(void *_Memory,size_t _NewSize);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _recalloc(void *_Memory,size_t _Count,size_t _Size);
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _aligned_free(void *_Memory);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_malloc(size_t _Size,size_t _Alignment);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_offset_malloc(size_t _Size,size_t _Alignment,size_t _Offset);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_realloc(void *_Memory,size_t _Size,size_t _Alignment);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_recalloc(void *_Memory,size_t _Count,size_t _Size,size_t _Alignment);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_offset_realloc(void *_Memory,size_t _Size,size_t _Alignment,size_t _Offset);
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _aligned_offset_recalloc(void *_Memory,size_t _Count,size_t _Size,size_t _Alignment,size_t _Offset);





  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _itow(int _Value,wchar_t *_Dest,int _Radix) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _ltow(long _Value,wchar_t *_Dest,int _Radix) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _ultow(unsigned long _Value,wchar_t *_Dest,int _Radix) ;

  double __attribute__((__cdecl__)) __mingw_wcstod(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr);
  float __attribute__((__cdecl__)) __mingw_wcstof(const wchar_t * __restrict__ nptr, wchar_t ** __restrict__ endptr);
  long double __attribute__((__cdecl__)) __mingw_wcstold(const wchar_t * __restrict__, wchar_t ** __restrict__);


  static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
  double __attribute__((__cdecl__)) wcstod(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr){
    return __mingw_wcstod(_Str,_EndPtr);
  }
  static __attribute__ ((__unused__)) __inline__ __attribute__((__cdecl__))
  float __attribute__((__cdecl__)) wcstof(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr){
    return __mingw_wcstof(_Str,_EndPtr);
  }






  long double __attribute__((__cdecl__)) wcstold(const wchar_t * __restrict__, wchar_t ** __restrict__);

  __attribute__ ((__dllimport__)) double __attribute__((__cdecl__)) _wcstod_l(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr,_locale_t _Locale);
  long __attribute__((__cdecl__)) wcstol(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr,int _Radix);
  __attribute__ ((__dllimport__)) long __attribute__((__cdecl__)) _wcstol_l(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr,int _Radix,_locale_t _Locale);
  unsigned long __attribute__((__cdecl__)) wcstoul(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr,int _Radix);
  __attribute__ ((__dllimport__)) unsigned long __attribute__((__cdecl__)) _wcstoul_l(const wchar_t * __restrict__ _Str,wchar_t ** __restrict__ _EndPtr,int _Radix,_locale_t _Locale);
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wgetenv(const wchar_t *_VarName) ;


  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wsystem(const wchar_t *_Command);

  __attribute__ ((__dllimport__)) double __attribute__((__cdecl__)) _wtof(const wchar_t *_Str);
  __attribute__ ((__dllimport__)) double __attribute__((__cdecl__)) _wtof_l(const wchar_t *_Str,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wtoi(const wchar_t *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wtoi_l(const wchar_t *_Str,_locale_t _Locale);
  __attribute__ ((__dllimport__)) long __attribute__((__cdecl__)) _wtol(const wchar_t *_Str);
  __attribute__ ((__dllimport__)) long __attribute__((__cdecl__)) _wtol_l(const wchar_t *_Str,_locale_t _Locale);

  __extension__ __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _i64tow(long long _Val,wchar_t *_DstBuf,int _Radix) ;
  __extension__ __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _ui64tow(unsigned long long _Val,wchar_t *_DstBuf,int _Radix) ;
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _wtoi64(const wchar_t *_Str);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _wtoi64_l(const wchar_t *_Str,_locale_t _Locale);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _wcstoi64(const wchar_t *_Str,wchar_t **_EndPtr,int _Radix);
  __extension__ __attribute__ ((__dllimport__)) long long __attribute__((__cdecl__)) _wcstoi64_l(const wchar_t *_Str,wchar_t **_EndPtr,int _Radix,_locale_t _Locale);
  __extension__ __attribute__ ((__dllimport__)) unsigned long long __attribute__((__cdecl__)) _wcstoui64(const wchar_t *_Str,wchar_t **_EndPtr,int _Radix);
  __extension__ __attribute__ ((__dllimport__)) unsigned long long __attribute__((__cdecl__)) _wcstoui64_l(const wchar_t *_Str ,wchar_t **_EndPtr,int _Radix,_locale_t _Locale);


  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _putenv(const char *_EnvString);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wputenv(const wchar_t *_EnvString);



  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _fullpath(char *_FullPath,const char *_Path,size_t _SizeInBytes);
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _ecvt(double _Val,int _NumOfDigits,int *_PtDec,int *_PtSign) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _fcvt(double _Val,int _NumOfDec,int *_PtDec,int *_PtSign) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _gcvt(double _Val,int _NumOfDigits,char *_DstBuf) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atodbl(_CRT_DOUBLE *_Result,char *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atoldbl(_LDOUBLE *_Result,char *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atoflt(_CRT_FLOAT *_Result,char *_Str);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atodbl_l(_CRT_DOUBLE *_Result,char *_Str,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atoldbl_l(_LDOUBLE *_Result,char *_Str,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _atoflt_l(_CRT_FLOAT *_Result,char *_Str,_locale_t _Locale);
# 628 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
unsigned long __attribute__((__cdecl__)) _lrotl(unsigned long,int);
unsigned long __attribute__((__cdecl__)) _lrotr(unsigned long,int);





  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _makepath(char *_Path,const char *_Drive,const char *_Dir,const char *_Filename,const char *_Ext);
  _onexit_t __attribute__((__cdecl__)) _onexit(_onexit_t _Func);





       
       


  __extension__ unsigned long long __attribute__((__cdecl__)) _rotl64(unsigned long long _Val,int _Shift);
  __extension__ unsigned long long __attribute__((__cdecl__)) _rotr64(unsigned long long Value,int Shift);
       
       
       
       


  unsigned int __attribute__((__cdecl__)) _rotr(unsigned int _Val,int _Shift);
  unsigned int __attribute__((__cdecl__)) _rotl(unsigned int _Val,int _Shift);
       
       
  __extension__ unsigned long long __attribute__((__cdecl__)) _rotr64(unsigned long long _Val,int _Shift);
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _searchenv(const char *_Filename,const char *_EnvVar,char *_ResultPath) ;
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _splitpath(const char *_FullPath,char *_Drive,char *_Dir,char *_Filename,char *_Ext) ;
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _swab(char *_Buf1,char *_Buf2,int _SizeInBytes);



  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wfullpath(wchar_t *_FullPath,const wchar_t *_Path,size_t _SizeInWords);
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _wmakepath(wchar_t *_ResultPath,const wchar_t *_Drive,const wchar_t *_Dir,const wchar_t *_Filename,const wchar_t *_Ext);




  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _wsearchenv(const wchar_t *_Filename,const wchar_t *_EnvVar,wchar_t *_ResultPath) ;
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _wsplitpath(const wchar_t *_FullPath,wchar_t *_Drive,wchar_t *_Dir,wchar_t *_Filename,wchar_t *_Ext) ;


  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _beep(unsigned _Frequency,unsigned _Duration) __attribute__ ((__deprecated__));

  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _seterrormode(int _Mode) __attribute__ ((__deprecated__));
  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) _sleep(unsigned long _Duration) __attribute__ ((__deprecated__));
# 699 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
  char *__attribute__((__cdecl__)) ecvt(double _Val,int _NumOfDigits,int *_PtDec,int *_PtSign) ;
  char *__attribute__((__cdecl__)) fcvt(double _Val,int _NumOfDec,int *_PtDec,int *_PtSign) ;
  char *__attribute__((__cdecl__)) gcvt(double _Val,int _NumOfDigits,char *_DstBuf) ;
  char *__attribute__((__cdecl__)) itoa(int _Val,char *_DstBuf,int _Radix) ;
  char *__attribute__((__cdecl__)) ltoa(long _Val,char *_DstBuf,int _Radix) ;
  int __attribute__((__cdecl__)) putenv(const char *_EnvString) ;



  void __attribute__((__cdecl__)) swab(char *_Buf1,char *_Buf2,int _SizeInBytes) ;


  char *__attribute__((__cdecl__)) ultoa(unsigned long _Val,char *_Dstbuf,int _Radix) ;
  _onexit_t __attribute__((__cdecl__)) onexit(_onexit_t _Func);





  typedef struct { __extension__ long long quot, rem; } lldiv_t;

  __extension__ lldiv_t __attribute__((__cdecl__)) lldiv(long long, long long);

  __extension__ long long __attribute__((__cdecl__)) llabs(long long);

  __extension__ extern inline __attribute__((__gnu_inline__)) long long __attribute__((__cdecl__)) llabs(long long _j) { return (_j >= 0 ? _j : -_j); }


  __extension__ long long __attribute__((__cdecl__)) strtoll(const char * __restrict__, char ** __restrict, int);
  __extension__ unsigned long long __attribute__((__cdecl__)) strtoull(const char * __restrict__, char ** __restrict__, int);


  __extension__ long long __attribute__((__cdecl__)) atoll (const char *);


  __extension__ long long __attribute__((__cdecl__)) wtoll (const wchar_t *);
  __extension__ char *__attribute__((__cdecl__)) lltoa (long long, char *, int);
  __extension__ char *__attribute__((__cdecl__)) ulltoa (unsigned long long , char *, int);
  __extension__ wchar_t *__attribute__((__cdecl__)) lltow (long long, wchar_t *, int);
  __extension__ wchar_t *__attribute__((__cdecl__)) ulltow (unsigned long long, wchar_t *, int);



  __extension__ extern inline __attribute__((__gnu_inline__)) long long __attribute__((__cdecl__)) atoll (const char * _c) { return _atoi64 (_c); }
  __extension__ extern inline __attribute__((__gnu_inline__)) char *__attribute__((__cdecl__)) lltoa (long long _n, char * _c, int _i) { return _i64toa (_n, _c, _i); }
  __extension__ extern inline __attribute__((__gnu_inline__)) char *__attribute__((__cdecl__)) ulltoa (unsigned long long _n, char * _c, int _i) { return _ui64toa (_n, _c, _i); }
  __extension__ extern inline __attribute__((__gnu_inline__)) long long __attribute__((__cdecl__)) wtoll (const wchar_t * _w) { return _wtoi64 (_w); }
  __extension__ extern inline __attribute__((__gnu_inline__)) wchar_t *__attribute__((__cdecl__)) lltow (long long _n, wchar_t * _w, int _i) { return _i64tow (_n, _w, _i); }
  __extension__ extern inline __attribute__((__gnu_inline__)) wchar_t *__attribute__((__cdecl__)) ulltow (unsigned long long _n, wchar_t * _w, int _i) { return _ui64tow (_n, _w, _i); }
# 757 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 3
#pragma pack(pop)

# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdlib_s.h" 1 3
# 9 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdlib_s.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/stdlib_s.h" 2 3





  __attribute__ ((__dllimport__)) void * __attribute__((__cdecl__)) bsearch_s(const void *_Key,const void *_Base,rsize_t _NumOfElements,rsize_t _SizeOfElements,int (__attribute__((__cdecl__)) * _PtFuncCompare)(void *, const void *, const void *), void *_Context);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _dupenv_s(char **_PBuffer,size_t *_PBufferSizeInBytes,const char *_VarName);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) getenv_s(size_t *_ReturnSize,char *_DstBuf,rsize_t _DstSize,const char *_VarName);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _itoa_s(int _Value,char *_DstBuf,size_t _Size,int _Radix);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _i64toa_s(long long _Val,char *_DstBuf,size_t _Size,int _Radix);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ui64toa_s(unsigned long long _Val,char *_DstBuf,size_t _Size,int _Radix);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ltoa_s(long _Val,char *_DstBuf,size_t _Size,int _Radix);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) mbstowcs_s(size_t *_PtNumOfCharConverted,wchar_t *_DstBuf,size_t _SizeInWords,const char *_SrcBuf,size_t _MaxCount);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _mbstowcs_s_l(size_t *_PtNumOfCharConverted,wchar_t *_DstBuf,size_t _SizeInWords,const char *_SrcBuf,size_t _MaxCount,_locale_t _Locale);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ultoa_s(unsigned long _Val,char *_DstBuf,size_t _Size,int _Radix);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wctomb_s(int *_SizeConverted,char *_MbCh,rsize_t _SizeInBytes,wchar_t _WCh);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wctomb_s_l(int *_SizeConverted,char *_MbCh,size_t _SizeInBytes,wchar_t _WCh,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wcstombs_s(size_t *_PtNumOfCharConverted,char *_Dst,size_t _DstSizeInBytes,const wchar_t *_Src,size_t _MaxCountInBytes);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcstombs_s_l(size_t *_PtNumOfCharConverted,char *_Dst,size_t _DstSizeInBytes,const wchar_t *_Src,size_t _MaxCountInBytes,_locale_t _Locale);
 


  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _ecvt_s(char *_DstBuf,size_t _Size,double _Val,int _NumOfDights,int *_PtDec,int *_PtSign);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _fcvt_s(char *_DstBuf,size_t _Size,double _Val,int _NumOfDec,int *_PtDec,int *_PtSign);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _gcvt_s(char *_DstBuf,size_t _Size,double _Val,int _NumOfDigits);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _makepath_s(char *_PathResult,size_t _Size,const char *_Drive,const char *_Dir,const char *_Filename,const char *_Ext);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _putenv_s(const char *_Name,const char *_Value);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _searchenv_s(const char *_Filename,const char *_EnvVar,char *_ResultPath,size_t _SizeInBytes);

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _splitpath_s(const char *_FullPath,char *_Drive,size_t _DriveSize,char *_Dir,size_t _DirSize,char *_Filename,size_t _FilenameSize,char *_Ext,size_t _ExtSize);
 



  __attribute__ ((__dllimport__)) void __attribute__((__cdecl__)) qsort_s(void *_Base,size_t _NumOfElements,size_t _SizeOfElements,int (__attribute__((__cdecl__)) *_PtFuncCompare)(void *,const void *,const void *),void *_Context);
# 760 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 2 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 1 3
# 11 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
#pragma pack(push,_CRT_PACKING)
# 46 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
  typedef struct _heapinfo {
    int *_pentry;
    size_t _size;
    int _useflag;
  } _HEAPINFO;


  extern unsigned int _amblksiz;
# 74 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
void * __mingw_aligned_malloc (size_t _Size, size_t _Alignment);
void __mingw_aligned_free (void *_Memory);
void * __mingw_aligned_offset_realloc (void *_Memory, size_t _Size, size_t _Alignment, size_t _Offset);
void * __mingw_aligned_realloc (void *_Memory, size_t _Size, size_t _Offset);


# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mm_malloc.h" 1 3 4
# 29 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mm_malloc.h" 3 4
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/errno.h" 1 3 4
# 30 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mm_malloc.h" 2 3 4


static __inline__ void *
_mm_malloc (size_t __size, size_t __align)
{
  void * __malloc_ptr;
  void * __aligned_ptr;


  if (__align & (__align - 1))
    {

      (*_errno()) = 22;

      return ((void *) 0);
    }

  if (__size == 0)
    return ((void *) 0);





    if (__align < 2 * sizeof (void *))
      __align = 2 * sizeof (void *);

  __malloc_ptr = malloc (__size + __align);
  if (!__malloc_ptr)
    return ((void *) 0);


  __aligned_ptr = (void *) (((size_t) __malloc_ptr + __align)
       & ~((size_t) (__align) - 1));


  ((void **) __aligned_ptr)[-1] = __malloc_ptr;

  return __aligned_ptr;
}

static __inline__ void
_mm_free (void *__aligned_ptr)
{
  if (__aligned_ptr)
    free (((void **) __aligned_ptr)[-1]);
}
# 81 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 2 3




  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _resetstkoflw (void);

  __attribute__ ((__dllimport__)) unsigned long __attribute__((__cdecl__)) _set_malloc_crt_max_wait(unsigned long _NewValue);

  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _expand(void *_Memory,size_t _NewSize);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _msize(void *_Memory);






  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _get_sbh_threshold(void);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _set_sbh_threshold(size_t _NewValue);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _set_amblksiz(size_t _Value);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _get_amblksiz(size_t *_Value);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _heapadd(void *_Memory,size_t _Size);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _heapchk(void);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _heapmin(void);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _heapset(unsigned int _Fill);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _heapwalk(_HEAPINFO *_EntryInfo);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _heapused(size_t *_Used,size_t *_Commit);
  __attribute__ ((__dllimport__)) intptr_t __attribute__((__cdecl__)) _get_heap_handle(void);
# 120 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
  static __inline void *_MarkAllocaS(void *_Ptr,unsigned int _Marker) {
    if(_Ptr) {
      *((unsigned int*)_Ptr) = _Marker;
      _Ptr = (char*)_Ptr + 16;
    }
    return _Ptr;
  }
# 139 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
  static __inline void __attribute__((__cdecl__)) _freea(void *_Memory) {
    unsigned int _Marker;
    if(_Memory) {
      _Memory = (char*)_Memory - 16;
      _Marker = *(unsigned int *)_Memory;
      if(_Marker==0xDDDD) {
 free(_Memory);
      }





    }
  }
# 185 "C:/msys64/mingw64/x86_64-w64-mingw32/include/malloc.h" 3
#pragma pack(pop)
# 761 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stdlib.h" 2 3
# 18 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/string.h" 1 3
# 45 "C:/msys64/mingw64/x86_64-w64-mingw32/include/string.h" 3
  __attribute__ ((__dllimport__)) void *__attribute__((__cdecl__)) _memccpy(void *_Dst,const void *_Src,int _Val,size_t _MaxCount);
  void *__attribute__((__cdecl__)) memchr(const void *_Buf ,int _Val,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _memicmp(const void *_Buf1,const void *_Buf2,size_t _Size);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _memicmp_l(const void *_Buf1,const void *_Buf2,size_t _Size,_locale_t _Locale);
  int __attribute__((__cdecl__)) memcmp(const void *_Buf1,const void *_Buf2,size_t _Size);
  void * __attribute__((__cdecl__)) memcpy(void * __restrict__ _Dst,const void * __restrict__ _Src,size_t _Size) ;
  __attribute__((dllimport)) errno_t __attribute__((__cdecl__)) memcpy_s (void *_dest,size_t _numberOfElements,const void *_src,size_t _count);
  void * __attribute__((__cdecl__)) mempcpy (void *_Dst, const void *_Src, size_t _Size);
  void * __attribute__((__cdecl__)) memset(void *_Dst,int _Val,size_t _Size);

  void * __attribute__((__cdecl__)) memccpy(void *_Dst,const void *_Src,int _Val,size_t _Size) ;
  int __attribute__((__cdecl__)) memicmp(const void *_Buf1,const void *_Buf2,size_t _Size) ;


  char * __attribute__((__cdecl__)) _strset(char *_Str,int _Val) ;
  char * __attribute__((__cdecl__)) _strset_l(char *_Str,int _Val,_locale_t _Locale) ;
  char * __attribute__((__cdecl__)) strcpy(char * __restrict__ _Dest,const char * __restrict__ _Source);
  char * __attribute__((__cdecl__)) strcat(char * __restrict__ _Dest,const char * __restrict__ _Source);
  int __attribute__((__cdecl__)) strcmp(const char *_Str1,const char *_Str2);
  size_t __attribute__((__cdecl__)) strlen(const char *_Str);
  size_t __attribute__((__cdecl__)) strnlen(const char *_Str,size_t _MaxCount);
  void *__attribute__((__cdecl__)) memmove(void *_Dst,const void *_Src,size_t _Size) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strdup(const char *_Src);
  char *__attribute__((__cdecl__)) strchr(const char *_Str,int _Val);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _stricmp(const char *_Str1,const char *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strcmpi(const char *_Str1,const char *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _stricmp_l(const char *_Str1,const char *_Str2,_locale_t _Locale);
  int __attribute__((__cdecl__)) strcoll(const char *_Str1,const char *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strcoll_l(const char *_Str1,const char *_Str2,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _stricoll(const char *_Str1,const char *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _stricoll_l(const char *_Str1,const char *_Str2,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strncoll (const char *_Str1,const char *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strncoll_l(const char *_Str1,const char *_Str2,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strnicoll (const char *_Str1,const char *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strnicoll_l(const char *_Str1,const char *_Str2,size_t _MaxCount,_locale_t _Locale);
  size_t __attribute__((__cdecl__)) strcspn(const char *_Str,const char *_Control);
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strerror(const char *_ErrMsg) ;
  char *__attribute__((__cdecl__)) strerror(int) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strlwr(char *_String) ;
  char *strlwr_l(char *_String,_locale_t _Locale) ;
  char *__attribute__((__cdecl__)) strncat(char * __restrict__ _Dest,const char * __restrict__ _Source,size_t _Count) ;
  int __attribute__((__cdecl__)) strncmp(const char *_Str1,const char *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strnicmp(const char *_Str1,const char *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _strnicmp_l(const char *_Str1,const char *_Str2,size_t _MaxCount,_locale_t _Locale);
  char *strncpy(char * __restrict__ _Dest,const char * __restrict__ _Source,size_t _Count) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strnset(char *_Str,int _Val,size_t _MaxCount) ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strnset_l(char *str,int c,size_t count,_locale_t _Locale) ;
  char *__attribute__((__cdecl__)) strpbrk(const char *_Str,const char *_Control);
  char *__attribute__((__cdecl__)) strrchr(const char *_Str,int _Ch);
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strrev(char *_Str);
  size_t __attribute__((__cdecl__)) strspn(const char *_Str,const char *_Control);
  char *__attribute__((__cdecl__)) strstr(const char *_Str,const char *_SubStr);
  char *__attribute__((__cdecl__)) strtok(char * __restrict__ _Str,const char * __restrict__ _Delim) ;
       

  char *strtok_r(char * __restrict__ _Str, const char * __restrict__ _Delim, char ** __restrict__ __last);
       
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strupr(char *_String) ;
  __attribute__ ((__dllimport__)) char *_strupr_l(char *_String,_locale_t _Locale) ;
  size_t __attribute__((__cdecl__)) strxfrm(char * __restrict__ _Dst,const char * __restrict__ _Src,size_t _MaxCount);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _strxfrm_l(char * __restrict__ _Dst,const char * __restrict__ _Src,size_t _MaxCount,_locale_t _Locale);


  char *__attribute__((__cdecl__)) strdup(const char *_Src) ;
  int __attribute__((__cdecl__)) strcmpi(const char *_Str1,const char *_Str2) ;
  int __attribute__((__cdecl__)) stricmp(const char *_Str1,const char *_Str2) ;
  char *__attribute__((__cdecl__)) strlwr(char *_Str) ;
  int __attribute__((__cdecl__)) strnicmp(const char *_Str1,const char *_Str,size_t _MaxCount) ;
  int __attribute__((__cdecl__)) strncasecmp (const char *, const char *, size_t);
  int __attribute__((__cdecl__)) strcasecmp (const char *, const char *);

  extern inline __attribute__((__gnu_inline__)) int __attribute__((__cdecl__)) strncasecmp (const char *__sz1, const char *__sz2, size_t __sizeMaxCompare) { return _strnicmp (__sz1, __sz2, __sizeMaxCompare); }
  extern inline __attribute__((__gnu_inline__)) int __attribute__((__cdecl__)) strcasecmp (const char *__sz1, const char *__sz2) { return _stricmp (__sz1, __sz2); }




  char *__attribute__((__cdecl__)) strnset(char *_Str,int _Val,size_t _MaxCount) ;
  char *__attribute__((__cdecl__)) strrev(char *_Str) ;
  char *__attribute__((__cdecl__)) strset(char *_Str,int _Val) ;
  char *__attribute__((__cdecl__)) strupr(char *_Str) ;





  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcsdup(const wchar_t *_Str);
  wchar_t *__attribute__((__cdecl__)) wcscat(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Source) ;
  wchar_t *__attribute__((__cdecl__)) wcschr(const wchar_t *_Str,wchar_t _Ch);
  int __attribute__((__cdecl__)) wcscmp(const wchar_t *_Str1,const wchar_t *_Str2);
  wchar_t *__attribute__((__cdecl__)) wcscpy(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Source) ;
  size_t __attribute__((__cdecl__)) wcscspn(const wchar_t *_Str,const wchar_t *_Control);
  size_t __attribute__((__cdecl__)) wcslen(const wchar_t *_Str);
  size_t __attribute__((__cdecl__)) wcsnlen(const wchar_t *_Src,size_t _MaxCount);
  wchar_t *wcsncat(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Source,size_t _Count) ;
  int __attribute__((__cdecl__)) wcsncmp(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount);
  wchar_t *wcsncpy(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Source,size_t _Count) ;
  wchar_t *__attribute__((__cdecl__)) _wcsncpy_l(wchar_t * __restrict__ _Dest,const wchar_t * __restrict__ _Source,size_t _Count,_locale_t _Locale) ;
  wchar_t *__attribute__((__cdecl__)) wcspbrk(const wchar_t *_Str,const wchar_t *_Control);
  wchar_t *__attribute__((__cdecl__)) wcsrchr(const wchar_t *_Str,wchar_t _Ch);
  size_t __attribute__((__cdecl__)) wcsspn(const wchar_t *_Str,const wchar_t *_Control);
  wchar_t *__attribute__((__cdecl__)) wcsstr(const wchar_t *_Str,const wchar_t *_SubStr);
  wchar_t *__attribute__((__cdecl__)) wcstok(wchar_t * __restrict__ _Str,const wchar_t * __restrict__ _Delim) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcserror(int _ErrNum) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) __wcserror(const wchar_t *_Str) ;
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsicmp(const wchar_t *_Str1,const wchar_t *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsicmp_l(const wchar_t *_Str1,const wchar_t *_Str2,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsnicmp(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsnicmp_l(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcsnset(wchar_t *_Str,wchar_t _Val,size_t _MaxCount) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcsrev(wchar_t *_Str);
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcsset(wchar_t *_Str,wchar_t _Val) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcslwr(wchar_t *_String) ;
  __attribute__ ((__dllimport__)) wchar_t *_wcslwr_l(wchar_t *_String,_locale_t _Locale) ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcsupr(wchar_t *_String) ;
  __attribute__ ((__dllimport__)) wchar_t *_wcsupr_l(wchar_t *_String,_locale_t _Locale) ;
  size_t __attribute__((__cdecl__)) wcsxfrm(wchar_t * __restrict__ _Dst,const wchar_t * __restrict__ _Src,size_t _MaxCount);
  __attribute__ ((__dllimport__)) size_t __attribute__((__cdecl__)) _wcsxfrm_l(wchar_t * __restrict__ _Dst,const wchar_t * __restrict__ _Src,size_t _MaxCount,_locale_t _Locale);
  int __attribute__((__cdecl__)) wcscoll(const wchar_t *_Str1,const wchar_t *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcscoll_l(const wchar_t *_Str1,const wchar_t *_Str2,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsicoll(const wchar_t *_Str1,const wchar_t *_Str2);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsicoll_l(const wchar_t *_Str1,const wchar_t *_Str2,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsncoll(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsncoll_l(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsnicoll(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount);
  __attribute__ ((__dllimport__)) int __attribute__((__cdecl__)) _wcsnicoll_l(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount,_locale_t _Locale);


  wchar_t *__attribute__((__cdecl__)) wcsdup(const wchar_t *_Str) ;

  int __attribute__((__cdecl__)) wcsicmp(const wchar_t *_Str1,const wchar_t *_Str2) ;
  int __attribute__((__cdecl__)) wcsnicmp(const wchar_t *_Str1,const wchar_t *_Str2,size_t _MaxCount) ;
  wchar_t *__attribute__((__cdecl__)) wcsnset(wchar_t *_Str,wchar_t _Val,size_t _MaxCount) ;
  wchar_t *__attribute__((__cdecl__)) wcsrev(wchar_t *_Str) ;
  wchar_t *__attribute__((__cdecl__)) wcsset(wchar_t *_Str,wchar_t _Val) ;
  wchar_t *__attribute__((__cdecl__)) wcslwr(wchar_t *_Str) ;
  wchar_t *__attribute__((__cdecl__)) wcsupr(wchar_t *_Str) ;
  int __attribute__((__cdecl__)) wcsicoll(const wchar_t *_Str1,const wchar_t *_Str2) ;







# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/string_s.h" 1 3
# 9 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/string_s.h" 3
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/string.h" 1 3
# 10 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/string_s.h" 2 3
# 24 "C:/msys64/mingw64/x86_64-w64-mingw32/include/sec_api/string_s.h" 3
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strset_s(char *_Dst,size_t _DstSize,int _Value);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strerror_s(char *_Buf,size_t _SizeInBytes,const char *_ErrMsg);
  __attribute__((dllimport)) errno_t __attribute__((__cdecl__)) strerror_s(char *_Buf,size_t _SizeInBytes,int _ErrNum);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strlwr_s(char *_Str,size_t _Size);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strlwr_s_l(char *_Str,size_t _Size,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strnset_s(char *_Str,size_t _Size,int _Val,size_t _MaxCount);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strupr_s(char *_Str,size_t _Size);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strupr_s_l(char *_Str,size_t _Size,_locale_t _Locale);

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) strncat_s(char *_Dst,size_t _DstSizeInChars,const char *_Src,size_t _MaxCount);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strncat_s_l(char *_Dst,size_t _DstSizeInChars,const char *_Src,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) strcpy_s(char *_Dst, rsize_t _SizeInBytes, const char *_Src);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) strncpy_s(char *_Dst, size_t _DstSizeInChars, const char *_Src, size_t _MaxCount);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _strncpy_s_l(char *_Dst, size_t _DstSizeInChars, const char *_Src, size_t _MaxCount, _locale_t _Locale);
  ;
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) strtok_s(char *_Str,const char *_Delim,char **_Context);
  __attribute__ ((__dllimport__)) char *__attribute__((__cdecl__)) _strtok_s_l(char *_Str,const char *_Delim,char **_Context,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) strcat_s(char *_Dst, rsize_t _SizeInBytes, const char * _Src);
 

  extern __inline__ __attribute__((__always_inline__,__gnu_inline__)) size_t __attribute__((__cdecl__)) strnlen_s(const char * _src, size_t _count) {
    return _src ? strnlen(_src, _count) : 0;
  }

  __attribute__((dllimport)) errno_t __attribute__((__cdecl__)) memmove_s(void *_dest,size_t _numberOfElements,const void *_src,size_t _count);


  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) wcstok_s(wchar_t *_Str,const wchar_t *_Delim,wchar_t **_Context);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcserror_s(wchar_t *_Buf,size_t _SizeInWords,int _ErrNum);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) __wcserror_s(wchar_t *_Buffer,size_t _SizeInWords,const wchar_t *_ErrMsg);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsnset_s(wchar_t *_Dst,size_t _DstSizeInWords,wchar_t _Val,size_t _MaxCount);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsset_s(wchar_t *_Str,size_t _SizeInWords,wchar_t _Val);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcslwr_s(wchar_t *_Str,size_t _SizeInWords);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcslwr_s_l(wchar_t *_Str,size_t _SizeInWords,_locale_t _Locale);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsupr_s(wchar_t *_Str,size_t _Size);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsupr_s_l(wchar_t *_Str,size_t _Size,_locale_t _Locale);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wcscpy_s(wchar_t *_Dst, rsize_t _SizeInWords, const wchar_t *_Src);
 
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wcscat_s(wchar_t * _Dst, rsize_t _SizeInWords, const wchar_t *_Src);
 

  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wcsncat_s(wchar_t *_Dst,size_t _DstSizeInChars,const wchar_t *_Src,size_t _MaxCount);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsncat_s_l(wchar_t *_Dst,size_t _DstSizeInChars,const wchar_t *_Src,size_t _MaxCount,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) wcsncpy_s(wchar_t *_Dst, size_t _DstSizeInChars, const wchar_t *_Src, size_t _MaxCount);
  ;
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsncpy_s_l(wchar_t *_Dst, size_t _DstSizeInChars, const wchar_t *_Src, size_t _MaxCount, _locale_t _Locale);
  ;
  __attribute__ ((__dllimport__)) wchar_t *__attribute__((__cdecl__)) _wcstok_s_l(wchar_t *_Str,const wchar_t *_Delim,wchar_t **_Context,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsset_s_l(wchar_t *_Str,size_t _SizeInChars,unsigned int _Val,_locale_t _Locale);
  __attribute__ ((__dllimport__)) errno_t __attribute__((__cdecl__)) _wcsnset_s_l(wchar_t *_Str,size_t _SizeInChars,unsigned int _Val, size_t _Count,_locale_t _Locale);

  extern __inline__ __attribute__((__always_inline__,__gnu_inline__)) size_t __attribute__((__cdecl__)) wcsnlen_s(const wchar_t * _src, size_t _count) {
    return _src ? wcsnlen(_src, _count) : 0;
  }
# 191 "C:/msys64/mingw64/x86_64-w64-mingw32/include/string.h" 2 3
# 19 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/tmmintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/tmmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pmmintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pmmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mmintrin.h" 1 3 4
# 44 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mmintrin.h" 3 4
typedef int __m64 __attribute__ ((__vector_size__ (8), __may_alias__));


typedef int __m64_u __attribute__ ((__vector_size__ (8), __may_alias__, __aligned__ (1)));


typedef int __v2si __attribute__ ((__vector_size__ (8)));
typedef short __v4hi __attribute__ ((__vector_size__ (8)));
typedef char __v8qi __attribute__ ((__vector_size__ (8)));
typedef long long __v1di __attribute__ ((__vector_size__ (8)));
typedef float __v2sf __attribute__ ((__vector_size__ (8)));


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_empty (void)
{
  __builtin_ia32_emms ();
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_empty (void)
{
  _mm_empty ();
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi32_si64 (int __i)
{
  return (__m64) __builtin_ia32_vec_init_v2si (__i, 0);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_from_int (int __i)
{
  return _mm_cvtsi32_si64 (__i);
}





extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_from_int64 (long long __i)
{
  return (__m64) __i;
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_m64 (long long __i)
{
  return (__m64) __i;
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64x_si64 (long long __i)
{
  return (__m64) __i;
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pi64x (long long __i)
{
  return (__m64) __i;
}



extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_si32 (__m64 __i)
{
  return __builtin_ia32_vec_ext_v2si ((__v2si)__i, 0);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_to_int (__m64 __i)
{
  return _mm_cvtsi64_si32 (__i);
}





extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_to_int64 (__m64 __i)
{
  return (long long)__i;
}

extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtm64_si64 (__m64 __i)
{
  return (long long)__i;
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_si64x (__m64 __i)
{
  return (long long)__i;
}





extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packs_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_packsswb ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_packsswb (__m64 __m1, __m64 __m2)
{
  return _mm_packs_pi16 (__m1, __m2);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packs_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_packssdw ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_packssdw (__m64 __m1, __m64 __m2)
{
  return _mm_packs_pi32 (__m1, __m2);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packs_pu16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_packuswb ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_packuswb (__m64 __m1, __m64 __m2)
{
  return _mm_packs_pu16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpckhbw ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpckhbw (__m64 __m1, __m64 __m2)
{
  return _mm_unpackhi_pi8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpckhwd ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpckhwd (__m64 __m1, __m64 __m2)
{
  return _mm_unpackhi_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpckhdq ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpckhdq (__m64 __m1, __m64 __m2)
{
  return _mm_unpackhi_pi32 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpcklbw ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpcklbw (__m64 __m1, __m64 __m2)
{
  return _mm_unpacklo_pi8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpcklwd ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpcklwd (__m64 __m1, __m64 __m2)
{
  return _mm_unpacklo_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_punpckldq ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_punpckldq (__m64 __m1, __m64 __m2)
{
  return _mm_unpacklo_pi32 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddb (__m64 __m1, __m64 __m2)
{
  return _mm_add_pi8 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddw (__m64 __m1, __m64 __m2)
{
  return _mm_add_pi16 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddd ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddd (__m64 __m1, __m64 __m2)
{
  return _mm_add_pi32 (__m1, __m2);
}
# 328 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mmintrin.h" 3 4
extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_si64 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddq ((__v1di)__m1, (__v1di)__m2);
}







extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddsb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddsb (__m64 __m1, __m64 __m2)
{
  return _mm_adds_pi8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddsw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddsw (__m64 __m1, __m64 __m2)
{
  return _mm_adds_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_pu8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddusb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddusb (__m64 __m1, __m64 __m2)
{
  return _mm_adds_pu8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_pu16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_paddusw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_paddusw (__m64 __m1, __m64 __m2)
{
  return _mm_adds_pu16 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubb (__m64 __m1, __m64 __m2)
{
  return _mm_sub_pi8 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubw (__m64 __m1, __m64 __m2)
{
  return _mm_sub_pi16 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubd ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubd (__m64 __m1, __m64 __m2)
{
  return _mm_sub_pi32 (__m1, __m2);
}
# 444 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/mmintrin.h" 3 4
extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_si64 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubq ((__v1di)__m1, (__v1di)__m2);
}







extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubsb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubsb (__m64 __m1, __m64 __m2)
{
  return _mm_subs_pi8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubsw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubsw (__m64 __m1, __m64 __m2)
{
  return _mm_subs_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_pu8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubusb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubusb (__m64 __m1, __m64 __m2)
{
  return _mm_subs_pu8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_pu16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_psubusw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psubusw (__m64 __m1, __m64 __m2)
{
  return _mm_subs_pu16 (__m1, __m2);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_madd_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pmaddwd ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmaddwd (__m64 __m1, __m64 __m2)
{
  return _mm_madd_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhi_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pmulhw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmulhw (__m64 __m1, __m64 __m2)
{
  return _mm_mulhi_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mullo_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pmullw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmullw (__m64 __m1, __m64 __m2)
{
  return _mm_mullo_pi16 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_pi16 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psllw ((__v4hi)__m, (__v4hi)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psllw (__m64 __m, __m64 __count)
{
  return _mm_sll_pi16 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_pi16 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psllwi ((__v4hi)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psllwi (__m64 __m, int __count)
{
  return _mm_slli_pi16 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_pi32 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_pslld ((__v2si)__m, (__v2si)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pslld (__m64 __m, __m64 __count)
{
  return _mm_sll_pi32 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_pi32 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_pslldi ((__v2si)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pslldi (__m64 __m, int __count)
{
  return _mm_slli_pi32 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_si64 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psllq ((__v1di)__m, (__v1di)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psllq (__m64 __m, __m64 __count)
{
  return _mm_sll_si64 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_si64 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psllqi ((__v1di)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psllqi (__m64 __m, int __count)
{
  return _mm_slli_si64 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sra_pi16 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psraw ((__v4hi)__m, (__v4hi)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psraw (__m64 __m, __m64 __count)
{
  return _mm_sra_pi16 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srai_pi16 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psrawi ((__v4hi)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrawi (__m64 __m, int __count)
{
  return _mm_srai_pi16 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sra_pi32 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psrad ((__v2si)__m, (__v2si)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrad (__m64 __m, __m64 __count)
{
  return _mm_sra_pi32 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srai_pi32 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psradi ((__v2si)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psradi (__m64 __m, int __count)
{
  return _mm_srai_pi32 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_pi16 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psrlw ((__v4hi)__m, (__v4hi)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrlw (__m64 __m, __m64 __count)
{
  return _mm_srl_pi16 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_pi16 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psrlwi ((__v4hi)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrlwi (__m64 __m, int __count)
{
  return _mm_srli_pi16 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_pi32 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psrld ((__v2si)__m, (__v2si)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrld (__m64 __m, __m64 __count)
{
  return _mm_srl_pi32 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_pi32 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psrldi ((__v2si)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrldi (__m64 __m, int __count)
{
  return _mm_srli_pi32 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_si64 (__m64 __m, __m64 __count)
{
  return (__m64) __builtin_ia32_psrlq ((__v1di)__m, (__v1di)__count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrlq (__m64 __m, __m64 __count)
{
  return _mm_srl_si64 (__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_si64 (__m64 __m, int __count)
{
  return (__m64) __builtin_ia32_psrlqi ((__v1di)__m, __count);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psrlqi (__m64 __m, int __count)
{
  return _mm_srli_si64 (__m, __count);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_and_si64 (__m64 __m1, __m64 __m2)
{
  return __builtin_ia32_pand (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pand (__m64 __m1, __m64 __m2)
{
  return _mm_and_si64 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_andnot_si64 (__m64 __m1, __m64 __m2)
{
  return __builtin_ia32_pandn (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pandn (__m64 __m1, __m64 __m2)
{
  return _mm_andnot_si64 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_si64 (__m64 __m1, __m64 __m2)
{
  return __builtin_ia32_por (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_por (__m64 __m1, __m64 __m2)
{
  return _mm_or_si64 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_si64 (__m64 __m1, __m64 __m2)
{
  return __builtin_ia32_pxor (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pxor (__m64 __m1, __m64 __m2)
{
  return _mm_xor_si64 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpeqb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpeqb (__m64 __m1, __m64 __m2)
{
  return _mm_cmpeq_pi8 (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpgtb ((__v8qi)__m1, (__v8qi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpgtb (__m64 __m1, __m64 __m2)
{
  return _mm_cmpgt_pi8 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpeqw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpeqw (__m64 __m1, __m64 __m2)
{
  return _mm_cmpeq_pi16 (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpgtw ((__v4hi)__m1, (__v4hi)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpgtw (__m64 __m1, __m64 __m2)
{
  return _mm_cmpgt_pi16 (__m1, __m2);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpeqd ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpeqd (__m64 __m1, __m64 __m2)
{
  return _mm_cmpeq_pi32 (__m1, __m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)
{
  return (__m64) __builtin_ia32_pcmpgtd ((__v2si)__m1, (__v2si)__m2);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pcmpgtd (__m64 __m1, __m64 __m2)
{
  return _mm_cmpgt_pi32 (__m1, __m2);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setzero_si64 (void)
{
  return (__m64)0LL;
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pi32 (int __i1, int __i0)
{
  return (__m64) __builtin_ia32_vec_init_v2si (__i0, __i1);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pi16 (short __w3, short __w2, short __w1, short __w0)
{
  return (__m64) __builtin_ia32_vec_init_v4hi (__w0, __w1, __w2, __w3);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pi8 (char __b7, char __b6, char __b5, char __b4,
      char __b3, char __b2, char __b1, char __b0)
{
  return (__m64) __builtin_ia32_vec_init_v8qi (__b0, __b1, __b2, __b3,
            __b4, __b5, __b6, __b7);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_pi32 (int __i0, int __i1)
{
  return _mm_set_pi32 (__i1, __i0);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_pi16 (short __w0, short __w1, short __w2, short __w3)
{
  return _mm_set_pi16 (__w3, __w2, __w1, __w0);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_pi8 (char __b0, char __b1, char __b2, char __b3,
       char __b4, char __b5, char __b6, char __b7)
{
  return _mm_set_pi8 (__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_pi32 (int __i)
{
  return _mm_set_pi32 (__i, __i);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_pi16 (short __w)
{
  return _mm_set_pi16 (__w, __w, __w, __w);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_pi8 (char __b)
{
  return _mm_set_pi8 (__b, __b, __b, __b, __b, __b, __b, __b);
}
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 2 3 4





enum _mm_hint
{

  _MM_HINT_ET0 = 7,
  _MM_HINT_ET1 = 6,
  _MM_HINT_T0 = 3,
  _MM_HINT_T1 = 2,
  _MM_HINT_T2 = 1,
  _MM_HINT_NTA = 0
};




extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_prefetch (const void *__P, enum _mm_hint __I)
{
  __builtin_prefetch (__P, (__I & 0x4) >> 2, __I & 0x3);
}
# 73 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
typedef float __m128 __attribute__ ((__vector_size__ (16), __may_alias__));


typedef float __m128_u __attribute__ ((__vector_size__ (16), __may_alias__, __aligned__ (1)));


typedef float __v4sf __attribute__ ((__vector_size__ (16)));
# 113 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_undefined_ps (void)
{
  __m128 __Y = __Y;
  return __Y;
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setzero_ps (void)
{
  return __extension__ (__m128){ 0.0f, 0.0f, 0.0f, 0.0f };
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_addss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_subss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_mulss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_divss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_ss (__m128 __A)
{
  return (__m128) __builtin_ia32_sqrtss ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp_ss (__m128 __A)
{
  return (__m128) __builtin_ia32_rcpss ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt_ss (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrtss ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxss ((__v4sf)__A, (__v4sf)__B);
}



extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_ps (__m128 __A, __m128 __B)
{
  return (__m128) ((__v4sf)__A + (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_ps (__m128 __A, __m128 __B)
{
  return (__m128) ((__v4sf)__A - (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_ps (__m128 __A, __m128 __B)
{
  return (__m128) ((__v4sf)__A * (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_ps (__m128 __A, __m128 __B)
{
  return (__m128) ((__v4sf)__A / (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_sqrtps ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rcpps ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrtps ((__v4sf)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxps ((__v4sf)__A, (__v4sf)__B);
}



extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_and_ps (__m128 __A, __m128 __B)
{
  return __builtin_ia32_andps (__A, __B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_andnot_ps (__m128 __A, __m128 __B)
{
  return __builtin_ia32_andnps (__A, __B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_ps (__m128 __A, __m128 __B)
{
  return __builtin_ia32_orps (__A, __B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_ps (__m128 __A, __m128 __B)
{
  return __builtin_ia32_xorps (__A, __B);
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpeqss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpltss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpless ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movss ((__v4sf) __A,
     (__v4sf)
     __builtin_ia32_cmpltss ((__v4sf) __B,
        (__v4sf)
        __A));
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movss ((__v4sf) __A,
     (__v4sf)
     __builtin_ia32_cmpless ((__v4sf) __B,
        (__v4sf)
        __A));
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpneqss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnlt_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpnltss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnle_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpnless ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpngt_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movss ((__v4sf) __A,
     (__v4sf)
     __builtin_ia32_cmpnltss ((__v4sf) __B,
         (__v4sf)
         __A));
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnge_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movss ((__v4sf) __A,
     (__v4sf)
     __builtin_ia32_cmpnless ((__v4sf) __B,
         (__v4sf)
         __A));
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpord_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpordss ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpunord_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpunordss ((__v4sf)__A, (__v4sf)__B);
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpeqps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpltps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpleps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpgtps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpgeps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpneqps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnlt_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpnltps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnle_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpnleps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpngt_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpngtps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnge_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpngeps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpord_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpordps ((__v4sf)__A, (__v4sf)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpunord_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_cmpunordps ((__v4sf)__A, (__v4sf)__B);
}




extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comieq_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comieq ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comilt_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comilt ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comile_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comile ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comigt_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comigt ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comige_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comige ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comineq_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_comineq ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomieq_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomieq ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomilt_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomilt ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomile_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomile ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomigt_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomigt ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomige_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomige ((__v4sf)__A, (__v4sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomineq_ss (__m128 __A, __m128 __B)
{
  return __builtin_ia32_ucomineq ((__v4sf)__A, (__v4sf)__B);
}



extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_si32 (__m128 __A)
{
  return __builtin_ia32_cvtss2si ((__v4sf) __A);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_ss2si (__m128 __A)
{
  return _mm_cvtss_si32 (__A);
}






extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_si64 (__m128 __A)
{
  return __builtin_ia32_cvtss2si64 ((__v4sf) __A);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_si64x (__m128 __A)
{
  return __builtin_ia32_cvtss2si64 ((__v4sf) __A);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_pi32 (__m128 __A)
{
  return (__m64) __builtin_ia32_cvtps2pi ((__v4sf) __A);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_ps2pi (__m128 __A)
{
  return _mm_cvtps_pi32 (__A);
}


extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_si32 (__m128 __A)
{
  return __builtin_ia32_cvttss2si ((__v4sf) __A);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_ss2si (__m128 __A)
{
  return _mm_cvttss_si32 (__A);
}





extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_si64 (__m128 __A)
{
  return __builtin_ia32_cvttss2si64 ((__v4sf) __A);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_si64x (__m128 __A)
{
  return __builtin_ia32_cvttss2si64 ((__v4sf) __A);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttps_pi32 (__m128 __A)
{
  return (__m64) __builtin_ia32_cvttps2pi ((__v4sf) __A);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_ps2pi (__m128 __A)
{
  return _mm_cvttps_pi32 (__A);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi32_ss (__m128 __A, int __B)
{
  return (__m128) __builtin_ia32_cvtsi2ss ((__v4sf) __A, __B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_si2ss (__m128 __A, int __B)
{
  return _mm_cvtsi32_ss (__A, __B);
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_ss (__m128 __A, long long __B)
{
  return (__m128) __builtin_ia32_cvtsi642ss ((__v4sf) __A, __B);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64x_ss (__m128 __A, long long __B)
{
  return (__m128) __builtin_ia32_cvtsi642ss ((__v4sf) __A, __B);
}




extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpi32_ps (__m128 __A, __m64 __B)
{
  return (__m128) __builtin_ia32_cvtpi2ps ((__v4sf) __A, (__v2si)__B);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_pi2ps (__m128 __A, __m64 __B)
{
  return _mm_cvtpi32_ps (__A, __B);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpi16_ps (__m64 __A)
{
  __v4hi __sign;
  __v2si __hisi, __losi;
  __v4sf __zero, __ra, __rb;




  __sign = __builtin_ia32_pcmpgtw ((__v4hi)0LL, (__v4hi)__A);


  __losi = (__v2si) __builtin_ia32_punpcklwd ((__v4hi)__A, __sign);
  __hisi = (__v2si) __builtin_ia32_punpckhwd ((__v4hi)__A, __sign);


  __zero = (__v4sf) _mm_setzero_ps ();
  __ra = __builtin_ia32_cvtpi2ps (__zero, __losi);
  __rb = __builtin_ia32_cvtpi2ps (__ra, __hisi);

  return (__m128) __builtin_ia32_movlhps (__ra, __rb);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpu16_ps (__m64 __A)
{
  __v2si __hisi, __losi;
  __v4sf __zero, __ra, __rb;


  __losi = (__v2si) __builtin_ia32_punpcklwd ((__v4hi)__A, (__v4hi)0LL);
  __hisi = (__v2si) __builtin_ia32_punpckhwd ((__v4hi)__A, (__v4hi)0LL);


  __zero = (__v4sf) _mm_setzero_ps ();
  __ra = __builtin_ia32_cvtpi2ps (__zero, __losi);
  __rb = __builtin_ia32_cvtpi2ps (__ra, __hisi);

  return (__m128) __builtin_ia32_movlhps (__ra, __rb);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpi8_ps (__m64 __A)
{
  __v8qi __sign;




  __sign = __builtin_ia32_pcmpgtb ((__v8qi)0LL, (__v8qi)__A);


  __A = (__m64) __builtin_ia32_punpcklbw ((__v8qi)__A, __sign);

  return _mm_cvtpi16_ps(__A);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpu8_ps(__m64 __A)
{
  __A = (__m64) __builtin_ia32_punpcklbw ((__v8qi)__A, (__v8qi)0LL);
  return _mm_cvtpu16_ps(__A);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpi32x2_ps(__m64 __A, __m64 __B)
{
  __v4sf __zero = (__v4sf) _mm_setzero_ps ();
  __v4sf __sfa = __builtin_ia32_cvtpi2ps (__zero, (__v2si)__A);
  __v4sf __sfb = __builtin_ia32_cvtpi2ps (__sfa, (__v2si)__B);
  return (__m128) __builtin_ia32_movlhps (__sfa, __sfb);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_pi16(__m128 __A)
{
  __v4sf __hisf = (__v4sf)__A;
  __v4sf __losf = __builtin_ia32_movhlps (__hisf, __hisf);
  __v2si __hisi = __builtin_ia32_cvtps2pi (__hisf);
  __v2si __losi = __builtin_ia32_cvtps2pi (__losf);
  return (__m64) __builtin_ia32_packssdw (__hisi, __losi);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_pi8(__m128 __A)
{
  __v4hi __tmp = (__v4hi) _mm_cvtps_pi16 (__A);
  return (__m64) __builtin_ia32_packsswb (__tmp, (__v4hi)0LL);
}



extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_ps (__m128 __A, __m128 __B, int const __mask)
{
  return (__m128) __builtin_ia32_shufps ((__v4sf)__A, (__v4sf)__B, __mask);
}







extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpckhps ((__v4sf)__A, (__v4sf)__B);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpcklps ((__v4sf)__A, (__v4sf)__B);
}



extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadh_pi (__m128 __A, __m64 const *__P)
{
  return (__m128) __builtin_ia32_loadhps ((__v4sf)__A, (const __v2sf *)__P);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeh_pi (__m64 *__P, __m128 __A)
{
  __builtin_ia32_storehps ((__v2sf *)__P, (__v4sf)__A);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movehl_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movhlps ((__v4sf)__A, (__v4sf)__B);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movelh_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movlhps ((__v4sf)__A, (__v4sf)__B);
}



extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadl_pi (__m128 __A, __m64 const *__P)
{
  return (__m128) __builtin_ia32_loadlps ((__v4sf)__A, (const __v2sf *)__P);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storel_pi (__m64 *__P, __m128 __A)
{
  __builtin_ia32_storelps ((__v2sf *)__P, (__v4sf)__A);
}


extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movemask_ps (__m128 __A)
{
  return __builtin_ia32_movmskps ((__v4sf)__A);
}


extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_getcsr (void)
{
  return __builtin_ia32_stmxcsr ();
}


extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_GET_EXCEPTION_STATE (void)
{
  return _mm_getcsr() & 0x003f;
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_GET_EXCEPTION_MASK (void)
{
  return _mm_getcsr() & 0x1f80;
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_GET_ROUNDING_MODE (void)
{
  return _mm_getcsr() & 0x6000;
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_GET_FLUSH_ZERO_MODE (void)
{
  return _mm_getcsr() & 0x8000;
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setcsr (unsigned int __I)
{
  __builtin_ia32_ldmxcsr (__I);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_SET_EXCEPTION_STATE(unsigned int __mask)
{
  _mm_setcsr((_mm_getcsr() & ~0x003f) | __mask);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_SET_EXCEPTION_MASK (unsigned int __mask)
{
  _mm_setcsr((_mm_getcsr() & ~0x1f80) | __mask);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_SET_ROUNDING_MODE (unsigned int __mode)
{
  _mm_setcsr((_mm_getcsr() & ~0x6000) | __mode);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_MM_SET_FLUSH_ZERO_MODE (unsigned int __mode)
{
  _mm_setcsr((_mm_getcsr() & ~0x8000) | __mode);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_ss (float __F)
{
  return __extension__ (__m128)(__v4sf){ __F, 0.0f, 0.0f, 0.0f };
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_ps (float __F)
{
  return __extension__ (__m128)(__v4sf){ __F, __F, __F, __F };
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_ps1 (float __F)
{
  return _mm_set1_ps (__F);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_ss (float const *__P)
{
  return _mm_set_ss (*__P);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load1_ps (float const *__P)
{
  return _mm_set1_ps (*__P);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_ps1 (float const *__P)
{
  return _mm_load1_ps (__P);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_ps (float const *__P)
{
  return *(__m128 *)__P;
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadu_ps (float const *__P)
{
  return *(__m128_u *)__P;
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadr_ps (float const *__P)
{
  __v4sf __tmp = *(__v4sf *)__P;
  return (__m128) __builtin_ia32_shufps (__tmp, __tmp, (((0) << 6) | ((1) << 4) | ((2) << 2) | (3)));
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_ps (const float __Z, const float __Y, const float __X, const float __W)
{
  return __extension__ (__m128)(__v4sf){ __W, __X, __Y, __Z };
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_ps (float __Z, float __Y, float __X, float __W)
{
  return __extension__ (__m128)(__v4sf){ __Z, __Y, __X, __W };
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_ss (float *__P, __m128 __A)
{
  *__P = ((__v4sf)__A)[0];
}

extern __inline float __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_f32 (__m128 __A)
{
  return ((__v4sf)__A)[0];
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_ps (float *__P, __m128 __A)
{
  *(__m128 *)__P = __A;
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_ps (float *__P, __m128 __A)
{
  *(__m128_u *)__P = __A;
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store1_ps (float *__P, __m128 __A)
{
  __v4sf __va = (__v4sf)__A;
  __v4sf __tmp = __builtin_ia32_shufps (__va, __va, (((0) << 6) | ((0) << 4) | ((0) << 2) | (0)));
  _mm_storeu_ps (__P, __tmp);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_ps1 (float *__P, __m128 __A)
{
  _mm_store1_ps (__P, __A);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storer_ps (float *__P, __m128 __A)
{
  __v4sf __va = (__v4sf)__A;
  __v4sf __tmp = __builtin_ia32_shufps (__va, __va, (((0) << 6) | ((1) << 4) | ((2) << 2) | (3)));
  _mm_store_ps (__P, __tmp);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_move_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_shuffle ((__v4sf)__A, (__v4sf)__B,
                                     __extension__
                                     (__attribute__((__vector_size__ (16))) int)
                                     {4,1,2,3});
}



extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_pi16 (__m64 const __A, int const __N)
{
  return __builtin_ia32_vec_ext_v4hi ((__v4hi)__A, __N);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pextrw (__m64 const __A, int const __N)
{
  return _mm_extract_pi16 (__A, __N);
}
# 1047 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_pi16 (__m64 const __A, int const __D, int const __N)
{
  return (__m64) __builtin_ia32_vec_set_v4hi ((__v4hi)__A, __D, __N);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pinsrw (__m64 const __A, int const __D, int const __N)
{
  return _mm_insert_pi16 (__A, __D, __N);
}
# 1067 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_pi16 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pmaxsw ((__v4hi)__A, (__v4hi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmaxsw (__m64 __A, __m64 __B)
{
  return _mm_max_pi16 (__A, __B);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_pu8 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pmaxub ((__v8qi)__A, (__v8qi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmaxub (__m64 __A, __m64 __B)
{
  return _mm_max_pu8 (__A, __B);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_pi16 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pminsw ((__v4hi)__A, (__v4hi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pminsw (__m64 __A, __m64 __B)
{
  return _mm_min_pi16 (__A, __B);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_pu8 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pminub ((__v8qi)__A, (__v8qi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pminub (__m64 __A, __m64 __B)
{
  return _mm_min_pu8 (__A, __B);
}


extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movemask_pi8 (__m64 __A)
{
  return __builtin_ia32_pmovmskb ((__v8qi)__A);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmovmskb (__m64 __A)
{
  return _mm_movemask_pi8 (__A);
}



extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhi_pu16 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pmulhuw ((__v4hi)__A, (__v4hi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pmulhuw (__m64 __A, __m64 __B)
{
  return _mm_mulhi_pu16 (__A, __B);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_pi16 (__m64 __A, int const __N)
{
  return (__m64) __builtin_ia32_pshufw ((__v4hi)__A, __N);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pshufw (__m64 __A, int const __N)
{
  return _mm_shuffle_pi16 (__A, __N);
}
# 1169 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskmove_si64 (__m64 __A, __m64 __N, char *__P)
{



  typedef long long __v2di __attribute__ ((__vector_size__ (16)));
  typedef char __v16qi __attribute__ ((__vector_size__ (16)));

  __v2di __A128 = __extension__ (__v2di) { ((__v1di) __A)[0], 0 };
  __v2di __N128 = __extension__ (__v2di) { ((__v1di) __N)[0], 0 };


  long long unsigned int offset = ((long long unsigned int) __P) & 0xf;
  if (offset)
    {


      if (offset > 8)
 offset = 8;
      __P = (char *) (((long long unsigned int) __P) - offset);


      switch (offset)
 {
 case 1:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 8);
   break;
 case 2:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 2 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 2 * 8);
   break;
 case 3:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 3 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 3 * 8);
   break;
 case 4:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 4 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 4 * 8);
   break;
 case 5:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 5 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 5 * 8);
   break;
 case 6:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 6 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 6 * 8);
   break;
 case 7:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 7 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 7 * 8);
   break;
 case 8:
   __A128 = __builtin_ia32_pslldqi128 (__A128, 8 * 8);
   __N128 = __builtin_ia32_pslldqi128 (__N128, 8 * 8);
   break;
 default:
   break;
 }
    }
  __builtin_ia32_maskmovdqu ((__v16qi)__A128, (__v16qi)__N128, __P);



}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_maskmovq (__m64 __A, __m64 __N, char *__P)
{
  _mm_maskmove_si64 (__A, __N, __P);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_avg_pu8 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pavgb ((__v8qi)__A, (__v8qi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pavgb (__m64 __A, __m64 __B)
{
  return _mm_avg_pu8 (__A, __B);
}


extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_avg_pu16 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_pavgw ((__v4hi)__A, (__v4hi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_pavgw (__m64 __A, __m64 __B)
{
  return _mm_avg_pu16 (__A, __B);
}




extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sad_pu8 (__m64 __A, __m64 __B)
{
  return (__m64) __builtin_ia32_psadbw ((__v8qi)__A, (__v8qi)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_psadbw (__m64 __A, __m64 __B)
{
  return _mm_sad_pu8 (__A, __B);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_pi (__m64 *__P, __m64 __A)
{
  __builtin_ia32_movntq ((unsigned long long *)__P, (unsigned long long)__A);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_ps (float *__P, __m128 __A)
{
  __builtin_ia32_movntps (__P, (__v4sf)__A);
}



extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sfence (void)
{
  __builtin_ia32_sfence ();
}
# 1324 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 1 3 4
# 1325 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 2 3 4
# 1336 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xmmintrin.h" 3 4
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_pause (void)
{
  __builtin_ia32_pause ();
}
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 2 3 4
# 40 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 3 4
typedef double __v2df __attribute__ ((__vector_size__ (16)));
typedef long long __v2di __attribute__ ((__vector_size__ (16)));
typedef unsigned long long __v2du __attribute__ ((__vector_size__ (16)));
typedef int __v4si __attribute__ ((__vector_size__ (16)));
typedef unsigned int __v4su __attribute__ ((__vector_size__ (16)));
typedef short __v8hi __attribute__ ((__vector_size__ (16)));
typedef unsigned short __v8hu __attribute__ ((__vector_size__ (16)));
typedef char __v16qi __attribute__ ((__vector_size__ (16)));
typedef signed char __v16qs __attribute__ ((__vector_size__ (16)));
typedef unsigned char __v16qu __attribute__ ((__vector_size__ (16)));



typedef long long __m128i __attribute__ ((__vector_size__ (16), __may_alias__));
typedef double __m128d __attribute__ ((__vector_size__ (16), __may_alias__));


typedef long long __m128i_u __attribute__ ((__vector_size__ (16), __may_alias__, __aligned__ (1)));
typedef double __m128d_u __attribute__ ((__vector_size__ (16), __may_alias__, __aligned__ (1)));






extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_sd (double __F)
{
  return __extension__ (__m128d){ __F, 0.0 };
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_pd (double __F)
{
  return __extension__ (__m128d){ __F, __F };
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pd1 (double __F)
{
  return _mm_set1_pd (__F);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_pd (double __W, double __X)
{
  return __extension__ (__m128d){ __X, __W };
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_pd (double __W, double __X)
{
  return __extension__ (__m128d){ __W, __X };
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_undefined_pd (void)
{
  __m128d __Y = __Y;
  return __Y;
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setzero_pd (void)
{
  return __extension__ (__m128d){ 0.0, 0.0 };
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_move_sd (__m128d __A, __m128d __B)
{
  return __extension__ (__m128d) __builtin_shuffle ((__v2df)__A, (__v2df)__B, (__v2di){2, 1});
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_pd (double const *__P)
{
  return *(__m128d *)__P;
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadu_pd (double const *__P)
{
  return *(__m128d_u *)__P;
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load1_pd (double const *__P)
{
  return _mm_set1_pd (*__P);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_sd (double const *__P)
{
  return _mm_set_sd (*__P);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_pd1 (double const *__P)
{
  return _mm_load1_pd (__P);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadr_pd (double const *__P)
{
  __m128d __tmp = _mm_load_pd (__P);
  return __builtin_ia32_shufpd (__tmp, __tmp, (((0) << 1) | (1)));
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_pd (double *__P, __m128d __A)
{
  *(__m128d *)__P = __A;
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_pd (double *__P, __m128d __A)
{
  *(__m128d_u *)__P = __A;
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_sd (double *__P, __m128d __A)
{
  *__P = ((__v2df)__A)[0];
}

extern __inline double __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_f64 (__m128d __A)
{
  return ((__v2df)__A)[0];
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storel_pd (double *__P, __m128d __A)
{
  _mm_store_sd (__P, __A);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeh_pd (double *__P, __m128d __A)
{
  *__P = ((__v2df)__A)[1];
}



extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store1_pd (double *__P, __m128d __A)
{
  _mm_store_pd (__P, __builtin_ia32_shufpd (__A, __A, (((0) << 1) | (0))));
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_pd1 (double *__P, __m128d __A)
{
  _mm_store1_pd (__P, __A);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storer_pd (double *__P, __m128d __A)
{
  _mm_store_pd (__P, __builtin_ia32_shufpd (__A, __A, (((0) << 1) | (1))));
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi128_si32 (__m128i __A)
{
  return __builtin_ia32_vec_ext_v4si ((__v4si)__A, 0);
}



extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi128_si64 (__m128i __A)
{
  return ((__v2di)__A)[0];
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi128_si64x (__m128i __A)
{
  return ((__v2di)__A)[0];
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_pd (__m128d __A, __m128d __B)
{
  return (__m128d) ((__v2df)__A + (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_addsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_pd (__m128d __A, __m128d __B)
{
  return (__m128d) ((__v2df)__A - (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_subsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_pd (__m128d __A, __m128d __B)
{
  return (__m128d) ((__v2df)__A * (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_mulsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_pd (__m128d __A, __m128d __B)
{
  return (__m128d) ((__v2df)__A / (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_divsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_pd (__m128d __A)
{
  return (__m128d)__builtin_ia32_sqrtpd ((__v2df)__A);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_sd (__m128d __A, __m128d __B)
{
  __v2df __tmp = __builtin_ia32_movsd ((__v2df)__A, (__v2df)__B);
  return (__m128d)__builtin_ia32_sqrtsd ((__v2df)__tmp);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_minpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_minsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_maxpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_maxsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_and_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_andpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_andnot_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_andnpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_orpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_xorpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpeqpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpltpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmplepd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpgtpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpgepd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpneqpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnlt_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpnltpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnle_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpnlepd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpngt_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpngtpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnge_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpngepd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpord_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpordpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpunord_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpunordpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpeqsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpltsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmplesd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movsd ((__v2df) __A,
      (__v2df)
      __builtin_ia32_cmpltsd ((__v2df) __B,
         (__v2df)
         __A));
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movsd ((__v2df) __A,
      (__v2df)
      __builtin_ia32_cmplesd ((__v2df) __B,
         (__v2df)
         __A));
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpneqsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnlt_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpnltsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnle_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpnlesd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpngt_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movsd ((__v2df) __A,
      (__v2df)
      __builtin_ia32_cmpnltsd ((__v2df) __B,
          (__v2df)
          __A));
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpnge_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movsd ((__v2df) __A,
      (__v2df)
      __builtin_ia32_cmpnlesd ((__v2df) __B,
          (__v2df)
          __A));
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpord_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpordsd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpunord_sd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_cmpunordsd ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comieq_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdeq ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comilt_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdlt ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comile_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdle ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comigt_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdgt ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comige_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdge ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_comineq_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_comisdneq ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomieq_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdeq ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomilt_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdlt ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomile_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdle ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomigt_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdgt ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomige_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdge ((__v2df)__A, (__v2df)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_ucomineq_sd (__m128d __A, __m128d __B)
{
  return __builtin_ia32_ucomisdneq ((__v2df)__A, (__v2df)__B);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_epi64x (long long __q1, long long __q0)
{
  return __extension__ (__m128i)(__v2di){ __q0, __q1 };
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_epi64 (__m64 __q1, __m64 __q0)
{
  return _mm_set_epi64x ((long long)__q1, (long long)__q0);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_epi32 (int __q3, int __q2, int __q1, int __q0)
{
  return __extension__ (__m128i)(__v4si){ __q0, __q1, __q2, __q3 };
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_epi16 (short __q7, short __q6, short __q5, short __q4,
        short __q3, short __q2, short __q1, short __q0)
{
  return __extension__ (__m128i)(__v8hi){
    __q0, __q1, __q2, __q3, __q4, __q5, __q6, __q7 };
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set_epi8 (char __q15, char __q14, char __q13, char __q12,
       char __q11, char __q10, char __q09, char __q08,
       char __q07, char __q06, char __q05, char __q04,
       char __q03, char __q02, char __q01, char __q00)
{
  return __extension__ (__m128i)(__v16qi){
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,
    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15
  };
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_epi64x (long long __A)
{
  return _mm_set_epi64x (__A, __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_epi64 (__m64 __A)
{
  return _mm_set_epi64 (__A, __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_epi32 (int __A)
{
  return _mm_set_epi32 (__A, __A, __A, __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_epi16 (short __A)
{
  return _mm_set_epi16 (__A, __A, __A, __A, __A, __A, __A, __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_set1_epi8 (char __A)
{
  return _mm_set_epi8 (__A, __A, __A, __A, __A, __A, __A, __A,
         __A, __A, __A, __A, __A, __A, __A, __A);
}




extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_epi64 (__m64 __q0, __m64 __q1)
{
  return _mm_set_epi64 (__q1, __q0);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_epi32 (int __q0, int __q1, int __q2, int __q3)
{
  return _mm_set_epi32 (__q3, __q2, __q1, __q0);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_epi16 (short __q0, short __q1, short __q2, short __q3,
         short __q4, short __q5, short __q6, short __q7)
{
  return _mm_set_epi16 (__q7, __q6, __q5, __q4, __q3, __q2, __q1, __q0);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setr_epi8 (char __q00, char __q01, char __q02, char __q03,
        char __q04, char __q05, char __q06, char __q07,
        char __q08, char __q09, char __q10, char __q11,
        char __q12, char __q13, char __q14, char __q15)
{
  return _mm_set_epi8 (__q15, __q14, __q13, __q12, __q11, __q10, __q09, __q08,
         __q07, __q06, __q05, __q04, __q03, __q02, __q01, __q00);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_load_si128 (__m128i const *__P)
{
  return *__P;
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadu_si128 (__m128i_u const *__P)
{
  return *__P;
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadl_epi64 (__m128i_u const *__P)
{
  return _mm_set_epi64 ((__m64)0LL, *(__m64_u *)__P);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadu_si64 (void const *__P)
{
  return _mm_loadl_epi64 ((__m128i_u *)__P);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_si128 (__m128i *__P, __m128i __B)
{
  *__P = __B;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_si128 (__m128i_u *__P, __m128i __B)
{
  *__P = __B;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storel_epi64 (__m128i_u *__P, __m128i __B)
{
  *(__m64_u *)__P = (__m64) ((__v2di)__B)[0];
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_si64 (void *__P, __m128i __B)
{
  _mm_storel_epi64 ((__m128i_u *)__P, __B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movepi64_pi64 (__m128i __B)
{
  return (__m64) ((__v2di)__B)[0];
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movpi64_epi64 (__m64 __A)
{
  return _mm_set_epi64 ((__m64)0LL, __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_move_epi64 (__m128i __A)
{
  return (__m128i)__builtin_ia32_movq128 ((__v2di) __A);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_undefined_si128 (void)
{
  __m128i __Y = __Y;
  return __Y;
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_setzero_si128 (void)
{
  return __extension__ (__m128i)(__v4si){ 0, 0, 0, 0 };
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi32_pd (__m128i __A)
{
  return (__m128d)__builtin_ia32_cvtdq2pd ((__v4si) __A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi32_ps (__m128i __A)
{
  return (__m128)__builtin_ia32_cvtdq2ps ((__v4si) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_epi32 (__m128d __A)
{
  return (__m128i)__builtin_ia32_cvtpd2dq ((__v2df) __A);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_pi32 (__m128d __A)
{
  return (__m64)__builtin_ia32_cvtpd2pi ((__v2df) __A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_ps (__m128d __A)
{
  return (__m128)__builtin_ia32_cvtpd2ps ((__v2df) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttpd_epi32 (__m128d __A)
{
  return (__m128i)__builtin_ia32_cvttpd2dq ((__v2df) __A);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttpd_pi32 (__m128d __A)
{
  return (__m64)__builtin_ia32_cvttpd2pi ((__v2df) __A);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpi32_pd (__m64 __A)
{
  return (__m128d)__builtin_ia32_cvtpi2pd ((__v2si) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_epi32 (__m128 __A)
{
  return (__m128i)__builtin_ia32_cvtps2dq ((__v4sf) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttps_epi32 (__m128 __A)
{
  return (__m128i)__builtin_ia32_cvttps2dq ((__v4sf) __A);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_pd (__m128 __A)
{
  return (__m128d)__builtin_ia32_cvtps2pd ((__v4sf) __A);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_si32 (__m128d __A)
{
  return __builtin_ia32_cvtsd2si ((__v2df) __A);
}



extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_si64 (__m128d __A)
{
  return __builtin_ia32_cvtsd2si64 ((__v2df) __A);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_si64x (__m128d __A)
{
  return __builtin_ia32_cvtsd2si64 ((__v2df) __A);
}


extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_si32 (__m128d __A)
{
  return __builtin_ia32_cvttsd2si ((__v2df) __A);
}



extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_si64 (__m128d __A)
{
  return __builtin_ia32_cvttsd2si64 ((__v2df) __A);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_si64x (__m128d __A)
{
  return __builtin_ia32_cvttsd2si64 ((__v2df) __A);
}


extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_ss (__m128 __A, __m128d __B)
{
  return (__m128)__builtin_ia32_cvtsd2ss ((__v4sf) __A, (__v2df) __B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi32_sd (__m128d __A, int __B)
{
  return (__m128d)__builtin_ia32_cvtsi2sd ((__v2df) __A, __B);
}



extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_sd (__m128d __A, long long __B)
{
  return (__m128d)__builtin_ia32_cvtsi642sd ((__v2df) __A, __B);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64x_sd (__m128d __A, long long __B)
{
  return (__m128d)__builtin_ia32_cvtsi642sd ((__v2df) __A, __B);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_sd (__m128d __A, __m128 __B)
{
  return (__m128d)__builtin_ia32_cvtss2sd ((__v2df) __A, (__v4sf)__B);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_pd(__m128d __A, __m128d __B, const int __mask)
{
  return (__m128d)__builtin_ia32_shufpd ((__v2df)__A, (__v2df)__B, __mask);
}






extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_unpckhpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_pd (__m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_unpcklpd ((__v2df)__A, (__v2df)__B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadh_pd (__m128d __A, double const *__B)
{
  return (__m128d)__builtin_ia32_loadhpd ((__v2df)__A, __B);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loadl_pd (__m128d __A, double const *__B)
{
  return (__m128d)__builtin_ia32_loadlpd ((__v2df)__A, __B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movemask_pd (__m128d __A)
{
  return __builtin_ia32_movmskpd ((__v2df)__A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packs_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_packsswb128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packs_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_packssdw128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packus_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_packuswb128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpckhbw128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpckhwd128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpckhdq128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpackhi_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpckhqdq128 ((__v2di)__A, (__v2di)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpcklbw128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpcklwd128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpckldq128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_unpacklo_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_punpcklqdq128 ((__v2di)__A, (__v2di)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v16qu)__A + (__v16qu)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hu)__A + (__v8hu)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4su)__A + (__v4su)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A + (__v2du)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_paddsb128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_paddsw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_paddusb128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_adds_epu16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_paddusw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v16qu)__A - (__v16qu)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hu)__A - (__v8hu)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4su)__A - (__v4su)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A - (__v2du)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psubsb128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psubsw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psubusb128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_subs_epu16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psubusw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_madd_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmaddwd128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhi_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmulhw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mullo_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hu)__A * (__v8hu)__B);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_su32 (__m64 __A, __m64 __B)
{
  return (__m64)__builtin_ia32_pmuludq ((__v2si)__A, (__v2si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmuludq128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_epi16 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psllwi128 ((__v8hi)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_epi32 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_pslldi128 ((__v4si)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_epi64 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psllqi128 ((__v2di)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srai_epi16 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psrawi128 ((__v8hi)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srai_epi32 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psradi128 ((__v4si)__A, __B);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_bsrli_si128 (__m128i __A, const int __N)
{
  return (__m128i)__builtin_ia32_psrldqi128 (__A, __N * 8);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_bslli_si128 (__m128i __A, const int __N)
{
  return (__m128i)__builtin_ia32_pslldqi128 (__A, __N * 8);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_si128 (__m128i __A, const int __N)
{
  return (__m128i)__builtin_ia32_psrldqi128 (__A, __N * 8);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_slli_si128 (__m128i __A, const int __N)
{
  return (__m128i)__builtin_ia32_pslldqi128 (__A, __N * 8);
}
# 1218 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_epi16 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psrlwi128 ((__v8hi)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_epi32 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psrldi128 ((__v4si)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srli_epi64 (__m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_psrlqi128 ((__v2di)__A, __B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psllw128((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pslld128((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sll_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psllq128((__v2di)__A, (__v2di)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sra_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psraw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sra_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrad128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrlw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrld128 ((__v4si)__A, (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_srl_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrlq128 ((__v2di)__A, (__v2di)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_and_si128 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A & (__v2du)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_andnot_si128 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pandn128 ((__v2di)__A, (__v2di)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_si128 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A | (__v2du)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_si128 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A ^ (__v2du)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v16qi)__A == (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hi)__A == (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4si)__A == (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v16qs)__A < (__v16qs)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hi)__A < (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4si)__A < (__v4si)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v16qs)__A > (__v16qs)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v8hi)__A > (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4si)__A > (__v4si)__B);
}


extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_epi16 (__m128i const __A, int const __N)
{
  return (unsigned short) __builtin_ia32_vec_ext_v8hi ((__v8hi)__A, __N);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_epi16 (__m128i const __A, int const __D, int const __N)
{
  return (__m128i) __builtin_ia32_vec_set_v8hi ((__v8hi)__A, __D, __N);
}
# 1382 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmaxsw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmaxub128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pminsw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pminub128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movemask_epi8 (__m128i __A)
{
  return __builtin_ia32_pmovmskb128 ((__v16qi)__A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhi_epu16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pmulhuw128 ((__v8hi)__A, (__v8hi)__B);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shufflehi_epi16 (__m128i __A, const int __mask)
{
  return (__m128i)__builtin_ia32_pshufhw ((__v8hi)__A, __mask);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shufflelo_epi16 (__m128i __A, const int __mask)
{
  return (__m128i)__builtin_ia32_pshuflw ((__v8hi)__A, __mask);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_epi32 (__m128i __A, const int __mask)
{
  return (__m128i)__builtin_ia32_pshufd ((__v4si)__A, __mask);
}
# 1445 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/emmintrin.h" 3 4
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskmoveu_si128 (__m128i __A, __m128i __B, char *__C)
{
  __builtin_ia32_maskmovdqu ((__v16qi)__A, (__v16qi)__B, __C);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_avg_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pavgb128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_avg_epu16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_pavgw128 ((__v8hi)__A, (__v8hi)__B);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sad_epu8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psadbw128 ((__v16qi)__A, (__v16qi)__B);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_si32 (int *__A, int __B)
{
  __builtin_ia32_movnti (__A, __B);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_si64 (long long int *__A, long long int __B)
{
  __builtin_ia32_movnti64 (__A, __B);
}


extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_si128 (__m128i *__A, __m128i __B)
{
  __builtin_ia32_movntdq ((__v2di *)__A, (__v2di)__B);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_pd (double *__A, __m128d __B)
{
  __builtin_ia32_movntpd (__A, (__v2df)__B);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_clflush (void const *__A)
{
  __builtin_ia32_clflush (__A);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_lfence (void)
{
  __builtin_ia32_lfence ();
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mfence (void)
{
  __builtin_ia32_mfence ();
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi32_si128 (int __A)
{
  return _mm_set_epi32 (0, 0, 0, __A);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64_si128 (long long __A)
{
  return _mm_set_epi64x (0, __A);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsi64x_si128 (long long __A)
{
  return _mm_set_epi64x (0, __A);
}




extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castpd_ps(__m128d __A)
{
  return (__m128) __A;
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castpd_si128(__m128d __A)
{
  return (__m128i) __A;
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castps_pd(__m128 __A)
{
  return (__m128d) __A;
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castps_si128(__m128 __A)
{
  return (__m128i) __A;
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castsi128_ps(__m128i __A)
{
  return (__m128) __A;
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_castsi128_pd(__m128i __A)
{
  return (__m128d) __A;
}
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pmmintrin.h" 2 3 4
# 49 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pmmintrin.h" 3 4
extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_addsub_ps (__m128 __X, __m128 __Y)
{
  return (__m128) __builtin_ia32_addsubps ((__v4sf)__X, (__v4sf)__Y);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_ps (__m128 __X, __m128 __Y)
{
  return (__m128) __builtin_ia32_haddps ((__v4sf)__X, (__v4sf)__Y);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_ps (__m128 __X, __m128 __Y)
{
  return (__m128) __builtin_ia32_hsubps ((__v4sf)__X, (__v4sf)__Y);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movehdup_ps (__m128 __X)
{
  return (__m128) __builtin_ia32_movshdup ((__v4sf)__X);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_moveldup_ps (__m128 __X)
{
  return (__m128) __builtin_ia32_movsldup ((__v4sf)__X);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_addsub_pd (__m128d __X, __m128d __Y)
{
  return (__m128d) __builtin_ia32_addsubpd ((__v2df)__X, (__v2df)__Y);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_pd (__m128d __X, __m128d __Y)
{
  return (__m128d) __builtin_ia32_haddpd ((__v2df)__X, (__v2df)__Y);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_pd (__m128d __X, __m128d __Y)
{
  return (__m128d) __builtin_ia32_hsubpd ((__v2df)__X, (__v2df)__Y);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_loaddup_pd (double const *__P)
{
  return _mm_load1_pd (__P);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movedup_pd (__m128d __X)
{
  return _mm_shuffle_pd (__X, __X, (((0) << 1) | (0)));
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_lddqu_si128 (__m128i const *__P)
{
  return (__m128i) __builtin_ia32_lddqu ((char const *)__P);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_monitor (void const * __P, unsigned int __E, unsigned int __H)
{
  __builtin_ia32_monitor (__P, __E, __H);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mwait (unsigned int __E, unsigned int __H)
{
  __builtin_ia32_mwait (__E, __H);
}
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/tmmintrin.h" 2 3 4







extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phaddw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phaddd128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadds_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phaddsw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phaddw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadd_pi32 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phaddd ((__v2si)__X, (__v2si)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hadds_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phaddsw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phsubw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phsubd128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsubs_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_phsubsw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phsubw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsub_pi32 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phsubd ((__v2si)__X, (__v2si)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_hsubs_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_phsubsw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maddubs_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaddubsw128 ((__v16qi)__X, (__v16qi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maddubs_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_pmaddubsw ((__v8qi)__X, (__v8qi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhrs_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmulhrsw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhrs_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_pmulhrsw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pshufb128 ((__v16qi)__X, (__v16qi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_pi8 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_pshufb ((__v8qi)__X, (__v8qi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psignb128 ((__v16qi)__X, (__v16qi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_epi16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psignw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psignd128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_pi8 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_psignb ((__v8qi)__X, (__v8qi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_pi16 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_psignw ((__v4hi)__X, (__v4hi)__Y);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_sign_pi32 (__m64 __X, __m64 __Y)
{
  return (__m64) __builtin_ia32_psignd ((__v2si)__X, (__v2si)__Y);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_alignr_epi8(__m128i __X, __m128i __Y, const int __N)
{
  return (__m128i) __builtin_ia32_palignr128 ((__v2di)__X,
           (__v2di)__Y, __N * 8);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_alignr_pi8(__m64 __X, __m64 __Y, const int __N)
{
  return (__m64) __builtin_ia32_palignr ((__v1di)__X,
      (__v1di)__Y, __N * 8);
}
# 208 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/tmmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_epi8 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pabsb128 ((__v16qi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_epi16 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pabsw128 ((__v8hi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pabsd128 ((__v4si)__X);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_pi8 (__m64 __X)
{
  return (__m64) __builtin_ia32_pabsb ((__v8qi)__X);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_pi16 (__m64 __X)
{
  return (__m64) __builtin_ia32_pabsw ((__v4hi)__X);
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_pi32 (__m64 __X)
{
  return (__m64) __builtin_ia32_pabsd ((__v2si)__X);
}
# 33 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 2 3 4
# 66 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testz_si128 (__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestz128 ((__v2di)__M, (__v2di)__V);
}



extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testc_si128 (__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestc128 ((__v2di)__M, (__v2di)__V);
}



extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testnzc_si128 (__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestnzc128 ((__v2di)__M, (__v2di)__V);
}
# 99 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_round_pd (__m128d __V, const int __M)
{
  return (__m128d) __builtin_ia32_roundpd ((__v2df)__V, __M);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_round_sd(__m128d __D, __m128d __V, const int __M)
{
  return (__m128d) __builtin_ia32_roundsd ((__v2df)__D,
        (__v2df)__V,
        __M);
}
# 124 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_round_ps (__m128 __V, const int __M)
{
  return (__m128) __builtin_ia32_roundps ((__v4sf)__V, __M);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_round_ss (__m128 __D, __m128 __V, const int __M)
{
  return (__m128) __builtin_ia32_roundss ((__v4sf)__D,
       (__v4sf)__V,
       __M);
}
# 165 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blend_epi16 (__m128i __X, __m128i __Y, const int __M)
{
  return (__m128i) __builtin_ia32_pblendw128 ((__v8hi)__X,
           (__v8hi)__Y,
           __M);
}






extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blendv_epi8 (__m128i __X, __m128i __Y, __m128i __M)
{
  return (__m128i) __builtin_ia32_pblendvb128 ((__v16qi)__X,
            (__v16qi)__Y,
            (__v16qi)__M);
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blend_ps (__m128 __X, __m128 __Y, const int __M)
{
  return (__m128) __builtin_ia32_blendps ((__v4sf)__X,
       (__v4sf)__Y,
       __M);
}






extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blendv_ps (__m128 __X, __m128 __Y, __m128 __M)
{
  return (__m128) __builtin_ia32_blendvps ((__v4sf)__X,
        (__v4sf)__Y,
        (__v4sf)__M);
}





extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blend_pd (__m128d __X, __m128d __Y, const int __M)
{
  return (__m128d) __builtin_ia32_blendpd ((__v2df)__X,
        (__v2df)__Y,
        __M);
}






extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_blendv_pd (__m128d __X, __m128d __Y, __m128d __M)
{
  return (__m128d) __builtin_ia32_blendvpd ((__v2df)__X,
         (__v2df)__Y,
         (__v2df)__M);
}





extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dp_ps (__m128 __X, __m128 __Y, const int __M)
{
  return (__m128) __builtin_ia32_dpps ((__v4sf)__X,
           (__v4sf)__Y,
           __M);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dp_pd (__m128d __X, __m128d __Y, const int __M)
{
  return (__m128d) __builtin_ia32_dppd ((__v2df)__X,
     (__v2df)__Y,
     __M);
}
# 267 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi64 (__m128i __X, __m128i __Y)
{
  return (__m128i) ((__v2di)__X == (__v2di)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pminsb128 ((__v16qi)__X, (__v16qi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaxsb128 ((__v16qi)__X, (__v16qi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epu16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pminuw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epu16 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaxuw128 ((__v8hi)__X, (__v8hi)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pminsd128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaxsd128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epu32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pminud128 ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epu32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaxud128 ((__v4si)__X, (__v4si)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mullo_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) ((__v4su)__X * (__v4su)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmuldq128 ((__v4si)__X, (__v4si)__Y);
}







extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_ps (__m128 __D, __m128 __S, const int __N)
{
  return (__m128) __builtin_ia32_insertps128 ((__v4sf)__D,
           (__v4sf)__S,
           __N);
}
# 365 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_ps (__m128 __X, const int __N)
{
  union { int i; float f; } __tmp;
  __tmp.f = __builtin_ia32_vec_ext_v4sf ((__v4sf)__X, __N);
  return __tmp.i;
}
# 398 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_epi8 (__m128i __D, int __S, const int __N)
{
  return (__m128i) __builtin_ia32_vec_set_v16qi ((__v16qi)__D,
       __S, __N);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_epi32 (__m128i __D, int __S, const int __N)
{
  return (__m128i) __builtin_ia32_vec_set_v4si ((__v4si)__D,
       __S, __N);
}


extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_insert_epi64 (__m128i __D, long long __S, const int __N)
{
  return (__m128i) __builtin_ia32_vec_set_v2di ((__v2di)__D,
       __S, __N);
}
# 440 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_epi8 (__m128i __X, const int __N)
{
   return (unsigned char) __builtin_ia32_vec_ext_v16qi ((__v16qi)__X, __N);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_epi32 (__m128i __X, const int __N)
{
   return __builtin_ia32_vec_ext_v4si ((__v4si)__X, __N);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_extract_epi64 (__m128i __X, const int __N)
{
  return __builtin_ia32_vec_ext_v2di ((__v2di)__X, __N);
}
# 473 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_minpos_epu16 (__m128i __X)
{
  return (__m128i) __builtin_ia32_phminposuw128 ((__v8hi)__X);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi8_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxbd128 ((__v16qi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi16_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxwd128 ((__v8hi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi8_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxbq128 ((__v16qi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi32_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxdq128 ((__v4si)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi16_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxwq128 ((__v8hi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi8_epi16 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxbw128 ((__v16qi)__X);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu8_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxbd128 ((__v16qi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu16_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxwd128 ((__v8hi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu8_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxbq128 ((__v16qi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu32_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxdq128 ((__v4si)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu16_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxwq128 ((__v8hi)__X);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu8_epi16 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxbw128 ((__v16qi)__X);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_packus_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_packusdw128 ((__v4si)__X, (__v4si)__Y);
}






extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mpsadbw_epu8 (__m128i __X, __m128i __Y, const int __M)
{
  return (__m128i) __builtin_ia32_mpsadbw128 ((__v16qi)__X,
           (__v16qi)__Y, __M);
}







extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_stream_load_si128 (__m128i *__X)
{
  return (__m128i) __builtin_ia32_movntdqa ((__v2di *) __X);
}
# 622 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistrm (__m128i __X, __m128i __Y, const int __M)
{
  return (__m128i) __builtin_ia32_pcmpistrm128 ((__v16qi)__X,
      (__v16qi)__Y,
      __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistri (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistri128 ((__v16qi)__X,
          (__v16qi)__Y,
          __M);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestrm (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return (__m128i) __builtin_ia32_pcmpestrm128 ((__v16qi)__X, __LX,
      (__v16qi)__Y, __LY,
      __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestri (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestri128 ((__v16qi)__X, __LX,
          (__v16qi)__Y, __LY,
          __M);
}
# 675 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistra (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistria128 ((__v16qi)__X,
           (__v16qi)__Y,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistrc (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistric128 ((__v16qi)__X,
           (__v16qi)__Y,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistro (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistrio128 ((__v16qi)__X,
           (__v16qi)__Y,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistrs (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistris128 ((__v16qi)__X,
           (__v16qi)__Y,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpistrz (__m128i __X, __m128i __Y, const int __M)
{
  return __builtin_ia32_pcmpistriz128 ((__v16qi)__X,
           (__v16qi)__Y,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestra (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestria128 ((__v16qi)__X, __LX,
           (__v16qi)__Y, __LY,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestrc (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestric128 ((__v16qi)__X, __LX,
           (__v16qi)__Y, __LY,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestro (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestrio128 ((__v16qi)__X, __LX,
           (__v16qi)__Y, __LY,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestrs (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestris128 ((__v16qi)__X, __LX,
           (__v16qi)__Y, __LY,
           __M);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpestrz (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)
{
  return __builtin_ia32_pcmpestriz128 ((__v16qi)__X, __LX,
           (__v16qi)__Y, __LY,
           __M);
}
# 795 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi64 (__m128i __X, __m128i __Y)
{
  return (__m128i) ((__v2di)__X > (__v2di)__Y);
}
# 811 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/popcntintrin.h" 1 3 4
# 34 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/popcntintrin.h" 3 4
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_u32 (unsigned int __X)
{
  return __builtin_popcount (__X);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_u64 (unsigned long long __X)
{
  return __builtin_popcountll (__X);
}
# 812 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 2 3 4
# 826 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/smmintrin.h" 3 4
extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_crc32_u8 (unsigned int __C, unsigned char __V)
{
  return __builtin_ia32_crc32qi (__C, __V);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_crc32_u16 (unsigned int __C, unsigned short __V)
{
  return __builtin_ia32_crc32hi (__C, __V);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_crc32_u32 (unsigned int __C, unsigned int __V)
{
  return __builtin_ia32_crc32si (__C, __V);
}


extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_crc32_u64 (unsigned long long __C, unsigned long long __V)
{
  return __builtin_ia32_crc32di (__C, __V);
}
# 20 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/wmmintrin.h" 1 3 4
# 43 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/wmmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aesdec_si128 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_aesdec128 ((__v2di)__X, (__v2di)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aesdeclast_si128 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_aesdeclast128 ((__v2di)__X,
       (__v2di)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aesenc_si128 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_aesenc128 ((__v2di)__X, (__v2di)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aesenclast_si128 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_aesenclast128 ((__v2di)__X, (__v2di)__Y);
}



extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aesimc_si128 (__m128i __X)
{
  return (__m128i) __builtin_ia32_aesimc128 ((__v2di)__X);
}





extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_aeskeygenassist_si128 (__m128i __X, const int __C)
{
  return (__m128i) __builtin_ia32_aeskeygenassist128 ((__v2di)__X, __C);
}
# 115 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/wmmintrin.h" 3 4
extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_clmulepi64_si128 (__m128i __X, __m128i __Y, const int __I)
{
  return (__m128i) __builtin_ia32_pclmulqdq128 ((__v2di)__X,
      (__v2di)__Y, __I);
}
# 21 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 1 3 4
# 41 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 3 4
# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/fxsrintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/fxsrintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_fxsave (void *__P)
{
  __builtin_ia32_fxsave (__P);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_fxrstor (void *__P)
{
  __builtin_ia32_fxrstor (__P);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_fxsave64 (void *__P)
{
  __builtin_ia32_fxsave64 (__P);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_fxrstor64 (void *__P)
{
  __builtin_ia32_fxrstor64 (__P);
}
# 42 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsaveintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsaveintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsave (void *__P, long long __M)
{
  __builtin_ia32_xsave (__P, __M);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xrstor (void *__P, long long __M)
{
  __builtin_ia32_xrstor (__P, __M);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsetbv (unsigned int __A, long long __V)
{
  __builtin_ia32_xsetbv (__A, __V);
}

extern __inline long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xgetbv (unsigned int __A)
{
  return __builtin_ia32_xgetbv (__A);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsave64 (void *__P, long long __M)
{
  __builtin_ia32_xsave64 (__P, __M);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xrstor64 (void *__P, long long __M)
{
  __builtin_ia32_xrstor64 (__P, __M);
}
# 44 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsaveoptintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsaveoptintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsaveopt (void *__P, long long __M)
{
  __builtin_ia32_xsaveopt (__P, __M);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsaveopt64 (void *__P, long long __M)
{
  __builtin_ia32_xsaveopt64 (__P, __M);
}
# 46 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsavesintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsavesintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsaves (void *__P, long long __M)
{
  __builtin_ia32_xsaves (__P, __M);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xrstors (void *__P, long long __M)
{
  __builtin_ia32_xrstors (__P, __M);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xrstors64 (void *__P, long long __M)
{
  __builtin_ia32_xrstors64 (__P, __M);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsaves64 (void *__P, long long __M)
{
  __builtin_ia32_xsaves64 (__P, __M);
}
# 48 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsavecintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xsavecintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsavec (void *__P, long long __M)
{
  __builtin_ia32_xsavec (__P, __M);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xsavec64 (void *__P, long long __M)
{
  __builtin_ia32_xsavec64 (__P, __M);
}
# 50 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 1 3 4
# 41 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
typedef double __v4df __attribute__ ((__vector_size__ (32)));
typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef long long __v4di __attribute__ ((__vector_size__ (32)));
typedef unsigned long long __v4du __attribute__ ((__vector_size__ (32)));
typedef int __v8si __attribute__ ((__vector_size__ (32)));
typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));
typedef short __v16hi __attribute__ ((__vector_size__ (32)));
typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));
typedef char __v32qi __attribute__ ((__vector_size__ (32)));
typedef signed char __v32qs __attribute__ ((__vector_size__ (32)));
typedef unsigned char __v32qu __attribute__ ((__vector_size__ (32)));



typedef float __m256 __attribute__ ((__vector_size__ (32),
         __may_alias__));
typedef long long __m256i __attribute__ ((__vector_size__ (32),
       __may_alias__));
typedef double __m256d __attribute__ ((__vector_size__ (32),
           __may_alias__));


typedef float __m256_u __attribute__ ((__vector_size__ (32),
           __may_alias__,
           __aligned__ (1)));
typedef long long __m256i_u __attribute__ ((__vector_size__ (32),
         __may_alias__,
         __aligned__ (1)));
typedef double __m256d_u __attribute__ ((__vector_size__ (32),
      __may_alias__,
      __aligned__ (1)));
# 140 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_pd (__m256d __A, __m256d __B)
{
  return (__m256d) ((__v4df)__A + (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_ps (__m256 __A, __m256 __B)
{
  return (__m256) ((__v8sf)__A + (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_addsub_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_addsubpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_addsub_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_addsubps256 ((__v8sf)__A, (__v8sf)__B);
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_and_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_andpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_and_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_andps256 ((__v8sf)__A, (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_andnot_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_andnpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_andnot_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_andnps256 ((__v8sf)__A, (__v8sf)__B);
}





extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blend_pd (__m256d __X, __m256d __Y, const int __M)
{
  return (__m256d) __builtin_ia32_blendpd256 ((__v4df)__X,
           (__v4df)__Y,
           __M);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blend_ps (__m256 __X, __m256 __Y, const int __M)
{
  return (__m256) __builtin_ia32_blendps256 ((__v8sf)__X,
          (__v8sf)__Y,
          __M);
}
# 218 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blendv_pd (__m256d __X, __m256d __Y, __m256d __M)
{
  return (__m256d) __builtin_ia32_blendvpd256 ((__v4df)__X,
            (__v4df)__Y,
            (__v4df)__M);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blendv_ps (__m256 __X, __m256 __Y, __m256 __M)
{
  return (__m256) __builtin_ia32_blendvps256 ((__v8sf)__X,
           (__v8sf)__Y,
           (__v8sf)__M);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_div_pd (__m256d __A, __m256d __B)
{
  return (__m256d) ((__v4df)__A / (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_div_ps (__m256 __A, __m256 __B)
{
  return (__m256) ((__v8sf)__A / (__v8sf)__B);
}





extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dp_ps (__m256 __X, __m256 __Y, const int __M)
{
  return (__m256) __builtin_ia32_dpps256 ((__v8sf)__X,
       (__v8sf)__Y,
       __M);
}






extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hadd_pd (__m256d __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_haddpd256 ((__v4df)__X, (__v4df)__Y);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hadd_ps (__m256 __X, __m256 __Y)
{
  return (__m256) __builtin_ia32_haddps256 ((__v8sf)__X, (__v8sf)__Y);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hsub_pd (__m256d __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_hsubpd256 ((__v4df)__X, (__v4df)__Y);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hsub_ps (__m256 __X, __m256 __Y)
{
  return (__m256) __builtin_ia32_hsubps256 ((__v8sf)__X, (__v8sf)__Y);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_maxpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_maxps256 ((__v8sf)__A, (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_minpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_minps256 ((__v8sf)__A, (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mul_pd (__m256d __A, __m256d __B)
{
  return (__m256d) ((__v4df)__A * (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mul_ps (__m256 __A, __m256 __B)
{
  return (__m256) ((__v8sf)__A * (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_or_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_orpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_or_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_orps256 ((__v8sf)__A, (__v8sf)__B);
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_pd (__m256d __A, __m256d __B, const int __mask)
{
  return (__m256d) __builtin_ia32_shufpd256 ((__v4df)__A, (__v4df)__B,
          __mask);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_ps (__m256 __A, __m256 __B, const int __mask)
{
  return (__m256) __builtin_ia32_shufps256 ((__v8sf)__A, (__v8sf)__B,
         __mask);
}
# 359 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_pd (__m256d __A, __m256d __B)
{
  return (__m256d) ((__v4df)__A - (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_ps (__m256 __A, __m256 __B)
{
  return (__m256) ((__v8sf)__A - (__v8sf)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_xor_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_xorpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_xor_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_xorps256 ((__v8sf)__A, (__v8sf)__B);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_pd (__m128d __X, __m128d __Y, const int __P)
{
  return (__m128d) __builtin_ia32_cmppd ((__v2df)__X, (__v2df)__Y, __P);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_ps (__m128 __X, __m128 __Y, const int __P)
{
  return (__m128) __builtin_ia32_cmpps ((__v4sf)__X, (__v4sf)__Y, __P);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_pd (__m256d __X, __m256d __Y, const int __P)
{
  return (__m256d) __builtin_ia32_cmppd256 ((__v4df)__X, (__v4df)__Y,
         __P);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_ps (__m256 __X, __m256 __Y, const int __P)
{
  return (__m256) __builtin_ia32_cmpps256 ((__v8sf)__X, (__v8sf)__Y,
        __P);
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_sd (__m128d __X, __m128d __Y, const int __P)
{
  return (__m128d) __builtin_ia32_cmpsd ((__v2df)__X, (__v2df)__Y, __P);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_ss (__m128 __X, __m128 __Y, const int __P)
{
  return (__m128) __builtin_ia32_cmpss ((__v4sf)__X, (__v4sf)__Y, __P);
}
# 447 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi32_pd (__m128i __A)
{
  return (__m256d)__builtin_ia32_cvtdq2pd256 ((__v4si) __A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi32_ps (__m256i __A)
{
  return (__m256)__builtin_ia32_cvtdq2ps256 ((__v8si) __A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtpd_ps (__m256d __A)
{
  return (__m128)__builtin_ia32_cvtpd2ps256 ((__v4df) __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_epi32 (__m256 __A)
{
  return (__m256i)__builtin_ia32_cvtps2dq256 ((__v8sf) __A);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_pd (__m128 __A)
{
  return (__m256d)__builtin_ia32_cvtps2pd256 ((__v4sf) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttpd_epi32 (__m256d __A)
{
  return (__m128i)__builtin_ia32_cvttpd2dq256 ((__v4df) __A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtpd_epi32 (__m256d __A)
{
  return (__m128i)__builtin_ia32_cvtpd2dq256 ((__v4df) __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttps_epi32 (__m256 __A)
{
  return (__m256i)__builtin_ia32_cvttps2dq256 ((__v8sf) __A);
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsd_f64 (__m256d __A)
{
  return __A[0];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtss_f32 (__m256 __A)
{
  return __A[0];
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extractf128_pd (__m256d __X, const int __N)
{
  return (__m128d) __builtin_ia32_vextractf128_pd256 ((__v4df)__X, __N);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extractf128_ps (__m256 __X, const int __N)
{
  return (__m128) __builtin_ia32_vextractf128_ps256 ((__v8sf)__X, __N);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extractf128_si256 (__m256i __X, const int __N)
{
  return (__m128i) __builtin_ia32_vextractf128_si256 ((__v8si)__X, __N);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extract_epi32 (__m256i __X, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 2);
  return _mm_extract_epi32 (__Y, __N % 4);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extract_epi16 (__m256i __X, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 3);
  return _mm_extract_epi16 (__Y, __N % 8);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extract_epi8 (__m256i __X, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 4);
  return _mm_extract_epi8 (__Y, __N % 16);
}


extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extract_epi64 (__m256i __X, const int __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 1);
  return _mm_extract_epi64 (__Y, __N % 2);
}
# 601 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_zeroall (void)
{
  __builtin_ia32_vzeroall ();
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_zeroupper (void)
{
  __builtin_ia32_vzeroupper ();
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutevar_pd (__m128d __A, __m128i __C)
{
  return (__m128d) __builtin_ia32_vpermilvarpd ((__v2df)__A,
      (__v2di)__C);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutevar_pd (__m256d __A, __m256i __C)
{
  return (__m256d) __builtin_ia32_vpermilvarpd256 ((__v4df)__A,
         (__v4di)__C);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutevar_ps (__m128 __A, __m128i __C)
{
  return (__m128) __builtin_ia32_vpermilvarps ((__v4sf)__A,
            (__v4si)__C);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutevar_ps (__m256 __A, __m256i __C)
{
  return (__m256) __builtin_ia32_vpermilvarps256 ((__v8sf)__A,
        (__v8si)__C);
}


extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_permute_pd (__m128d __X, const int __C)
{
  return (__m128d) __builtin_ia32_vpermilpd ((__v2df)__X, __C);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute_pd (__m256d __X, const int __C)
{
  return (__m256d) __builtin_ia32_vpermilpd256 ((__v4df)__X, __C);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_permute_ps (__m128 __X, const int __C)
{
  return (__m128) __builtin_ia32_vpermilps ((__v4sf)__X, __C);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute_ps (__m256 __X, const int __C)
{
  return (__m256) __builtin_ia32_vpermilps256 ((__v8sf)__X, __C);
}
# 680 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute2f128_pd (__m256d __X, __m256d __Y, const int __C)
{
  return (__m256d) __builtin_ia32_vperm2f128_pd256 ((__v4df)__X,
          (__v4df)__Y,
          __C);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute2f128_ps (__m256 __X, __m256 __Y, const int __C)
{
  return (__m256) __builtin_ia32_vperm2f128_ps256 ((__v8sf)__X,
         (__v8sf)__Y,
         __C);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute2f128_si256 (__m256i __X, __m256i __Y, const int __C)
{
  return (__m256i) __builtin_ia32_vperm2f128_si256 ((__v8si)__X,
          (__v8si)__Y,
          __C);
}
# 720 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcast_ss (float const *__X)
{
  return (__m128) __builtin_ia32_vbroadcastss (__X);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_sd (double const *__X)
{
  return (__m256d) __builtin_ia32_vbroadcastsd256 (__X);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_ss (float const *__X)
{
  return (__m256) __builtin_ia32_vbroadcastss256 (__X);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_pd (__m128d const *__X)
{
  return (__m256d) __builtin_ia32_vbroadcastf128_pd256 (__X);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_ps (__m128 const *__X)
{
  return (__m256) __builtin_ia32_vbroadcastf128_ps256 (__X);
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insertf128_pd (__m256d __X, __m128d __Y, const int __O)
{
  return (__m256d) __builtin_ia32_vinsertf128_pd256 ((__v4df)__X,
           (__v2df)__Y,
           __O);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insertf128_ps (__m256 __X, __m128 __Y, const int __O)
{
  return (__m256) __builtin_ia32_vinsertf128_ps256 ((__v8sf)__X,
          (__v4sf)__Y,
          __O);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insertf128_si256 (__m256i __X, __m128i __Y, const int __O)
{
  return (__m256i) __builtin_ia32_vinsertf128_si256 ((__v8si)__X,
           (__v4si)__Y,
           __O);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insert_epi32 (__m256i __X, int __D, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 2);
  __Y = _mm_insert_epi32 (__Y, __D, __N % 4);
  return _mm256_insertf128_si256 (__X, __Y, __N >> 2);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insert_epi16 (__m256i __X, int __D, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 3);
  __Y = _mm_insert_epi16 (__Y, __D, __N % 8);
  return _mm256_insertf128_si256 (__X, __Y, __N >> 3);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insert_epi8 (__m256i __X, int __D, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 4);
  __Y = _mm_insert_epi8 (__Y, __D, __N % 16);
  return _mm256_insertf128_si256 (__X, __Y, __N >> 4);
}


extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insert_epi64 (__m256i __X, long long __D, int const __N)
{
  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 1);
  __Y = _mm_insert_epi64 (__Y, __D, __N % 2);
  return _mm256_insertf128_si256 (__X, __Y, __N >> 1);
}
# 859 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_load_pd (double const *__P)
{
  return *(__m256d *)__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_store_pd (double *__P, __m256d __A)
{
  *(__m256d *)__P = __A;
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_load_ps (float const *__P)
{
  return *(__m256 *)__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_store_ps (float *__P, __m256 __A)
{
  *(__m256 *)__P = __A;
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu_pd (double const *__P)
{
  return *(__m256d_u *)__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu_pd (double *__P, __m256d __A)
{
  *(__m256d_u *)__P = __A;
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu_ps (float const *__P)
{
  return *(__m256_u *)__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu_ps (float *__P, __m256 __A)
{
  *(__m256_u *)__P = __A;
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_load_si256 (__m256i const *__P)
{
  return *__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_store_si256 (__m256i *__P, __m256i __A)
{
  *__P = __A;
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu_si256 (__m256i_u const *__P)
{
  return *__P;
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu_si256 (__m256i_u *__P, __m256i __A)
{
  *__P = __A;
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskload_pd (double const *__P, __m128i __M)
{
  return (__m128d) __builtin_ia32_maskloadpd ((const __v2df *)__P,
           (__v2di)__M);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskstore_pd (double *__P, __m128i __M, __m128d __A)
{
  __builtin_ia32_maskstorepd ((__v2df *)__P, (__v2di)__M, (__v2df)__A);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskload_pd (double const *__P, __m256i __M)
{
  return (__m256d) __builtin_ia32_maskloadpd256 ((const __v4df *)__P,
       (__v4di)__M);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskstore_pd (double *__P, __m256i __M, __m256d __A)
{
  __builtin_ia32_maskstorepd256 ((__v4df *)__P, (__v4di)__M, (__v4df)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskload_ps (float const *__P, __m128i __M)
{
  return (__m128) __builtin_ia32_maskloadps ((const __v4sf *)__P,
          (__v4si)__M);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskstore_ps (float *__P, __m128i __M, __m128 __A)
{
  __builtin_ia32_maskstoreps ((__v4sf *)__P, (__v4si)__M, (__v4sf)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskload_ps (float const *__P, __m256i __M)
{
  return (__m256) __builtin_ia32_maskloadps256 ((const __v8sf *)__P,
      (__v8si)__M);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskstore_ps (float *__P, __m256i __M, __m256 __A)
{
  __builtin_ia32_maskstoreps256 ((__v8sf *)__P, (__v8si)__M, (__v8sf)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movehdup_ps (__m256 __X)
{
  return (__m256) __builtin_ia32_movshdup256 ((__v8sf)__X);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_moveldup_ps (__m256 __X)
{
  return (__m256) __builtin_ia32_movsldup256 ((__v8sf)__X);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movedup_pd (__m256d __X)
{
  return (__m256d) __builtin_ia32_movddup256 ((__v4df)__X);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_lddqu_si256 (__m256i const *__P)
{
  return (__m256i) __builtin_ia32_lddqu256 ((char const *)__P);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_stream_si256 (__m256i *__A, __m256i __B)
{
  __builtin_ia32_movntdq256 ((__v4di *)__A, (__v4di)__B);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_stream_pd (double *__A, __m256d __B)
{
  __builtin_ia32_movntpd256 (__A, (__v4df)__B);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_stream_ps (float *__P, __m256 __A)
{
  __builtin_ia32_movntps256 (__P, (__v8sf)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rcp_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rcpps256 ((__v8sf)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rsqrt_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rsqrtps256 ((__v8sf)__A);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sqrt_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_sqrtpd256 ((__v4df)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sqrt_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_sqrtps256 ((__v8sf)__A);
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_round_pd (__m256d __V, const int __M)
{
  return (__m256d) __builtin_ia32_roundpd256 ((__v4df)__V, __M);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_round_ps (__m256 __V, const int __M)
{
  return (__m256) __builtin_ia32_roundps256 ((__v8sf)__V, __M);
}
# 1074 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avxintrin.h" 3 4
extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_unpckhpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_unpcklpd256 ((__v4df)__A, (__v4df)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_unpckhps256 ((__v8sf)__A, (__v8sf)__B);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_unpcklps256 ((__v8sf)__A, (__v8sf)__B);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testz_pd (__m128d __M, __m128d __V)
{
  return __builtin_ia32_vtestzpd ((__v2df)__M, (__v2df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testc_pd (__m128d __M, __m128d __V)
{
  return __builtin_ia32_vtestcpd ((__v2df)__M, (__v2df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testnzc_pd (__m128d __M, __m128d __V)
{
  return __builtin_ia32_vtestnzcpd ((__v2df)__M, (__v2df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testz_ps (__m128 __M, __m128 __V)
{
  return __builtin_ia32_vtestzps ((__v4sf)__M, (__v4sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testc_ps (__m128 __M, __m128 __V)
{
  return __builtin_ia32_vtestcps ((__v4sf)__M, (__v4sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_testnzc_ps (__m128 __M, __m128 __V)
{
  return __builtin_ia32_vtestnzcps ((__v4sf)__M, (__v4sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testz_pd (__m256d __M, __m256d __V)
{
  return __builtin_ia32_vtestzpd256 ((__v4df)__M, (__v4df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testc_pd (__m256d __M, __m256d __V)
{
  return __builtin_ia32_vtestcpd256 ((__v4df)__M, (__v4df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testnzc_pd (__m256d __M, __m256d __V)
{
  return __builtin_ia32_vtestnzcpd256 ((__v4df)__M, (__v4df)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testz_ps (__m256 __M, __m256 __V)
{
  return __builtin_ia32_vtestzps256 ((__v8sf)__M, (__v8sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testc_ps (__m256 __M, __m256 __V)
{
  return __builtin_ia32_vtestcps256 ((__v8sf)__M, (__v8sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testnzc_ps (__m256 __M, __m256 __V)
{
  return __builtin_ia32_vtestnzcps256 ((__v8sf)__M, (__v8sf)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testz_si256 (__m256i __M, __m256i __V)
{
  return __builtin_ia32_ptestz256 ((__v4di)__M, (__v4di)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testc_si256 (__m256i __M, __m256i __V)
{
  return __builtin_ia32_ptestc256 ((__v4di)__M, (__v4di)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testnzc_si256 (__m256i __M, __m256i __V)
{
  return __builtin_ia32_ptestnzc256 ((__v4di)__M, (__v4di)__V);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movemask_pd (__m256d __A)
{
  return __builtin_ia32_movmskpd256 ((__v4df)__A);
}

extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movemask_ps (__m256 __A)
{
  return __builtin_ia32_movmskps256 ((__v8sf)__A);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_undefined_pd (void)
{
  __m256d __Y = __Y;
  return __Y;
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_undefined_ps (void)
{
  __m256 __Y = __Y;
  return __Y;
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_undefined_si256 (void)
{
  __m256i __Y = __Y;
  return __Y;
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setzero_pd (void)
{
  return __extension__ (__m256d){ 0.0, 0.0, 0.0, 0.0 };
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setzero_ps (void)
{
  return __extension__ (__m256){ 0.0, 0.0, 0.0, 0.0,
     0.0, 0.0, 0.0, 0.0 };
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setzero_si256 (void)
{
  return __extension__ (__m256i)(__v4di){ 0, 0, 0, 0 };
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_pd (double __A, double __B, double __C, double __D)
{
  return __extension__ (__m256d){ __D, __C, __B, __A };
}


extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_ps (float __A, float __B, float __C, float __D,
        float __E, float __F, float __G, float __H)
{
  return __extension__ (__m256){ __H, __G, __F, __E,
     __D, __C, __B, __A };
}


extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_epi32 (int __A, int __B, int __C, int __D,
    int __E, int __F, int __G, int __H)
{
  return __extension__ (__m256i)(__v8si){ __H, __G, __F, __E,
       __D, __C, __B, __A };
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_epi16 (short __q15, short __q14, short __q13, short __q12,
    short __q11, short __q10, short __q09, short __q08,
    short __q07, short __q06, short __q05, short __q04,
    short __q03, short __q02, short __q01, short __q00)
{
  return __extension__ (__m256i)(__v16hi){
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,
    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15
  };
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_epi8 (char __q31, char __q30, char __q29, char __q28,
    char __q27, char __q26, char __q25, char __q24,
    char __q23, char __q22, char __q21, char __q20,
    char __q19, char __q18, char __q17, char __q16,
    char __q15, char __q14, char __q13, char __q12,
    char __q11, char __q10, char __q09, char __q08,
    char __q07, char __q06, char __q05, char __q04,
    char __q03, char __q02, char __q01, char __q00)
{
  return __extension__ (__m256i)(__v32qi){
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,
    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15,
    __q16, __q17, __q18, __q19, __q20, __q21, __q22, __q23,
    __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31
  };
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_epi64x (long long __A, long long __B, long long __C,
     long long __D)
{
  return __extension__ (__m256i)(__v4di){ __D, __C, __B, __A };
}


extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_pd (double __A)
{
  return __extension__ (__m256d){ __A, __A, __A, __A };
}


extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_ps (float __A)
{
  return __extension__ (__m256){ __A, __A, __A, __A,
     __A, __A, __A, __A };
}


extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_epi32 (int __A)
{
  return __extension__ (__m256i)(__v8si){ __A, __A, __A, __A,
       __A, __A, __A, __A };
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_epi16 (short __A)
{
  return _mm256_set_epi16 (__A, __A, __A, __A, __A, __A, __A, __A,
      __A, __A, __A, __A, __A, __A, __A, __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_epi8 (char __A)
{
  return _mm256_set_epi8 (__A, __A, __A, __A, __A, __A, __A, __A,
     __A, __A, __A, __A, __A, __A, __A, __A,
     __A, __A, __A, __A, __A, __A, __A, __A,
     __A, __A, __A, __A, __A, __A, __A, __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set1_epi64x (long long __A)
{
  return __extension__ (__m256i)(__v4di){ __A, __A, __A, __A };
}




extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_pd (double __A, double __B, double __C, double __D)
{
  return _mm256_set_pd (__D, __C, __B, __A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_ps (float __A, float __B, float __C, float __D,
  float __E, float __F, float __G, float __H)
{
  return _mm256_set_ps (__H, __G, __F, __E, __D, __C, __B, __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_epi32 (int __A, int __B, int __C, int __D,
     int __E, int __F, int __G, int __H)
{
  return _mm256_set_epi32 (__H, __G, __F, __E, __D, __C, __B, __A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_epi16 (short __q15, short __q14, short __q13, short __q12,
     short __q11, short __q10, short __q09, short __q08,
     short __q07, short __q06, short __q05, short __q04,
     short __q03, short __q02, short __q01, short __q00)
{
  return _mm256_set_epi16 (__q00, __q01, __q02, __q03,
      __q04, __q05, __q06, __q07,
      __q08, __q09, __q10, __q11,
      __q12, __q13, __q14, __q15);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_epi8 (char __q31, char __q30, char __q29, char __q28,
     char __q27, char __q26, char __q25, char __q24,
     char __q23, char __q22, char __q21, char __q20,
     char __q19, char __q18, char __q17, char __q16,
     char __q15, char __q14, char __q13, char __q12,
     char __q11, char __q10, char __q09, char __q08,
     char __q07, char __q06, char __q05, char __q04,
     char __q03, char __q02, char __q01, char __q00)
{
  return _mm256_set_epi8 (__q00, __q01, __q02, __q03,
     __q04, __q05, __q06, __q07,
     __q08, __q09, __q10, __q11,
     __q12, __q13, __q14, __q15,
     __q16, __q17, __q18, __q19,
     __q20, __q21, __q22, __q23,
     __q24, __q25, __q26, __q27,
     __q28, __q29, __q30, __q31);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_epi64x (long long __A, long long __B, long long __C,
      long long __D)
{
  return _mm256_set_epi64x (__D, __C, __B, __A);
}



extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castpd_ps (__m256d __A)
{
  return (__m256) __A;
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castpd_si256 (__m256d __A)
{
  return (__m256i) __A;
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castps_pd (__m256 __A)
{
  return (__m256d) __A;
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castps_si256(__m256 __A)
{
  return (__m256i) __A;
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castsi256_ps (__m256i __A)
{
  return (__m256) __A;
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castsi256_pd (__m256i __A)
{
  return (__m256d) __A;
}

extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castpd256_pd128 (__m256d __A)
{
  return (__m128d) __builtin_ia32_pd_pd256 ((__v4df)__A);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castps256_ps128 (__m256 __A)
{
  return (__m128) __builtin_ia32_ps_ps256 ((__v8sf)__A);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castsi256_si128 (__m256i __A)
{
  return (__m128i) __builtin_ia32_si_si256 ((__v8si)__A);
}






extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castpd128_pd256 (__m128d __A)
{
  return (__m256d) __builtin_ia32_pd256_pd ((__v2df)__A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castps128_ps256 (__m128 __A)
{
  return (__m256) __builtin_ia32_ps256_ps ((__v4sf)__A);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_castsi128_si256 (__m128i __A)
{
  return (__m256i) __builtin_ia32_si256_si ((__v4si)__A);
}



extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_zextpd128_pd256 (__m128d __A)
{
  return _mm256_insertf128_pd (_mm256_setzero_pd (), __A, 0);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_zextps128_ps256 (__m128 __A)
{
  return _mm256_insertf128_ps (_mm256_setzero_ps (), __A, 0);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_zextsi128_si256 (__m128i __A)
{
  return _mm256_insertf128_si256 (_mm256_setzero_si256 (), __A, 0);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_m128 ( __m128 __H, __m128 __L)
{
  return _mm256_insertf128_ps (_mm256_castps128_ps256 (__L), __H, 1);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_m128d (__m128d __H, __m128d __L)
{
  return _mm256_insertf128_pd (_mm256_castpd128_pd256 (__L), __H, 1);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_set_m128i (__m128i __H, __m128i __L)
{
  return _mm256_insertf128_si256 (_mm256_castsi128_si256 (__L), __H, 1);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_m128 (__m128 __L, __m128 __H)
{
  return _mm256_set_m128 (__H, __L);
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_m128d (__m128d __L, __m128d __H)
{
  return _mm256_set_m128d (__H, __L);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_setr_m128i (__m128i __L, __m128i __H)
{
  return _mm256_set_m128i (__H, __L);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu2_m128 (float const *__PH, float const *__PL)
{
  return _mm256_insertf128_ps (_mm256_castps128_ps256 (_mm_loadu_ps (__PL)),
          _mm_loadu_ps (__PH), 1);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu2_m128 (float *__PH, float *__PL, __m256 __A)
{
  _mm_storeu_ps (__PL, _mm256_castps256_ps128 (__A));
  _mm_storeu_ps (__PH, _mm256_extractf128_ps (__A, 1));
}

extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu2_m128d (double const *__PH, double const *__PL)
{
  return _mm256_insertf128_pd (_mm256_castpd128_pd256 (_mm_loadu_pd (__PL)),
          _mm_loadu_pd (__PH), 1);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu2_m128d (double *__PH, double *__PL, __m256d __A)
{
  _mm_storeu_pd (__PL, _mm256_castpd256_pd128 (__A));
  _mm_storeu_pd (__PH, _mm256_extractf128_pd (__A, 1));
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_loadu2_m128i (__m128i_u const *__PH, __m128i_u const *__PL)
{
  return _mm256_insertf128_si256 (_mm256_castsi128_si256 (_mm_loadu_si128 (__PL)),
      _mm_loadu_si128 (__PH), 1);
}

extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu2_m128i (__m128i_u *__PH, __m128i_u *__PL, __m256i __A)
{
  _mm_storeu_si128 (__PL, _mm256_castsi256_si128 (__A));
  _mm_storeu_si128 (__PH, _mm256_extractf128_si256 (__A, 1));
}
# 52 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx2intrin.h" 1 3 4
# 41 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx2intrin.h" 3 4
extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mpsadbw_epu8 (__m256i __X, __m256i __Y, const int __M)
{
  return (__m256i) __builtin_ia32_mpsadbw256 ((__v32qi)__X,
           (__v32qi)__Y, __M);
}






extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_abs_epi8 (__m256i __A)
{
  return (__m256i)__builtin_ia32_pabsb256 ((__v32qi)__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_abs_epi16 (__m256i __A)
{
  return (__m256i)__builtin_ia32_pabsw256 ((__v16hi)__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_abs_epi32 (__m256i __A)
{
  return (__m256i)__builtin_ia32_pabsd256 ((__v8si)__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_packs_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_packssdw256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_packs_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_packsswb256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_packus_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_packusdw256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_packus_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_packuswb256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v32qu)__A + (__v32qu)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v16hu)__A + (__v16hu)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8su)__A + (__v8su)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_add_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A + (__v4du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_adds_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_paddsb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_adds_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_paddsw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_adds_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_paddusb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_adds_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_paddusw256 ((__v16hi)__A, (__v16hi)__B);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_alignr_epi8 (__m256i __A, __m256i __B, const int __N)
{
  return (__m256i) __builtin_ia32_palignr256 ((__v4di)__A,
           (__v4di)__B,
           __N * 8);
}
# 177 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx2intrin.h" 3 4
extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_and_si256 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A & (__v4du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_andnot_si256 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_andnotsi256 ((__v4di)__A, (__v4di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_avg_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pavgb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_avg_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pavgw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blendv_epi8 (__m256i __X, __m256i __Y, __m256i __M)
{
  return (__m256i) __builtin_ia32_pblendvb256 ((__v32qi)__X,
            (__v32qi)__Y,
            (__v32qi)__M);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blend_epi16 (__m256i __X, __m256i __Y, const int __M)
{
  return (__m256i) __builtin_ia32_pblendw256 ((__v16hi)__X,
           (__v16hi)__Y,
            __M);
}






extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v32qi)__A == (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v16hi)__A == (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8si)__A == (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4di)__A == (__v4di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v32qs)__A > (__v32qs)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v16hi)__A > (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8si)__A > (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4di)__A > (__v4di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hadd_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phaddw256 ((__v16hi)__X,
          (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hadd_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phaddd256 ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hadds_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phaddsw256 ((__v16hi)__X,
           (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hsub_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phsubw256 ((__v16hi)__X,
          (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hsub_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phsubd256 ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_hsubs_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_phsubsw256 ((__v16hi)__X,
           (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maddubs_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmaddubsw256 ((__v32qi)__X,
      (__v32qi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_madd_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaddwd256 ((__v16hi)__A,
          (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxsb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxsw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxsd256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxub256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxuw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epu32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmaxud256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminsb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminsw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminsd256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminub256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminuw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epu32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pminud256 ((__v8si)__A, (__v8si)__B);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movemask_epi8 (__m256i __A)
{
  return __builtin_ia32_pmovmskb256 ((__v32qi)__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi8_epi16 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxbw256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi8_epi32 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxbd256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi8_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxbq256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi16_epi32 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxwd256 ((__v8hi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi16_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxwq256 ((__v8hi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi32_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxdq256 ((__v4si)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu8_epi16 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxbw256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu8_epi32 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxbd256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu8_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxbq256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu16_epi32 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxwd256 ((__v8hi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu16_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxwq256 ((__v8hi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu32_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxdq256 ((__v4si)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mul_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmuldq256 ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mulhrs_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmulhrsw256 ((__v16hi)__X,
            (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mulhi_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmulhuw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mulhi_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmulhw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mullo_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v16hu)__A * (__v16hu)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mullo_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8su)__A * (__v8su)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mul_epu32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_pmuludq256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_or_si256 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A | (__v4du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sad_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psadbw256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_epi8 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pshufb256 ((__v32qi)__X,
          (__v32qi)__Y);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_epi32 (__m256i __A, const int __mask)
{
  return (__m256i)__builtin_ia32_pshufd256 ((__v8si)__A, __mask);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shufflehi_epi16 (__m256i __A, const int __mask)
{
  return (__m256i)__builtin_ia32_pshufhw256 ((__v16hi)__A, __mask);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shufflelo_epi16 (__m256i __A, const int __mask)
{
  return (__m256i)__builtin_ia32_pshuflw256 ((__v16hi)__A, __mask);
}
# 624 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx2intrin.h" 3 4
extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sign_epi8 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psignb256 ((__v32qi)__X, (__v32qi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sign_epi16 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psignw256 ((__v16hi)__X, (__v16hi)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sign_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psignd256 ((__v8si)__X, (__v8si)__Y);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_bslli_epi128 (__m256i __A, const int __N)
{
  return (__m256i)__builtin_ia32_pslldqi256 (__A, __N * 8);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_slli_si256 (__m256i __A, const int __N)
{
  return (__m256i)__builtin_ia32_pslldqi256 (__A, __N * 8);
}







extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_slli_epi16 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psllwi256 ((__v16hi)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sll_epi16 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psllw256((__v16hi)__A, (__v8hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_slli_epi32 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_pslldi256 ((__v8si)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sll_epi32 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_pslld256((__v8si)__A, (__v4si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_slli_epi64 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psllqi256 ((__v4di)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sll_epi64 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psllq256((__v4di)__A, (__v2di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srai_epi16 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psrawi256 ((__v16hi)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sra_epi16 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psraw256 ((__v16hi)__A, (__v8hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srai_epi32 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psradi256 ((__v8si)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sra_epi32 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psrad256 ((__v8si)__A, (__v4si)__B);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_bsrli_epi128 (__m256i __A, const int __N)
{
  return (__m256i)__builtin_ia32_psrldqi256 (__A, __N * 8);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srli_si256 (__m256i __A, const int __N)
{
  return (__m256i)__builtin_ia32_psrldqi256 (__A, __N * 8);
}







extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srli_epi16 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psrlwi256 ((__v16hi)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srl_epi16 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__A, (__v8hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srli_epi32 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psrldi256 ((__v8si)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srl_epi32 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psrld256((__v8si)__A, (__v4si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srli_epi64 (__m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_psrlqi256 ((__v4di)__A, __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srl_epi64 (__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psrlq256((__v4di)__A, (__v2di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v32qu)__A - (__v32qu)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v16hu)__A - (__v16hu)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8su)__A - (__v8su)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sub_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A - (__v4du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_subs_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psubsb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_subs_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psubsw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_subs_epu8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psubusb256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_subs_epu16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psubusw256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpckhbw256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpckhwd256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpckhdq256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpackhi_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpckhqdq256 ((__v4di)__A, (__v4di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpcklbw256 ((__v32qi)__A, (__v32qi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpcklwd256 ((__v16hi)__A, (__v16hi)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpckldq256 ((__v8si)__A, (__v8si)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_unpacklo_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_punpcklqdq256 ((__v4di)__A, (__v4di)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_xor_si256 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A ^ (__v4du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_stream_load_si256 (__m256i const *__X)
{
  return (__m256i) __builtin_ia32_movntdqa256 ((__v4di *) __X);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastss_ps (__m128 __X)
{
  return (__m128) __builtin_ia32_vbroadcastss_ps ((__v4sf)__X);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastss_ps (__m128 __X)
{
  return (__m256) __builtin_ia32_vbroadcastss_ps256 ((__v4sf)__X);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastsd_pd (__m128d __X)
{
  return (__m256d) __builtin_ia32_vbroadcastsd_pd256 ((__v2df)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastsi128_si256 (__m128i __X)
{
  return (__m256i) __builtin_ia32_vbroadcastsi256 ((__v2di)__X);
}


extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_blend_epi32 (__m128i __X, __m128i __Y, const int __M)
{
  return (__m128i) __builtin_ia32_pblendd128 ((__v4si)__X,
           (__v4si)__Y,
           __M);
}







extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_blend_epi32 (__m256i __X, __m256i __Y, const int __M)
{
  return (__m256i) __builtin_ia32_pblendd256 ((__v8si)__X,
           (__v8si)__Y,
           __M);
}






extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastb_epi8 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pbroadcastb256 ((__v16qi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastw_epi16 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pbroadcastw256 ((__v8hi)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastd_epi32 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pbroadcastd256 ((__v4si)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastq_epi64 (__m128i __X)
{
  return (__m256i) __builtin_ia32_pbroadcastq256 ((__v2di)__X);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastb_epi8 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pbroadcastb128 ((__v16qi)__X);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastw_epi16 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pbroadcastw128 ((__v8hi)__X);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastd_epi32 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pbroadcastd128 ((__v4si)__X);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastq_epi64 (__m128i __X)
{
  return (__m128i) __builtin_ia32_pbroadcastq128 ((__v2di)__X);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutevar8x32_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256 ((__v8si)__X, (__v8si)__Y);
}


extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute4x64_pd (__m256d __X, const int __M)
{
  return (__m256d) __builtin_ia32_permdf256 ((__v4df)__X, __M);
}





extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutevar8x32_ps (__m256 __X, __m256i __Y)
{
  return (__m256) __builtin_ia32_permvarsf256 ((__v8sf)__X, (__v8si)__Y);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute4x64_epi64 (__m256i __X, const int __M)
{
  return (__m256i) __builtin_ia32_permdi256 ((__v4di)__X, __M);
}







extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permute2x128_si256 (__m256i __X, __m256i __Y, const int __M)
{
  return (__m256i) __builtin_ia32_permti256 ((__v4di)__X, (__v4di)__Y, __M);
}






extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extracti128_si256 (__m256i __X, const int __M)
{
  return (__m128i) __builtin_ia32_extract128i256 ((__v4di)__X, __M);
}






extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_inserti128_si256 (__m256i __X, __m128i __Y, const int __M)
{
  return (__m256i) __builtin_ia32_insert128i256 ((__v4di)__X, (__v2di)__Y, __M);
}







extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskload_epi32 (int const *__X, __m256i __M )
{
  return (__m256i) __builtin_ia32_maskloadd256 ((const __v8si *)__X,
      (__v8si)__M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskload_epi64 (long long const *__X, __m256i __M )
{
  return (__m256i) __builtin_ia32_maskloadq256 ((const __v4di *)__X,
      (__v4di)__M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskload_epi32 (int const *__X, __m128i __M )
{
  return (__m128i) __builtin_ia32_maskloadd ((const __v4si *)__X,
          (__v4si)__M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskload_epi64 (long long const *__X, __m128i __M )
{
  return (__m128i) __builtin_ia32_maskloadq ((const __v2di *)__X,
          (__v2di)__M);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskstore_epi32 (int *__X, __m256i __M, __m256i __Y )
{
  __builtin_ia32_maskstored256 ((__v8si *)__X, (__v8si)__M, (__v8si)__Y);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskstore_epi64 (long long *__X, __m256i __M, __m256i __Y )
{
  __builtin_ia32_maskstoreq256 ((__v4di *)__X, (__v4di)__M, (__v4di)__Y);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskstore_epi32 (int *__X, __m128i __M, __m128i __Y )
{
  __builtin_ia32_maskstored ((__v4si *)__X, (__v4si)__M, (__v4si)__Y);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskstore_epi64 (long long *__X, __m128i __M, __m128i __Y )
{
  __builtin_ia32_maskstoreq (( __v2di *)__X, (__v2di)__M, (__v2di)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sllv_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv8si ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sllv_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv4si ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sllv_epi64 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv4di ((__v4di)__X, (__v4di)__Y);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sllv_epi64 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv2di ((__v2di)__X, (__v2di)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srav_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrav8si ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srav_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrav4si ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srlv_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv8si ((__v8si)__X, (__v8si)__Y);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srlv_epi32 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv4si ((__v4si)__X, (__v4si)__Y);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srlv_epi64 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv4di ((__v4di)__X, (__v4di)__Y);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srlv_epi64 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv2di ((__v2di)__X, (__v2di)__Y);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32gather_pd (double const *__base, __m128i __index, const int __scale)
{
  __v2df __zero = _mm_setzero_pd ();
  __v2df __mask = _mm_cmpeq_pd (__zero, __zero);

  return (__m128d) __builtin_ia32_gathersiv2df (_mm_undefined_pd (),
      __base,
      (__v4si)__index,
      __mask,
      __scale);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32gather_pd (__m128d __src, double const *__base, __m128i __index,
         __m128d __mask, const int __scale)
{
  return (__m128d) __builtin_ia32_gathersiv2df ((__v2df)__src,
      __base,
      (__v4si)__index,
      (__v2df)__mask,
      __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32gather_pd (double const *__base, __m128i __index, const int __scale)
{
  __v4df __zero = _mm256_setzero_pd ();
  __v4df __mask = _mm256_cmp_pd (__zero, __zero, 0x00);

  return (__m256d) __builtin_ia32_gathersiv4df (_mm256_undefined_pd (),
      __base,
      (__v4si)__index,
      __mask,
      __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32gather_pd (__m256d __src, double const *__base,
     __m128i __index, __m256d __mask, const int __scale)
{
  return (__m256d) __builtin_ia32_gathersiv4df ((__v4df)__src,
      __base,
      (__v4si)__index,
      (__v4df)__mask,
      __scale);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64gather_pd (double const *__base, __m128i __index, const int __scale)
{
  __v2df __src = _mm_setzero_pd ();
  __v2df __mask = _mm_cmpeq_pd (__src, __src);

  return (__m128d) __builtin_ia32_gatherdiv2df (__src,
      __base,
      (__v2di)__index,
      __mask,
      __scale);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64gather_pd (__m128d __src, double const *__base, __m128i __index,
         __m128d __mask, const int __scale)
{
  return (__m128d) __builtin_ia32_gatherdiv2df ((__v2df)__src,
      __base,
      (__v2di)__index,
      (__v2df)__mask,
      __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64gather_pd (double const *__base, __m256i __index, const int __scale)
{
  __v4df __src = _mm256_setzero_pd ();
  __v4df __mask = _mm256_cmp_pd (__src, __src, 0x00);

  return (__m256d) __builtin_ia32_gatherdiv4df (__src,
      __base,
      (__v4di)__index,
      __mask,
      __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64gather_pd (__m256d __src, double const *__base,
     __m256i __index, __m256d __mask, const int __scale)
{
  return (__m256d) __builtin_ia32_gatherdiv4df ((__v4df)__src,
      __base,
      (__v4di)__index,
      (__v4df)__mask,
      __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32gather_ps (float const *__base, __m128i __index, const int __scale)
{
  __v4sf __src = _mm_setzero_ps ();
  __v4sf __mask = _mm_cmpeq_ps (__src, __src);

  return (__m128) __builtin_ia32_gathersiv4sf (__src,
            __base,
            (__v4si)__index,
            __mask,
            __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32gather_ps (__m128 __src, float const *__base, __m128i __index,
         __m128 __mask, const int __scale)
{
  return (__m128) __builtin_ia32_gathersiv4sf ((__v4sf)__src,
            __base,
            (__v4si)__index,
            (__v4sf)__mask,
            __scale);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32gather_ps (float const *__base, __m256i __index, const int __scale)
{
  __v8sf __src = _mm256_setzero_ps ();
  __v8sf __mask = _mm256_cmp_ps (__src, __src, 0x00);

  return (__m256) __builtin_ia32_gathersiv8sf (__src,
            __base,
            (__v8si)__index,
            __mask,
            __scale);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32gather_ps (__m256 __src, float const *__base,
     __m256i __index, __m256 __mask, const int __scale)
{
  return (__m256) __builtin_ia32_gathersiv8sf ((__v8sf)__src,
            __base,
            (__v8si)__index,
            (__v8sf)__mask,
            __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64gather_ps (float const *__base, __m128i __index, const int __scale)
{
  __v4sf __src = _mm_setzero_ps ();
  __v4sf __mask = _mm_cmpeq_ps (__src, __src);

  return (__m128) __builtin_ia32_gatherdiv4sf (__src,
            __base,
            (__v2di)__index,
            __mask,
            __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64gather_ps (__m128 __src, float const *__base, __m128i __index,
         __m128 __mask, const int __scale)
{
  return (__m128) __builtin_ia32_gatherdiv4sf ((__v4sf)__src,
      __base,
      (__v2di)__index,
      (__v4sf)__mask,
      __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64gather_ps (float const *__base, __m256i __index, const int __scale)
{
  __v4sf __src = _mm_setzero_ps ();
  __v4sf __mask = _mm_cmpeq_ps (__src, __src);

  return (__m128) __builtin_ia32_gatherdiv4sf256 (__src,
        __base,
        (__v4di)__index,
        __mask,
        __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64gather_ps (__m128 __src, float const *__base,
     __m256i __index, __m128 __mask, const int __scale)
{
  return (__m128) __builtin_ia32_gatherdiv4sf256 ((__v4sf)__src,
        __base,
        (__v4di)__index,
        (__v4sf)__mask,
        __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32gather_epi64 (long long int const *__base,
       __m128i __index, const int __scale)
{
  __v2di __src = __extension__ (__v2di){ 0, 0 };
  __v2di __mask = __extension__ (__v2di){ ~0, ~0 };

  return (__m128i) __builtin_ia32_gathersiv2di (__src,
      __base,
      (__v4si)__index,
      __mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32gather_epi64 (__m128i __src, long long int const *__base,
     __m128i __index, __m128i __mask, const int __scale)
{
  return (__m128i) __builtin_ia32_gathersiv2di ((__v2di)__src,
      __base,
      (__v4si)__index,
      (__v2di)__mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32gather_epi64 (long long int const *__base,
   __m128i __index, const int __scale)
{
  __v4di __src = __extension__ (__v4di){ 0, 0, 0, 0 };
  __v4di __mask = __extension__ (__v4di){ ~0, ~0, ~0, ~0 };

  return (__m256i) __builtin_ia32_gathersiv4di (__src,
      __base,
      (__v4si)__index,
      __mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32gather_epi64 (__m256i __src, long long int const *__base,
        __m128i __index, __m256i __mask,
        const int __scale)
{
  return (__m256i) __builtin_ia32_gathersiv4di ((__v4di)__src,
      __base,
      (__v4si)__index,
      (__v4di)__mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64gather_epi64 (long long int const *__base,
       __m128i __index, const int __scale)
{
  __v2di __src = __extension__ (__v2di){ 0, 0 };
  __v2di __mask = __extension__ (__v2di){ ~0, ~0 };

  return (__m128i) __builtin_ia32_gatherdiv2di (__src,
      __base,
      (__v2di)__index,
      __mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64gather_epi64 (__m128i __src, long long int const *__base,
     __m128i __index, __m128i __mask, const int __scale)
{
  return (__m128i) __builtin_ia32_gatherdiv2di ((__v2di)__src,
      __base,
      (__v2di)__index,
      (__v2di)__mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64gather_epi64 (long long int const *__base,
   __m256i __index, const int __scale)
{
  __v4di __src = __extension__ (__v4di){ 0, 0, 0, 0 };
  __v4di __mask = __extension__ (__v4di){ ~0, ~0, ~0, ~0 };

  return (__m256i) __builtin_ia32_gatherdiv4di (__src,
      __base,
      (__v4di)__index,
      __mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64gather_epi64 (__m256i __src, long long int const *__base,
        __m256i __index, __m256i __mask,
        const int __scale)
{
  return (__m256i) __builtin_ia32_gatherdiv4di ((__v4di)__src,
      __base,
      (__v4di)__index,
      (__v4di)__mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32gather_epi32 (int const *__base, __m128i __index, const int __scale)
{
  __v4si __src = __extension__ (__v4si){ 0, 0, 0, 0 };
  __v4si __mask = __extension__ (__v4si){ ~0, ~0, ~0, ~0 };

  return (__m128i) __builtin_ia32_gathersiv4si (__src,
      __base,
      (__v4si)__index,
      __mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32gather_epi32 (__m128i __src, int const *__base, __m128i __index,
     __m128i __mask, const int __scale)
{
  return (__m128i) __builtin_ia32_gathersiv4si ((__v4si)__src,
      __base,
      (__v4si)__index,
      (__v4si)__mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32gather_epi32 (int const *__base, __m256i __index, const int __scale)
{
  __v8si __src = __extension__ (__v8si){ 0, 0, 0, 0, 0, 0, 0, 0 };
  __v8si __mask = __extension__ (__v8si){ ~0, ~0, ~0, ~0, ~0, ~0, ~0, ~0 };

  return (__m256i) __builtin_ia32_gathersiv8si (__src,
      __base,
      (__v8si)__index,
      __mask,
      __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32gather_epi32 (__m256i __src, int const *__base,
        __m256i __index, __m256i __mask,
        const int __scale)
{
  return (__m256i) __builtin_ia32_gathersiv8si ((__v8si)__src,
      __base,
      (__v8si)__index,
      (__v8si)__mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64gather_epi32 (int const *__base, __m128i __index, const int __scale)
{
  __v4si __src = __extension__ (__v4si){ 0, 0, 0, 0 };
  __v4si __mask = __extension__ (__v4si){ ~0, ~0, ~0, ~0 };

  return (__m128i) __builtin_ia32_gatherdiv4si (__src,
      __base,
      (__v2di)__index,
      __mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64gather_epi32 (__m128i __src, int const *__base, __m128i __index,
     __m128i __mask, const int __scale)
{
  return (__m128i) __builtin_ia32_gatherdiv4si ((__v4si)__src,
      __base,
      (__v2di)__index,
      (__v4si)__mask,
      __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64gather_epi32 (int const *__base, __m256i __index, const int __scale)
{
  __v4si __src = __extension__ (__v4si){ 0, 0, 0, 0 };
  __v4si __mask = __extension__ (__v4si){ ~0, ~0, ~0, ~0 };

  return (__m128i) __builtin_ia32_gatherdiv4si256 (__src,
         __base,
         (__v4di)__index,
         __mask,
         __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64gather_epi32 (__m128i __src, int const *__base,
        __m256i __index, __m128i __mask,
        const int __scale)
{
  return (__m128i) __builtin_ia32_gatherdiv4si256 ((__v4si)__src,
         __base,
         (__v4di)__index,
         (__v4si)__mask,
         __scale);
}
# 54 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512f")




typedef double __v8df __attribute__ ((__vector_size__ (64)));
typedef float __v16sf __attribute__ ((__vector_size__ (64)));
typedef long long __v8di __attribute__ ((__vector_size__ (64)));
typedef unsigned long long __v8du __attribute__ ((__vector_size__ (64)));
typedef int __v16si __attribute__ ((__vector_size__ (64)));
typedef unsigned int __v16su __attribute__ ((__vector_size__ (64)));
typedef short __v32hi __attribute__ ((__vector_size__ (64)));
typedef unsigned short __v32hu __attribute__ ((__vector_size__ (64)));
typedef char __v64qi __attribute__ ((__vector_size__ (64)));
typedef unsigned char __v64qu __attribute__ ((__vector_size__ (64)));



typedef float __m512 __attribute__ ((__vector_size__ (64), __may_alias__));
typedef long long __m512i __attribute__ ((__vector_size__ (64), __may_alias__));
typedef double __m512d __attribute__ ((__vector_size__ (64), __may_alias__));


typedef float __m512_u __attribute__ ((__vector_size__ (64), __may_alias__, __aligned__ (1)));
typedef long long __m512i_u __attribute__ ((__vector_size__ (64), __may_alias__, __aligned__ (1)));
typedef double __m512d_u __attribute__ ((__vector_size__ (64), __may_alias__, __aligned__ (1)));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_int2mask (int __M)
{
  return (__mmask16) __M;
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2int (__mmask16 __M)
{
  return (int) __M;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_epi64 (long long __A, long long __B, long long __C,
    long long __D, long long __E, long long __F,
    long long __G, long long __H)
{
  return __extension__ (__m512i) (__v8di)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_epi32 (int __A, int __B, int __C, int __D,
    int __E, int __F, int __G, int __H,
    int __I, int __J, int __K, int __L,
    int __M, int __N, int __O, int __P)
{
  return __extension__ (__m512i)(__v16si)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_epi16 (short __q31, short __q30, short __q29, short __q28,
    short __q27, short __q26, short __q25, short __q24,
    short __q23, short __q22, short __q21, short __q20,
    short __q19, short __q18, short __q17, short __q16,
    short __q15, short __q14, short __q13, short __q12,
    short __q11, short __q10, short __q09, short __q08,
    short __q07, short __q06, short __q05, short __q04,
    short __q03, short __q02, short __q01, short __q00)
{
  return __extension__ (__m512i)(__v32hi){
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,
    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15,
    __q16, __q17, __q18, __q19, __q20, __q21, __q22, __q23,
    __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31
  };
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_epi8 (char __q63, char __q62, char __q61, char __q60,
   char __q59, char __q58, char __q57, char __q56,
   char __q55, char __q54, char __q53, char __q52,
   char __q51, char __q50, char __q49, char __q48,
   char __q47, char __q46, char __q45, char __q44,
   char __q43, char __q42, char __q41, char __q40,
   char __q39, char __q38, char __q37, char __q36,
   char __q35, char __q34, char __q33, char __q32,
   char __q31, char __q30, char __q29, char __q28,
   char __q27, char __q26, char __q25, char __q24,
   char __q23, char __q22, char __q21, char __q20,
   char __q19, char __q18, char __q17, char __q16,
   char __q15, char __q14, char __q13, char __q12,
   char __q11, char __q10, char __q09, char __q08,
   char __q07, char __q06, char __q05, char __q04,
   char __q03, char __q02, char __q01, char __q00)
{
  return __extension__ (__m512i)(__v64qi){
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,
    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15,
    __q16, __q17, __q18, __q19, __q20, __q21, __q22, __q23,
    __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31,
    __q32, __q33, __q34, __q35, __q36, __q37, __q38, __q39,
    __q40, __q41, __q42, __q43, __q44, __q45, __q46, __q47,
    __q48, __q49, __q50, __q51, __q52, __q53, __q54, __q55,
    __q56, __q57, __q58, __q59, __q60, __q61, __q62, __q63
  };
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_pd (double __A, double __B, double __C, double __D,
        double __E, double __F, double __G, double __H)
{
  return __extension__ (__m512d)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set_ps (float __A, float __B, float __C, float __D,
        float __E, float __F, float __G, float __H,
        float __I, float __J, float __K, float __L,
        float __M, float __N, float __O, float __P)
{
  return __extension__ (__m512)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}
# 184 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_undefined_ps (void)
{
  __m512 __Y = __Y;
  return __Y;
}



extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_undefined_pd (void)
{
  __m512d __Y = __Y;
  return __Y;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_undefined_epi32 (void)
{
  __m512i __Y = __Y;
  return __Y;
}



extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_epi8 (char __A)
{
  return __extension__ (__m512i)(__v64qi)
  { __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A };
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_epi16 (short __A)
{
  return __extension__ (__m512i)(__v32hi)
  { __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A,
    __A, __A, __A, __A, __A, __A, __A, __A };
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_pd (double __A)
{
  return (__m512d) __builtin_ia32_broadcastsd512 (__extension__
        (__v2df) { __A, },
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_ps (float __A)
{
  return (__m512) __builtin_ia32_broadcastss512 (__extension__
       (__v4sf) { __A, },
       (__v16sf)
       _mm512_undefined_ps (),
       (__mmask16) -1);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set4_epi32 (int __A, int __B, int __C, int __D)
{
  return __extension__ (__m512i)(__v16si)
  { __D, __C, __B, __A, __D, __C, __B, __A,
    __D, __C, __B, __A, __D, __C, __B, __A };
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set4_epi64 (long long __A, long long __B, long long __C,
     long long __D)
{
  return __extension__ (__m512i) (__v8di)
  { __D, __C, __B, __A, __D, __C, __B, __A };
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set4_pd (double __A, double __B, double __C, double __D)
{
  return __extension__ (__m512d)
  { __D, __C, __B, __A, __D, __C, __B, __A };
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set4_ps (float __A, float __B, float __C, float __D)
{
  return __extension__ (__m512)
  { __D, __C, __B, __A, __D, __C, __B, __A,
    __D, __C, __B, __A, __D, __C, __B, __A };
}
# 308 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_setzero_ps (void)
{
  return __extension__ (__m512){ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_setzero (void)
{
  return _mm512_setzero_ps ();
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_setzero_pd (void)
{
  return __extension__ (__m512d) { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_setzero_epi32 (void)
{
  return __extension__ (__m512i)(__v8di){ 0, 0, 0, 0, 0, 0, 0, 0 };
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_setzero_si512 (void)
{
  return __extension__ (__m512i)(__v8di){ 0, 0, 0, 0, 0, 0, 0, 0 };
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_movapd512_mask ((__v8df) __A,
        (__v8df) __W,
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_movapd512_mask ((__v8df) __A,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movaps512_mask ((__v16sf) __A,
       (__v16sf) __W,
       (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movaps512_mask ((__v16sf) __A,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_load_pd (void const *__P)
{
  return *(__m512d *) __P;
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_load_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *) __P,
         (__v8df) __W,
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *) __P,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_store_pd (void *__P, __m512d __A)
{
  *(__m512d *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_store_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeapd512_mask ((__v8df *) __P, (__v8df) __A,
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_load_ps (void const *__P)
{
  return *(__m512 *) __P;
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_load_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *) __P,
        (__v16sf) __W,
        (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_load_ps (__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *) __P,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_store_ps (void *__P, __m512 __A)
{
  *(__m512 *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_store_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeaps512_mask ((__v16sf *) __P, (__v16sf) __A,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdqa64_512_mask ((__v8di) __A,
           (__v8di) __W,
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdqa64_512_mask ((__v8di) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_load_epi64 (void const *__P)
{
  return *(__m512i *) __P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_load_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_store_epi64 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_store_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_movdqa64store512_mask ((__v8di *) __P, (__v8di) __A,
     (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdqa32_512_mask ((__v16si) __A,
           (__v16si) __W,
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdqa32_512_mask ((__v16si) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_load_si512 (void const *__P)
{
  return *(__m512i *) __P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_load_epi32 (void const *__P)
{
  return *(__m512i *) __P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_load_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_load_epi32 (__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_store_si512 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_store_epi32 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_store_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_movdqa32store512_mask ((__v16si *) __P, (__v16si) __A,
     (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mullo_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A * (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mullo_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulld512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mullo_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulld512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mullox_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mullox_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return _mm512_mask_mov_epi64 (__W, __M, _mm512_mullox_epi64 (__A, __B));
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sllv_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sllv_epi32 (__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sllv_epi32 (__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srav_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srav_epi32 (__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srav_epi32 (__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srlv_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srlv_epi32 (__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srlv_epi32 (__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv16si_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A + (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A - (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sllv_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_undefined_pd (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sllv_epi64 (__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sllv_epi64 (__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psllv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srav_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srav_epi64 (__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srav_epi64 (__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrav8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srlv_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srlv_epi64 (__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srlv_epi64 (__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_psrlv8di_mask ((__v8di) __X,
       (__v8di) __Y,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A + (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuldq512_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_epi32 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuldq512_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_epi32 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuldq512_mask ((__v16si) __X,
        (__v16si) __Y,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A - (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_epu32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuludq512_mask ((__v16si) __X,
         (__v16si) __Y,
         (__v8di)
         _mm512_undefined_epi32 (),
         (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_epu32 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuludq512_mask ((__v16si) __X,
         (__v16si) __Y,
         (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_epu32 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmuludq512_mask ((__v16si) __X,
         (__v16si) __Y,
         (__v8di)
         _mm512_setzero_si512 (),
         __M);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_slli_epi64 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psllqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_slli_epi64 (__m512i __W, __mmask8 __U, __m512i __A,
   unsigned int __B)
{
  return (__m512i) __builtin_ia32_psllqi512_mask ((__v8di) __A, __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_slli_epi64 (__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psllqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}
# 1052 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sll_epi64 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psllq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sll_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psllq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sll_epi64 (__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psllq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srli_epi64 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrlqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srli_epi64 (__m512i __W, __mmask8 __U,
   __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrlqi512_mask ((__v8di) __A, __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srli_epi64 (__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrlqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}
# 1131 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srl_epi64 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srl_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srl_epi64 (__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srai_epi64 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psraqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srai_epi64 (__m512i __W, __mmask8 __U, __m512i __A,
   unsigned int __B)
{
  return (__m512i) __builtin_ia32_psraqi512_mask ((__v8di) __A, __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srai_epi64 (__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psraqi512_mask ((__v8di) __A, __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}
# 1210 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sra_epi64 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psraq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sra_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psraq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sra_epi64 (__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psraq512_mask ((__v8di) __A,
       (__v2di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_slli_epi32 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_pslldi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_slli_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
   unsigned int __B)
{
  return (__m512i) __builtin_ia32_pslldi512_mask ((__v16si) __A, __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_slli_epi32 (__mmask16 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_pslldi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}
# 1289 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sll_epi32 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_pslld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sll_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_pslld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sll_epi32 (__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_pslld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srli_epi32 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrldi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srli_epi32 (__m512i __W, __mmask16 __U,
   __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrldi512_mask ((__v16si) __A, __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srli_epi32 (__mmask16 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psrldi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}
# 1368 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srl_epi32 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srl_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srl_epi32 (__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrld512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srai_epi32 (__m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psradi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srai_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
   unsigned int __B)
{
  return (__m512i) __builtin_ia32_psradi512_mask ((__v16si) __A, __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srai_epi32 (__mmask16 __U, __m512i __A, unsigned int __B)
{
  return (__m512i) __builtin_ia32_psradi512_mask ((__v16si) __A, __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}
# 1447 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sra_epi32 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrad512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sra_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrad512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sra_epi32 (__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrad512_mask ((__v16si) __A,
       (__v4si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_addsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_addsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_addsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_add_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_addss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_addss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_addss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_subsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_subsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_subsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sub_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_subss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_subss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_subss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}
# 1648 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ternarylogic_epi64 (__m512i __A, __m512i __B, __m512i __C,
      const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogq512_mask ((__v8di) __A,
           (__v8di) __B,
           (__v8di) __C, __imm,
           (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ternarylogic_epi64 (__m512i __A, __mmask8 __U, __m512i __B,
    __m512i __C, const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogq512_mask ((__v8di) __A,
           (__v8di) __B,
           (__v8di) __C, __imm,
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_ternarylogic_epi64 (__mmask8 __U, __m512i __A, __m512i __B,
     __m512i __C, const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogq512_maskz ((__v8di) __A,
            (__v8di) __B,
            (__v8di) __C,
            __imm, (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ternarylogic_epi32 (__m512i __A, __m512i __B, __m512i __C,
      const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogd512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si) __C,
           __imm, (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ternarylogic_epi32 (__m512i __A, __mmask16 __U, __m512i __B,
    __m512i __C, const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogd512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si) __C,
           __imm, (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_ternarylogic_epi32 (__mmask16 __U, __m512i __A, __m512i __B,
     __m512i __C, const int __imm)
{
  return (__m512i) __builtin_ia32_pternlogd512_maskz ((__v16si) __A,
            (__v16si) __B,
            (__v16si) __C,
            __imm, (__mmask16) __U);
}
# 1737 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rcp14_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
         (__v8df)
         _mm512_undefined_pd (),
         (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rcp14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
         (__v8df) __W,
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rcp14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rcp14_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
        (__v16sf)
        _mm512_undefined_ps (),
        (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rcp14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
        (__v16sf) __W,
        (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rcp14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp14_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd ((__v2df) __B,
        (__v2df) __A);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rcp14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd_mask ((__v2df) __B,
      (__v2df) __A,
      (__v2df) __W,
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rcp14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd_mask ((__v2df) __B,
      (__v2df) __A,
      (__v2df) _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp14_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss ((__v4sf) __B,
       (__v4sf) __A);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rcp14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __B,
      (__v4sf) __A,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rcp14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __B,
      (__v4sf) __A,
      (__v4sf) _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rsqrt14_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
           (__v8df)
           _mm512_undefined_pd (),
           (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rsqrt14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
           (__v8df) __W,
           (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rsqrt14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rsqrt14_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rsqrt14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
          (__v16sf) __W,
          (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rsqrt14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt14_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd ((__v2df) __B,
          (__v2df) __A);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rsqrt14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd_mask ((__v2df) __B,
       (__v2df) __A,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rsqrt14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd_mask ((__v2df) __B,
       (__v2df) __A,
       (__v2df) _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt14_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss ((__v4sf) __B,
         (__v4sf) __A);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rsqrt14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __B,
       (__v4sf) __A,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rsqrt14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __B,
      (__v4sf) __A,
      (__v4sf) _mm_setzero_ps (),
      (__mmask8) __U);
}


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sqrt_round_pd (__m512d __A, const int __R)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sqrt_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
      const int __R)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df) __W,
        (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sqrt_round_pd (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sqrt_round_ps (__m512 __A, const int __R)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf)
       _mm512_undefined_ps (),
       (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sqrt_round_ps (__m512 __W, __mmask16 __U, __m512 __A, const int __R)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf) __W,
       (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sqrt_round_ps (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_sqrtsd_mask_round ((__v2df) __B,
           (__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sqrt_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
   const int __R)
{
  return (__m128d) __builtin_ia32_sqrtsd_mask_round ((__v2df) __B,
           (__v2df) __A,
           (__v2df) __W,
           (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sqrt_round_sd (__mmask8 __U, __m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_sqrtsd_mask_round ((__v2df) __B,
           (__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sqrt_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_sqrtss_mask_round ((__v4sf) __B,
          (__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sqrt_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
   const int __R)
{
  return (__m128) __builtin_ia32_sqrtss_mask_round ((__v4sf) __B,
          (__v4sf) __A,
          (__v4sf) __W,
          (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sqrt_round_ss (__mmask8 __U, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_sqrtss_mask_round ((__v4sf) __B,
          (__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U, __R);
}
# 2132 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi8_epi32 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbd512_mask ((__v16qi) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi8_epi32 (__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbd512_mask ((__v16qi) __A,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi8_epi32 (__mmask16 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbd512_mask ((__v16qi) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi8_epi64 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbq512_mask ((__v16qi) __A,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi8_epi64 (__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbq512_mask ((__v16qi) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbq512_mask ((__v16qi) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi16_epi32 (__m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwd512_mask ((__v16hi) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi16_epi32 (__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwd512_mask ((__v16hi) __A,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi16_epi32 (__mmask16 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwd512_mask ((__v16hi) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi16_epi64 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwq512_mask ((__v8hi) __A,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi16_epi64 (__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwq512_mask ((__v8hi) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovsxwq512_mask ((__v8hi) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi32_epi64 (__m256i __X)
{
  return (__m512i) __builtin_ia32_pmovsxdq512_mask ((__v8si) __X,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_epi64 (__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i) __builtin_ia32_pmovsxdq512_mask ((__v8si) __X,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi32_epi64 (__mmask8 __U, __m256i __X)
{
  return (__m512i) __builtin_ia32_pmovsxdq512_mask ((__v8si) __X,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu8_epi32 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbd512_mask ((__v16qi) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu8_epi32 (__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbd512_mask ((__v16qi) __A,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu8_epi32 (__mmask16 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbd512_mask ((__v16qi) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu8_epi64 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbq512_mask ((__v16qi) __A,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu8_epi64 (__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbq512_mask ((__v16qi) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbq512_mask ((__v16qi) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu16_epi32 (__m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwd512_mask ((__v16hi) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu16_epi32 (__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwd512_mask ((__v16hi) __A,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu16_epi32 (__mmask16 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwd512_mask ((__v16hi) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu16_epi64 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwq512_mask ((__v8hi) __A,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu16_epi64 (__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwq512_mask ((__v8hi) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m512i) __builtin_ia32_pmovzxwq512_mask ((__v8hi) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu32_epi64 (__m256i __X)
{
  return (__m512i) __builtin_ia32_pmovzxdq512_mask ((__v8si) __X,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu32_epi64 (__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i) __builtin_ia32_pmovzxdq512_mask ((__v8si) __X,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu32_epi64 (__mmask8 __U, __m256i __X)
{
  return (__m512i) __builtin_ia32_pmovzxdq512_mask ((__v8si) __X,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_addpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
     __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_addpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
      const int __R)
{
  return (__m512d) __builtin_ia32_addpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_addps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_addps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_addps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_subpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
     __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_subpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
      const int __R)
{
  return (__m512d) __builtin_ia32_subpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_subps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_subps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_subps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}
# 2595 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_mulpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
     __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_mulpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
      const int __R)
{
  return (__m512d) __builtin_ia32_mulpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_mulps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_mulps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_mulps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_div_round_pd (__m512d __M, __m512d __V, const int __R)
{
  return (__m512d) __builtin_ia32_divpd512_mask ((__v8df) __M,
       (__v8df) __V,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_div_round_pd (__m512d __W, __mmask8 __U, __m512d __M,
     __m512d __V, const int __R)
{
  return (__m512d) __builtin_ia32_divpd512_mask ((__v8df) __M,
       (__v8df) __V,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_div_round_pd (__mmask8 __U, __m512d __M, __m512d __V,
      const int __R)
{
  return (__m512d) __builtin_ia32_divpd512_mask ((__v8df) __M,
       (__v8df) __V,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_div_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_divps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_div_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_divps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_div_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_divps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_mulsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_mulsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_mulsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mul_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_mulss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_mulss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_mulss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_divsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_divsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_divsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_div_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_divss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_divss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_divss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}
# 2933 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
     __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
      const int __R)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
     __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
      const int __R)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
     __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_round_ps (__mmask16 __U, __m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U, __R);
}
# 3105 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_scalef_round_pd (__m512d __A, __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_scalef_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
        __m512d __B, const int __R)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __W,
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_scalef_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
         const int __R)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_scalef_round_ps (__m512 __A, __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_scalef_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
        __m512 __B, const int __R)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __W,
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_scalef_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
         const int __R)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_scalefsd_mask_round ((__v2df) __A,
             (__v2df) __B,
             (__v2df)
             _mm_setzero_pd (),
             (__mmask8) -1, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_scalef_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
     const int __R)
{
  return (__m128d) __builtin_ia32_scalefsd_mask_round ((__v2df) __A,
             (__v2df) __B,
             (__v2df) __W,
             (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_scalef_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_scalefsd_mask_round ((__v2df) __A,
             (__v2df) __B,
             (__v2df)
             _mm_setzero_pd (),
             (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_scalefss_mask_round ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) -1, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_scalef_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    const int __R)
{
  return (__m128) __builtin_ia32_scalefss_mask_round ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf) __W,
            (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_scalef_round_ss (__mmask8 __U, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_scalefss_mask_round ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U, __R);
}
# 3268 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmadd_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
       __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_round_pd (__m512d __A, __m512d __B, __m512d __C,
        __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
        __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmadd_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
       __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_round_ps (__m512 __A, __m512 __B, __m512 __C,
        __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
        __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsub_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsub_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
       __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsub_round_pd (__m512d __A, __m512d __B, __m512d __C,
        __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask3 ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsub_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
        __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_maskz ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsub_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsub_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
       __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsub_round_ps (__m512 __A, __m512 __B, __m512 __C,
        __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask3 ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsub_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
        __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmsubps512_maskz ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df) __C,
             (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
          __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df) __C,
             (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_round_pd (__m512d __A, __m512d __B, __m512d __C,
    __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask3 ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
    __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf) __C,
            (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
          __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf) __C,
            (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_round_ps (__m512 __A, __m512 __B, __m512 __C,
    __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask3 ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
    __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsubadd_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             -(__v8df) __C,
             (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
          __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             -(__v8df) __C,
             (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_round_pd (__m512d __A, __m512d __B, __m512d __C,
    __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfmsubaddpd512_mask3 ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
    __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
       (__v8df) __B,
       -(__v8df) __C,
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsubadd_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            -(__v16sf) __C,
            (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
          __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            -(__v16sf) __C,
            (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_round_ps (__m512 __A, __m512 __B, __m512 __C,
    __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfmsubaddps512_mask3 ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
    __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
             (__v16sf) __B,
             -(__v16sf) __C,
             (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmadd_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
        __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmadd_round_pd (__m512d __A, __m512d __B, __m512d __C,
         __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask3 ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmadd_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
         __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_maskz ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmadd_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
        __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmadd_round_ps (__m512 __A, __m512 __B, __m512 __C,
         __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask3 ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmadd_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
         __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmaddps512_maskz ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmsub_round_pd (__m512d __A, __m512d __B, __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmsub_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
        __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmsub_round_pd (__m512d __A, __m512d __B, __m512d __C,
         __mmask8 __U, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask3 ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmsub_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
         __m512d __C, const int __R)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_maskz ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmsub_round_ps (__m512 __A, __m512 __B, __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmsub_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
        __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmsub_round_ps (__m512 __A, __m512 __B, __m512 __C,
         __mmask16 __U, const int __R)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask3 ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmsub_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
         __m512 __C, const int __R)
{
  return (__m512) __builtin_ia32_vfnmsubps512_maskz ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U, __R);
}
# 3929 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_abs_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_abs_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastss_ps (__m128 __A)
{
  return (__m512) __builtin_ia32_broadcastss512 ((__v4sf) __A,
       (__v16sf)
       _mm512_undefined_ps (),
       (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastss_ps (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastss512 ((__v4sf) __A,
       (__v16sf) __O, __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastss_ps (__mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastss512 ((__v4sf) __A,
       (__v16sf)
       _mm512_setzero_ps (),
       __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastsd_pd (__m128d __A)
{
  return (__m512d) __builtin_ia32_broadcastsd512 ((__v2df) __A,
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastsd_pd (__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d) __builtin_ia32_broadcastsd512 ((__v2df) __A,
        (__v8df) __O, __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m512d) __builtin_ia32_broadcastsd512 ((__v2df) __A,
        (__v8df)
        _mm512_setzero_pd (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastd_epi32 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastd512 ((__v4si) __A,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastd_epi32 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastd512 ((__v4si) __A,
        (__v16si) __O, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastd_epi32 (__mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastd512 ((__v4si) __A,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_epi32 (int __A)
{
  return (__m512i) __builtin_ia32_pbroadcastd512_gpr_mask (__A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16)(-1));
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_set1_epi32 (__m512i __O, __mmask16 __M, int __A)
{
  return (__m512i) __builtin_ia32_pbroadcastd512_gpr_mask (__A, (__v16si) __O,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_set1_epi32 (__mmask16 __M, int __A)
{
  return (__m512i)
  __builtin_ia32_pbroadcastd512_gpr_mask (__A,
       (__v16si) _mm512_setzero_si512 (),
       __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastq_epi64 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastq512 ((__v2di) __A,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastq_epi64 (__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastq512 ((__v2di) __A,
        (__v8di) __O, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastq512 ((__v2di) __A,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_set1_epi64 (long long __A)
{
  return (__m512i) __builtin_ia32_pbroadcastq512_gpr_mask (__A,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8)(-1));
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_set1_epi64 (__m512i __O, __mmask8 __M, long long __A)
{
  return (__m512i) __builtin_ia32_pbroadcastq512_gpr_mask (__A, (__v8di) __O,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m512i)
  __builtin_ia32_pbroadcastq512_gpr_mask (__A,
       (__v8di) _mm512_setzero_si512 (),
       __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_f32x4 (__m128 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x4_512 ((__v4sf) __A,
           (__v16sf)
           _mm512_undefined_ps (),
           (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_f32x4 (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x4_512 ((__v4sf) __A,
           (__v16sf) __O,
           __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_f32x4 (__mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x4_512 ((__v4sf) __A,
           (__v16sf)
           _mm512_setzero_ps (),
           __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_i32x4 (__m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x4_512 ((__v4si) __A,
            (__v16si)
            _mm512_undefined_epi32 (),
            (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_i32x4 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x4_512 ((__v4si) __A,
            (__v16si) __O,
            __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_i32x4 (__mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x4_512 ((__v4si) __A,
            (__v16si)
            _mm512_setzero_si512 (),
            __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_f64x4 (__m256d __A)
{
  return (__m512d) __builtin_ia32_broadcastf64x4_512 ((__v4df) __A,
            (__v8df)
            _mm512_undefined_pd (),
            (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_f64x4 (__m512d __O, __mmask8 __M, __m256d __A)
{
  return (__m512d) __builtin_ia32_broadcastf64x4_512 ((__v4df) __A,
            (__v8df) __O,
            __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_f64x4 (__mmask8 __M, __m256d __A)
{
  return (__m512d) __builtin_ia32_broadcastf64x4_512 ((__v4df) __A,
            (__v8df)
            _mm512_setzero_pd (),
            __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_i64x4 (__m256i __A)
{
  return (__m512i) __builtin_ia32_broadcasti64x4_512 ((__v4di) __A,
            (__v8di)
            _mm512_undefined_epi32 (),
            (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_i64x4 (__m512i __O, __mmask8 __M, __m256i __A)
{
  return (__m512i) __builtin_ia32_broadcasti64x4_512 ((__v4di) __A,
            (__v8di) __O,
            __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_i64x4 (__mmask8 __M, __m256i __A)
{
  return (__m512i) __builtin_ia32_broadcasti64x4_512 ((__v4di) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            __M);
}

typedef enum
{
  _MM_PERM_AAAA = 0x00, _MM_PERM_AAAB = 0x01, _MM_PERM_AAAC = 0x02,
  _MM_PERM_AAAD = 0x03, _MM_PERM_AABA = 0x04, _MM_PERM_AABB = 0x05,
  _MM_PERM_AABC = 0x06, _MM_PERM_AABD = 0x07, _MM_PERM_AACA = 0x08,
  _MM_PERM_AACB = 0x09, _MM_PERM_AACC = 0x0A, _MM_PERM_AACD = 0x0B,
  _MM_PERM_AADA = 0x0C, _MM_PERM_AADB = 0x0D, _MM_PERM_AADC = 0x0E,
  _MM_PERM_AADD = 0x0F, _MM_PERM_ABAA = 0x10, _MM_PERM_ABAB = 0x11,
  _MM_PERM_ABAC = 0x12, _MM_PERM_ABAD = 0x13, _MM_PERM_ABBA = 0x14,
  _MM_PERM_ABBB = 0x15, _MM_PERM_ABBC = 0x16, _MM_PERM_ABBD = 0x17,
  _MM_PERM_ABCA = 0x18, _MM_PERM_ABCB = 0x19, _MM_PERM_ABCC = 0x1A,
  _MM_PERM_ABCD = 0x1B, _MM_PERM_ABDA = 0x1C, _MM_PERM_ABDB = 0x1D,
  _MM_PERM_ABDC = 0x1E, _MM_PERM_ABDD = 0x1F, _MM_PERM_ACAA = 0x20,
  _MM_PERM_ACAB = 0x21, _MM_PERM_ACAC = 0x22, _MM_PERM_ACAD = 0x23,
  _MM_PERM_ACBA = 0x24, _MM_PERM_ACBB = 0x25, _MM_PERM_ACBC = 0x26,
  _MM_PERM_ACBD = 0x27, _MM_PERM_ACCA = 0x28, _MM_PERM_ACCB = 0x29,
  _MM_PERM_ACCC = 0x2A, _MM_PERM_ACCD = 0x2B, _MM_PERM_ACDA = 0x2C,
  _MM_PERM_ACDB = 0x2D, _MM_PERM_ACDC = 0x2E, _MM_PERM_ACDD = 0x2F,
  _MM_PERM_ADAA = 0x30, _MM_PERM_ADAB = 0x31, _MM_PERM_ADAC = 0x32,
  _MM_PERM_ADAD = 0x33, _MM_PERM_ADBA = 0x34, _MM_PERM_ADBB = 0x35,
  _MM_PERM_ADBC = 0x36, _MM_PERM_ADBD = 0x37, _MM_PERM_ADCA = 0x38,
  _MM_PERM_ADCB = 0x39, _MM_PERM_ADCC = 0x3A, _MM_PERM_ADCD = 0x3B,
  _MM_PERM_ADDA = 0x3C, _MM_PERM_ADDB = 0x3D, _MM_PERM_ADDC = 0x3E,
  _MM_PERM_ADDD = 0x3F, _MM_PERM_BAAA = 0x40, _MM_PERM_BAAB = 0x41,
  _MM_PERM_BAAC = 0x42, _MM_PERM_BAAD = 0x43, _MM_PERM_BABA = 0x44,
  _MM_PERM_BABB = 0x45, _MM_PERM_BABC = 0x46, _MM_PERM_BABD = 0x47,
  _MM_PERM_BACA = 0x48, _MM_PERM_BACB = 0x49, _MM_PERM_BACC = 0x4A,
  _MM_PERM_BACD = 0x4B, _MM_PERM_BADA = 0x4C, _MM_PERM_BADB = 0x4D,
  _MM_PERM_BADC = 0x4E, _MM_PERM_BADD = 0x4F, _MM_PERM_BBAA = 0x50,
  _MM_PERM_BBAB = 0x51, _MM_PERM_BBAC = 0x52, _MM_PERM_BBAD = 0x53,
  _MM_PERM_BBBA = 0x54, _MM_PERM_BBBB = 0x55, _MM_PERM_BBBC = 0x56,
  _MM_PERM_BBBD = 0x57, _MM_PERM_BBCA = 0x58, _MM_PERM_BBCB = 0x59,
  _MM_PERM_BBCC = 0x5A, _MM_PERM_BBCD = 0x5B, _MM_PERM_BBDA = 0x5C,
  _MM_PERM_BBDB = 0x5D, _MM_PERM_BBDC = 0x5E, _MM_PERM_BBDD = 0x5F,
  _MM_PERM_BCAA = 0x60, _MM_PERM_BCAB = 0x61, _MM_PERM_BCAC = 0x62,
  _MM_PERM_BCAD = 0x63, _MM_PERM_BCBA = 0x64, _MM_PERM_BCBB = 0x65,
  _MM_PERM_BCBC = 0x66, _MM_PERM_BCBD = 0x67, _MM_PERM_BCCA = 0x68,
  _MM_PERM_BCCB = 0x69, _MM_PERM_BCCC = 0x6A, _MM_PERM_BCCD = 0x6B,
  _MM_PERM_BCDA = 0x6C, _MM_PERM_BCDB = 0x6D, _MM_PERM_BCDC = 0x6E,
  _MM_PERM_BCDD = 0x6F, _MM_PERM_BDAA = 0x70, _MM_PERM_BDAB = 0x71,
  _MM_PERM_BDAC = 0x72, _MM_PERM_BDAD = 0x73, _MM_PERM_BDBA = 0x74,
  _MM_PERM_BDBB = 0x75, _MM_PERM_BDBC = 0x76, _MM_PERM_BDBD = 0x77,
  _MM_PERM_BDCA = 0x78, _MM_PERM_BDCB = 0x79, _MM_PERM_BDCC = 0x7A,
  _MM_PERM_BDCD = 0x7B, _MM_PERM_BDDA = 0x7C, _MM_PERM_BDDB = 0x7D,
  _MM_PERM_BDDC = 0x7E, _MM_PERM_BDDD = 0x7F, _MM_PERM_CAAA = 0x80,
  _MM_PERM_CAAB = 0x81, _MM_PERM_CAAC = 0x82, _MM_PERM_CAAD = 0x83,
  _MM_PERM_CABA = 0x84, _MM_PERM_CABB = 0x85, _MM_PERM_CABC = 0x86,
  _MM_PERM_CABD = 0x87, _MM_PERM_CACA = 0x88, _MM_PERM_CACB = 0x89,
  _MM_PERM_CACC = 0x8A, _MM_PERM_CACD = 0x8B, _MM_PERM_CADA = 0x8C,
  _MM_PERM_CADB = 0x8D, _MM_PERM_CADC = 0x8E, _MM_PERM_CADD = 0x8F,
  _MM_PERM_CBAA = 0x90, _MM_PERM_CBAB = 0x91, _MM_PERM_CBAC = 0x92,
  _MM_PERM_CBAD = 0x93, _MM_PERM_CBBA = 0x94, _MM_PERM_CBBB = 0x95,
  _MM_PERM_CBBC = 0x96, _MM_PERM_CBBD = 0x97, _MM_PERM_CBCA = 0x98,
  _MM_PERM_CBCB = 0x99, _MM_PERM_CBCC = 0x9A, _MM_PERM_CBCD = 0x9B,
  _MM_PERM_CBDA = 0x9C, _MM_PERM_CBDB = 0x9D, _MM_PERM_CBDC = 0x9E,
  _MM_PERM_CBDD = 0x9F, _MM_PERM_CCAA = 0xA0, _MM_PERM_CCAB = 0xA1,
  _MM_PERM_CCAC = 0xA2, _MM_PERM_CCAD = 0xA3, _MM_PERM_CCBA = 0xA4,
  _MM_PERM_CCBB = 0xA5, _MM_PERM_CCBC = 0xA6, _MM_PERM_CCBD = 0xA7,
  _MM_PERM_CCCA = 0xA8, _MM_PERM_CCCB = 0xA9, _MM_PERM_CCCC = 0xAA,
  _MM_PERM_CCCD = 0xAB, _MM_PERM_CCDA = 0xAC, _MM_PERM_CCDB = 0xAD,
  _MM_PERM_CCDC = 0xAE, _MM_PERM_CCDD = 0xAF, _MM_PERM_CDAA = 0xB0,
  _MM_PERM_CDAB = 0xB1, _MM_PERM_CDAC = 0xB2, _MM_PERM_CDAD = 0xB3,
  _MM_PERM_CDBA = 0xB4, _MM_PERM_CDBB = 0xB5, _MM_PERM_CDBC = 0xB6,
  _MM_PERM_CDBD = 0xB7, _MM_PERM_CDCA = 0xB8, _MM_PERM_CDCB = 0xB9,
  _MM_PERM_CDCC = 0xBA, _MM_PERM_CDCD = 0xBB, _MM_PERM_CDDA = 0xBC,
  _MM_PERM_CDDB = 0xBD, _MM_PERM_CDDC = 0xBE, _MM_PERM_CDDD = 0xBF,
  _MM_PERM_DAAA = 0xC0, _MM_PERM_DAAB = 0xC1, _MM_PERM_DAAC = 0xC2,
  _MM_PERM_DAAD = 0xC3, _MM_PERM_DABA = 0xC4, _MM_PERM_DABB = 0xC5,
  _MM_PERM_DABC = 0xC6, _MM_PERM_DABD = 0xC7, _MM_PERM_DACA = 0xC8,
  _MM_PERM_DACB = 0xC9, _MM_PERM_DACC = 0xCA, _MM_PERM_DACD = 0xCB,
  _MM_PERM_DADA = 0xCC, _MM_PERM_DADB = 0xCD, _MM_PERM_DADC = 0xCE,
  _MM_PERM_DADD = 0xCF, _MM_PERM_DBAA = 0xD0, _MM_PERM_DBAB = 0xD1,
  _MM_PERM_DBAC = 0xD2, _MM_PERM_DBAD = 0xD3, _MM_PERM_DBBA = 0xD4,
  _MM_PERM_DBBB = 0xD5, _MM_PERM_DBBC = 0xD6, _MM_PERM_DBBD = 0xD7,
  _MM_PERM_DBCA = 0xD8, _MM_PERM_DBCB = 0xD9, _MM_PERM_DBCC = 0xDA,
  _MM_PERM_DBCD = 0xDB, _MM_PERM_DBDA = 0xDC, _MM_PERM_DBDB = 0xDD,
  _MM_PERM_DBDC = 0xDE, _MM_PERM_DBDD = 0xDF, _MM_PERM_DCAA = 0xE0,
  _MM_PERM_DCAB = 0xE1, _MM_PERM_DCAC = 0xE2, _MM_PERM_DCAD = 0xE3,
  _MM_PERM_DCBA = 0xE4, _MM_PERM_DCBB = 0xE5, _MM_PERM_DCBC = 0xE6,
  _MM_PERM_DCBD = 0xE7, _MM_PERM_DCCA = 0xE8, _MM_PERM_DCCB = 0xE9,
  _MM_PERM_DCCC = 0xEA, _MM_PERM_DCCD = 0xEB, _MM_PERM_DCDA = 0xEC,
  _MM_PERM_DCDB = 0xED, _MM_PERM_DCDC = 0xEE, _MM_PERM_DCDD = 0xEF,
  _MM_PERM_DDAA = 0xF0, _MM_PERM_DDAB = 0xF1, _MM_PERM_DDAC = 0xF2,
  _MM_PERM_DDAD = 0xF3, _MM_PERM_DDBA = 0xF4, _MM_PERM_DDBB = 0xF5,
  _MM_PERM_DDBC = 0xF6, _MM_PERM_DDBD = 0xF7, _MM_PERM_DDCA = 0xF8,
  _MM_PERM_DDCB = 0xF9, _MM_PERM_DDCC = 0xFA, _MM_PERM_DDCD = 0xFB,
  _MM_PERM_DDDA = 0xFC, _MM_PERM_DDDB = 0xFD, _MM_PERM_DDDC = 0xFE,
  _MM_PERM_DDDD = 0xFF
} _MM_PERM_ENUM;


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_epi32 (__m512i __A, _MM_PERM_ENUM __mask)
{
  return (__m512i) __builtin_ia32_pshufd512_mask ((__v16si) __A,
        __mask,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
      _MM_PERM_ENUM __mask)
{
  return (__m512i) __builtin_ia32_pshufd512_mask ((__v16si) __A,
        __mask,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_epi32 (__mmask16 __U, __m512i __A, _MM_PERM_ENUM __mask)
{
  return (__m512i) __builtin_ia32_pshufd512_mask ((__v16si) __A,
        __mask,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_i64x2 (__m512i __A, __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i64x2_mask ((__v8di) __A,
         (__v8di) __B, __imm,
         (__v8di)
         _mm512_undefined_epi32 (),
         (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_i64x2 (__m512i __W, __mmask8 __U, __m512i __A,
      __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i64x2_mask ((__v8di) __A,
         (__v8di) __B, __imm,
         (__v8di) __W,
         (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_i64x2 (__mmask8 __U, __m512i __A, __m512i __B,
       const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i64x2_mask ((__v8di) __A,
         (__v8di) __B, __imm,
         (__v8di)
         _mm512_setzero_si512 (),
         (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_i32x4 (__m512i __A, __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i32x4_mask ((__v16si) __A,
         (__v16si) __B,
         __imm,
         (__v16si)
         _mm512_undefined_epi32 (),
         (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_i32x4 (__m512i __W, __mmask16 __U, __m512i __A,
      __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i32x4_mask ((__v16si) __A,
         (__v16si) __B,
         __imm,
         (__v16si) __W,
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_i32x4 (__mmask16 __U, __m512i __A, __m512i __B,
       const int __imm)
{
  return (__m512i) __builtin_ia32_shuf_i32x4_mask ((__v16si) __A,
         (__v16si) __B,
         __imm,
         (__v16si)
         _mm512_setzero_si512 (),
         (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_f64x2 (__m512d __A, __m512d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_shuf_f64x2_mask ((__v8df) __A,
         (__v8df) __B, __imm,
         (__v8df)
         _mm512_undefined_pd (),
         (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_f64x2 (__m512d __W, __mmask8 __U, __m512d __A,
      __m512d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_shuf_f64x2_mask ((__v8df) __A,
         (__v8df) __B, __imm,
         (__v8df) __W,
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_f64x2 (__mmask8 __U, __m512d __A, __m512d __B,
       const int __imm)
{
  return (__m512d) __builtin_ia32_shuf_f64x2_mask ((__v8df) __A,
         (__v8df) __B, __imm,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_f32x4 (__m512 __A, __m512 __B, const int __imm)
{
  return (__m512) __builtin_ia32_shuf_f32x4_mask ((__v16sf) __A,
        (__v16sf) __B, __imm,
        (__v16sf)
        _mm512_undefined_ps (),
        (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_f32x4 (__m512 __W, __mmask16 __U, __m512 __A,
      __m512 __B, const int __imm)
{
  return (__m512) __builtin_ia32_shuf_f32x4_mask ((__v16sf) __A,
        (__v16sf) __B, __imm,
        (__v16sf) __W,
        (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_f32x4 (__mmask16 __U, __m512 __A, __m512 __B,
       const int __imm)
{
  return (__m512) __builtin_ia32_shuf_f32x4_mask ((__v16sf) __A,
        (__v16sf) __B, __imm,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U);
}
# 4623 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rolv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rolv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rolv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rorv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rorv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rorv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rolv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rolv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rolv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rorv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rorv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rorv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundpd_epi32 (__m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_undefined_si256 (),
           (__mmask8) -1, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si) __W,
           (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundpd_epi32 (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundpd_epu32 (__m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si)
            _mm256_undefined_si256 (),
            (__mmask8) -1, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si) __W,
            (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundpd_epu32 (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si)
            _mm256_setzero_si256 (),
            (__mmask8) __U, __R);
}
# 4832 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_epi32 (__m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si)
          _mm256_undefined_si256 (),
          (__mmask8) -1, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A,
          const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si) __W,
          (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_epi32 (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_epu32 (__m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_undefined_si256 (),
           (__mmask8) -1, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A,
          const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si) __W,
           (__mmask8) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_epu32 (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U, __R);
}
# 4912 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundps_epi32 (__m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundps_epi32 (__m512i __W, __mmask16 __U, __m512 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si) __W,
           (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundps_epi32 (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundps_epu32 (__m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si)
            _mm512_undefined_epi32 (),
            (__mmask16) -1, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundps_epu32 (__m512i __W, __mmask16 __U, __m512 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si) __W,
            (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundps_epu32 (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si)
            _mm512_setzero_si512 (),
            (__mmask16) __U, __R);
}
# 4992 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_epi32 (__m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_epi32 (__m512i __W, __mmask16 __U, __m512 __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si) __W,
          (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_epi32 (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_epu32 (__m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_epu32 (__m512i __W, __mmask16 __U, __m512 __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si) __W,
           (__mmask16) __U, __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_epu32 (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U, __R);
}
# 5071 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtu32_sd (__m128d __A, unsigned __B)
{
  return (__m128d) __builtin_ia32_cvtusi2sd32 ((__v2df) __A, __B);
}



extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundu64_sd (__m128d __A, unsigned long long __B, const int __R)
{
  return (__m128d) __builtin_ia32_cvtusi2sd64 ((__v2df) __A, __B, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundi64_sd (__m128d __A, long long __B, const int __R)
{
  return (__m128d) __builtin_ia32_cvtsi2sd64 ((__v2df) __A, __B, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsi64_sd (__m128d __A, long long __B, const int __R)
{
  return (__m128d) __builtin_ia32_cvtsi2sd64 ((__v2df) __A, __B, __R);
}
# 5114 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundu32_ss (__m128 __A, unsigned __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtusi2ss32 ((__v4sf) __A, __B, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsi32_ss (__m128 __A, int __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtsi2ss32 ((__v4sf) __A, __B, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundi32_ss (__m128 __A, int __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtsi2ss32 ((__v4sf) __A, __B, __R);
}
# 5147 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundu64_ss (__m128 __A, unsigned long long __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtusi2ss64 ((__v4sf) __A, __B, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsi64_ss (__m128 __A, long long __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtsi2ss64 ((__v4sf) __A, __B, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundi64_ss (__m128 __A, long long __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtsi2ss64 ((__v4sf) __A, __B, __R);
}
# 5180 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
        (__v16hi)
        _mm256_undefined_si256 (),
        (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_storeu_epi16 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdw512mem_mask ((__v16hi *) __P, (__v16si) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
        (__v16hi) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
        (__v16hi)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
         (__v16hi)
         _mm256_undefined_si256 (),
         (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
         (__v16hi) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
         (__v16hi)
         _mm256_setzero_si256 (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
          (__v16hi)
          _mm256_undefined_si256 (),
          (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
          (__v16hi) __O,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
          (__v16hi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
        (__v8si)
        _mm256_undefined_si256 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
        (__v8si) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
         (__v8si)
         _mm256_undefined_si256 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_storeu_epi32 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
         (__v8si) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
         (__v8si)
         _mm256_setzero_si256 (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
          (__v8si)
          _mm256_undefined_si256 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqd512mem_mask ((__v8si*) __P, (__v8di) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
          (__v8si) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
        (__v8hi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
        (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
         (__v8hi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
         (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
          (__v8hi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqw512mem_mask ((__v8hi*) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
          (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi32_pd (__m256i __A)
{
  return (__m512d) __builtin_ia32_cvtdq2pd512_mask ((__v8si) __A,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d) __builtin_ia32_cvtdq2pd512_mask ((__v8si) __A,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d) __builtin_ia32_cvtdq2pd512_mask ((__v8si) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu32_pd (__m256i __A)
{
  return (__m512d) __builtin_ia32_cvtudq2pd512_mask ((__v8si) __A,
           (__v8df)
           _mm512_undefined_pd (),
           (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d) __builtin_ia32_cvtudq2pd512_mask ((__v8si) __A,
           (__v8df) __W,
           (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d) __builtin_ia32_cvtudq2pd512_mask ((__v8si) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U);
}


extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepi32_ps (__m512i __A, const int __R)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepi32_ps (__m512 __W, __mmask16 __U, __m512i __A,
          const int __R)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf) __W,
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepi32_ps (__mmask16 __U, __m512i __A, const int __R)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepu32_ps (__m512i __A, const int __R)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepu32_ps (__m512 __W, __mmask16 __U, __m512i __A,
          const int __R)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf) __W,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepu32_ps (__mmask16 __U, __m512i __A, const int __R)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U, __R);
}
# 5848 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extractf64x4_pd (__m512d __A, const int __imm)
{
  return (__m256d) __builtin_ia32_extractf64x4_mask ((__v8df) __A,
           __imm,
           (__v4df)
           _mm256_undefined_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extractf64x4_pd (__m256d __W, __mmask8 __U, __m512d __A,
        const int __imm)
{
  return (__m256d) __builtin_ia32_extractf64x4_mask ((__v8df) __A,
           __imm,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extractf64x4_pd (__mmask8 __U, __m512d __A, const int __imm)
{
  return (__m256d) __builtin_ia32_extractf64x4_mask ((__v8df) __A,
           __imm,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extractf32x4_ps (__m512 __A, const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_mask ((__v16sf) __A,
          __imm,
          (__v4sf)
          _mm_undefined_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extractf32x4_ps (__m128 __W, __mmask8 __U, __m512 __A,
        const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_mask ((__v16sf) __A,
          __imm,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extractf32x4_ps (__mmask8 __U, __m512 __A, const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_mask ((__v16sf) __A,
          __imm,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extracti64x4_epi64 (__m512i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_extracti64x4_mask ((__v8di) __A,
           __imm,
           (__v4di)
           _mm256_undefined_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extracti64x4_epi64 (__m256i __W, __mmask8 __U, __m512i __A,
    const int __imm)
{
  return (__m256i) __builtin_ia32_extracti64x4_mask ((__v8di) __A,
           __imm,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extracti64x4_epi64 (__mmask8 __U, __m512i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_extracti64x4_mask ((__v8di) __A,
           __imm,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extracti32x4_epi32 (__m512i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_mask ((__v16si) __A,
           __imm,
           (__v4si)
           _mm_undefined_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extracti32x4_epi32 (__m128i __W, __mmask8 __U, __m512i __A,
    const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_mask ((__v16si) __A,
           __imm,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extracti32x4_epi32 (__mmask8 __U, __m512i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_mask ((__v16si) __A,
           __imm,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}
# 6055 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_inserti32x4 (__m512i __A, __m128i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x4_mask ((__v16si) __A,
          (__v4si) __B,
          __imm,
          (__v16si) __A, -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_insertf32x4 (__m512 __A, __m128 __B, const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x4_mask ((__v16sf) __A,
         (__v4sf) __B,
         __imm,
         (__v16sf) __A, -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_inserti64x4 (__m512i __A, __m256i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x4_mask ((__v8di) __A,
          (__v4di) __B,
          __imm,
          (__v8di)
          _mm512_undefined_epi32 (),
          (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_inserti64x4 (__m512i __W, __mmask8 __U, __m512i __A,
    __m256i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x4_mask ((__v8di) __A,
          (__v4di) __B,
          __imm,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_inserti64x4 (__mmask8 __U, __m512i __A, __m256i __B,
     const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x4_mask ((__v8di) __A,
          (__v4di) __B,
          __imm,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_insertf64x4 (__m512d __A, __m256d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x4_mask ((__v8df) __A,
          (__v4df) __B,
          __imm,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_insertf64x4 (__m512d __W, __mmask8 __U, __m512d __A,
    __m256d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x4_mask ((__v8df) __A,
          (__v4df) __B,
          __imm,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_insertf64x4 (__mmask8 __U, __m512d __A, __m256d __B,
     const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x4_mask ((__v8df) __A,
          (__v4df) __B,
          __imm,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U);
}
# 6194 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_loadu_pd (void const *__P)
{
  return *(__m512d_u *)__P;
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,
         (__v8df) __W,
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_storeu_pd (void *__P, __m512d __A)
{
  *(__m512d_u *)__P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeupd512_mask ((double *) __P, (__v8df) __A,
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_loadu_ps (void const *__P)
{
  return *(__m512_u *)__P;
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,
        (__v16sf) __W,
        (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_ps (__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_storeu_ps (void *__P, __m512 __A)
{
  *(__m512_u *)__P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeups512_mask ((float *) __P, (__v16sf) __A,
       (__mmask16) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_ss (__m128 __W, __mmask8 __U, const float *__P)
{
  return (__m128) __builtin_ia32_loadss_mask (__P, (__v4sf) __W, __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_ss (__mmask8 __U, const float *__P)
{
  return (__m128) __builtin_ia32_loadss_mask (__P, (__v4sf) _mm_setzero_ps (),
           __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_sd (__m128d __W, __mmask8 __U, const double *__P)
{
  return (__m128d) __builtin_ia32_loadsd_mask (__P, (__v2df) __W, __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_sd (__mmask8 __U, const double *__P)
{
  return (__m128d) __builtin_ia32_loadsd_mask (__P, (__v2df) _mm_setzero_pd (),
            __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_move_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movess_mask ((__v4sf) __A, (__v4sf) __B,
           (__v4sf) __W, __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_move_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_movess_mask ((__v4sf) __A, (__v4sf) __B,
           (__v4sf) _mm_setzero_ps (), __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_move_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movesd_mask ((__v2df) __A, (__v2df) __B,
            (__v2df) __W, __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_move_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_movesd_mask ((__v2df) __A, (__v2df) __B,
            (__v2df) _mm_setzero_pd (),
            __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_ss (float *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storess_mask (__P, (__v4sf) __A, (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_sd (double *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storesd_mask (__P, (__v2df) __A, (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_loadu_epi64 (void const *__P)
{
  return *(__m512i_u *) __P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,
           (__v8di) __W,
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_storeu_epi64 (void *__P, __m512i __A)
{
  *(__m512i_u *) __P = (__m512i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_storedqudi512_mask ((long long *) __P, (__v8di) __A,
         (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_loadu_si512 (void const *__P)
{
  return *(__m512i_u *)__P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_loadu_epi32 (void const *__P)
{
  return *(__m512i_u *) __P;
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,
           (__v16si) __W,
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_epi32 (__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_storeu_si512 (void *__P, __m512i __A)
{
  *(__m512i_u *)__P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_storeu_epi32 (void *__P, __m512i __A)
{
  *(__m512i_u *) __P = (__m512i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_storedqusi512_mask ((int *) __P, (__v16si) __A,
         (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutevar_pd (__m512d __A, __m512i __C)
{
  return (__m512d) __builtin_ia32_vpermilvarpd512_mask ((__v8df) __A,
       (__v8di) __C,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutevar_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d) __builtin_ia32_vpermilvarpd512_mask ((__v8df) __A,
       (__v8di) __C,
       (__v8df) __W,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutevar_pd (__mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d) __builtin_ia32_vpermilvarpd512_mask ((__v8df) __A,
       (__v8di) __C,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutevar_ps (__m512 __A, __m512i __C)
{
  return (__m512) __builtin_ia32_vpermilvarps512_mask ((__v16sf) __A,
             (__v16si) __C,
             (__v16sf)
             _mm512_undefined_ps (),
             (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutevar_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512) __builtin_ia32_vpermilvarps512_mask ((__v16sf) __A,
             (__v16si) __C,
             (__v16sf) __W,
             (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutevar_ps (__mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512) __builtin_ia32_vpermilvarps512_mask ((__v16sf) __A,
             (__v16si) __C,
             (__v16sf)
             _mm512_setzero_ps (),
             (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_epi64 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_mask ((__v8di) __I
                       ,
             (__v8di) __A,
             (__v8di) __B,
             (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_epi64 (__m512i __A, __mmask8 __U, __m512i __I,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_mask ((__v8di) __I
                       ,
             (__v8di) __A,
             (__v8di) __B,
             (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_epi64 (__m512i __A, __m512i __I,
     __mmask8 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varq512_mask ((__v8di) __A,
             (__v8di) __I
                       ,
             (__v8di) __B,
             (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_epi64 (__mmask8 __U, __m512i __A,
     __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_maskz ((__v8di) __I
                 ,
       (__v8di) __A,
       (__v8di) __B,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_epi32 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_mask ((__v16si) __I
                       ,
             (__v16si) __A,
             (__v16si) __B,
             (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_epi32 (__m512i __A, __mmask16 __U,
    __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_mask ((__v16si) __I
                       ,
             (__v16si) __A,
             (__v16si) __B,
             (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_epi32 (__m512i __A, __m512i __I,
     __mmask16 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2vard512_mask ((__v16si) __A,
             (__v16si) __I
                       ,
             (__v16si) __B,
             (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_epi32 (__mmask16 __U, __m512i __A,
     __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_maskz ((__v16si) __I
                 ,
       (__v16si) __A,
       (__v16si) __B,
       (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_pd (__m512d __A, __m512i __I, __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_mask ((__v8di) __I
                 ,
       (__v8df) __A,
       (__v8df) __B,
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_pd (__m512d __A, __mmask8 __U, __m512i __I,
        __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_mask ((__v8di) __I
                 ,
       (__v8df) __A,
       (__v8df) __B,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_pd (__m512d __A, __m512i __I, __mmask8 __U,
         __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermi2varpd512_mask ((__v8df) __A,
       (__v8di) __I
                 ,
       (__v8df) __B,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_pd (__mmask8 __U, __m512d __A, __m512i __I,
         __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_maskz ((__v8di) __I
                  ,
        (__v8df) __A,
        (__v8df) __B,
        (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_ps (__m512 __A, __m512i __I, __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_mask ((__v16si) __I
                       ,
             (__v16sf) __A,
             (__v16sf) __B,
             (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_ps (__m512 __A, __mmask16 __U, __m512i __I, __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_mask ((__v16si) __I
                       ,
             (__v16sf) __A,
             (__v16sf) __B,
             (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_ps (__m512 __A, __m512i __I, __mmask16 __U,
         __m512 __B)
{
  return (__m512) __builtin_ia32_vpermi2varps512_mask ((__v16sf) __A,
             (__v16si) __I
                       ,
             (__v16sf) __B,
             (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_ps (__mmask16 __U, __m512 __A, __m512i __I,
         __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_maskz ((__v16si) __I
                 ,
       (__v16sf) __A,
       (__v16sf) __B,
       (__mmask16) __U);
}


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permute_pd (__m512d __X, const int __C)
{
  return (__m512d) __builtin_ia32_vpermilpd512_mask ((__v8df) __X, __C,
           (__v8df)
           _mm512_undefined_pd (),
           (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permute_pd (__m512d __W, __mmask8 __U, __m512d __X, const int __C)
{
  return (__m512d) __builtin_ia32_vpermilpd512_mask ((__v8df) __X, __C,
           (__v8df) __W,
           (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permute_pd (__mmask8 __U, __m512d __X, const int __C)
{
  return (__m512d) __builtin_ia32_vpermilpd512_mask ((__v8df) __X, __C,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permute_ps (__m512 __X, const int __C)
{
  return (__m512) __builtin_ia32_vpermilps512_mask ((__v16sf) __X, __C,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permute_ps (__m512 __W, __mmask16 __U, __m512 __X, const int __C)
{
  return (__m512) __builtin_ia32_vpermilps512_mask ((__v16sf) __X, __C,
          (__v16sf) __W,
          (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permute_ps (__mmask16 __U, __m512 __X, const int __C)
{
  return (__m512) __builtin_ia32_vpermilps512_mask ((__v16sf) __X, __C,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U);
}
# 6791 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex_epi64 (__m512i __X, const int __I)
{
  return (__m512i) __builtin_ia32_permdi512_mask ((__v8di) __X, __I,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) (-1));
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex_epi64 (__m512i __W, __mmask8 __M,
       __m512i __X, const int __I)
{
  return (__m512i) __builtin_ia32_permdi512_mask ((__v8di) __X, __I,
        (__v8di) __W,
        (__mmask8) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex_epi64 (__mmask8 __M, __m512i __X, const int __I)
{
  return (__m512i) __builtin_ia32_permdi512_mask ((__v8di) __X, __I,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex_pd (__m512d __X, const int __M)
{
  return (__m512d) __builtin_ia32_permdf512_mask ((__v8df) __X, __M,
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex_pd (__m512d __W, __mmask8 __U, __m512d __X, const int __M)
{
  return (__m512d) __builtin_ia32_permdf512_mask ((__v8df) __X, __M,
        (__v8df) __W,
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex_pd (__mmask8 __U, __m512d __X, const int __M)
{
  return (__m512d) __builtin_ia32_permdf512_mask ((__v8df) __X, __M,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U);
}
# 6885 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_epi64 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
           (__v8di) __X,
           (__v8di)
           _mm512_setzero_si512 (),
           __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
           (__v8di) __X,
           (__v8di)
           _mm512_undefined_epi32 (),
           (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_epi64 (__m512i __W, __mmask8 __M, __m512i __X,
          __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
           (__v8di) __X,
           (__v8di) __W,
           __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_epi32 (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
           (__v16si) __X,
           (__v16si)
           _mm512_setzero_si512 (),
           __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
           (__v16si) __X,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_epi32 (__m512i __W, __mmask16 __M, __m512i __X,
          __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
           (__v16si) __X,
           (__v16si) __W,
           __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_pd (__m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
           (__v8di) __X,
           (__v8df)
           _mm512_undefined_pd (),
           (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_pd (__m512d __W, __mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
           (__v8di) __X,
           (__v8df) __W,
           (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_pd (__mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
           (__v8di) __X,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_ps (__m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
          (__v16si) __X,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_ps (__m512 __W, __mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
          (__v16si) __X,
          (__v16sf) __W,
          (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_ps (__mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
          (__v16si) __X,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U);
}


extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_ps (__m512 __M, __m512 __V, const int __imm)
{
  return (__m512) __builtin_ia32_shufps512_mask ((__v16sf) __M,
       (__v16sf) __V, __imm,
       (__v16sf)
       _mm512_undefined_ps (),
       (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_ps (__m512 __W, __mmask16 __U, __m512 __M,
   __m512 __V, const int __imm)
{
  return (__m512) __builtin_ia32_shufps512_mask ((__v16sf) __M,
       (__v16sf) __V, __imm,
       (__v16sf) __W,
       (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_ps (__mmask16 __U, __m512 __M, __m512 __V, const int __imm)
{
  return (__m512) __builtin_ia32_shufps512_mask ((__v16sf) __M,
       (__v16sf) __V, __imm,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_pd (__m512d __M, __m512d __V, const int __imm)
{
  return (__m512d) __builtin_ia32_shufpd512_mask ((__v8df) __M,
        (__v8df) __V, __imm,
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_pd (__m512d __W, __mmask8 __U, __m512d __M,
   __m512d __V, const int __imm)
{
  return (__m512d) __builtin_ia32_shufpd512_mask ((__v8df) __M,
        (__v8df) __V, __imm,
        (__v8df) __W,
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_pd (__mmask8 __U, __m512d __M, __m512d __V,
    const int __imm)
{
  return (__m512d) __builtin_ia32_shufpd512_mask ((__v8df) __M,
        (__v8df) __V, __imm,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fixupimm_round_pd (__m512d __A, __m512d __B, __m512i __C,
     const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_mask ((__v8df) __A,
            (__v8df) __B,
            (__v8di) __C,
            __imm,
            (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fixupimm_round_pd (__m512d __A, __mmask8 __U, __m512d __B,
          __m512i __C, const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_mask ((__v8df) __A,
            (__v8df) __B,
            (__v8di) __C,
            __imm,
            (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fixupimm_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
    __m512i __C, const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_maskz ((__v8df) __A,
             (__v8df) __B,
             (__v8di) __C,
             __imm,
             (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fixupimm_round_ps (__m512 __A, __m512 __B, __m512i __C,
     const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_fixupimmps512_mask ((__v16sf) __A,
           (__v16sf) __B,
           (__v16si) __C,
           __imm,
           (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fixupimm_round_ps (__m512 __A, __mmask16 __U, __m512 __B,
          __m512i __C, const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_fixupimmps512_mask ((__v16sf) __A,
           (__v16sf) __B,
           (__v16si) __C,
           __imm,
           (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fixupimm_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
    __m512i __C, const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_fixupimmps512_maskz ((__v16sf) __A,
            (__v16sf) __B,
            (__v16si) __C,
            __imm,
            (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_round_sd (__m128d __A, __m128d __B, __m128i __C,
         const int __imm, const int __R)
{
  return (__m128d) __builtin_ia32_fixupimmsd_mask ((__v2df) __A,
         (__v2df) __B,
         (__v2di) __C, __imm,
         (__mmask8) -1, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_round_sd (__m128d __A, __mmask8 __U, __m128d __B,
       __m128i __C, const int __imm, const int __R)
{
  return (__m128d) __builtin_ia32_fixupimmsd_mask ((__v2df) __A,
         (__v2df) __B,
         (__v2di) __C, __imm,
         (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
        __m128i __C, const int __imm, const int __R)
{
  return (__m128d) __builtin_ia32_fixupimmsd_maskz ((__v2df) __A,
          (__v2df) __B,
          (__v2di) __C,
          __imm,
          (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_round_ss (__m128 __A, __m128 __B, __m128i __C,
         const int __imm, const int __R)
{
  return (__m128) __builtin_ia32_fixupimmss_mask ((__v4sf) __A,
        (__v4sf) __B,
        (__v4si) __C, __imm,
        (__mmask8) -1, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_round_ss (__m128 __A, __mmask8 __U, __m128 __B,
       __m128i __C, const int __imm, const int __R)
{
  return (__m128) __builtin_ia32_fixupimmss_mask ((__v4sf) __A,
        (__v4sf) __B,
        (__v4si) __C, __imm,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
        __m128i __C, const int __imm, const int __R)
{
  return (__m128) __builtin_ia32_fixupimmss_maskz ((__v4sf) __A,
         (__v4sf) __B,
         (__v4si) __C, __imm,
         (__mmask8) __U, __R);
}
# 7320 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movehdup_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_movshdup512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_movehdup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movshdup512_mask ((__v16sf) __A,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_movehdup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movshdup512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_moveldup_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_movsldup512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_moveldup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movsldup512_mask ((__v16sf) __A,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_moveldup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_movsldup512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_or_si512 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A | (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_or_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A | (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_or_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pord512_mask ((__v16si) __A,
      (__v16si) __B,
      (__v16si) __W,
      (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_or_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pord512_mask ((__v16si) __A,
      (__v16si) __B,
      (__v16si)
      _mm512_setzero_si512 (),
      (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_or_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A | (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_or_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_porq512_mask ((__v8di) __A,
      (__v8di) __B,
      (__v8di) __W,
      (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_or_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_porq512_mask ((__v8di) __A,
      (__v8di) __B,
      (__v8di)
      _mm512_setzero_si512 (),
      (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_xor_si512 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A ^ (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_xor_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A ^ (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_xor_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pxord512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_xor_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pxord512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_xor_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A ^ (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_xor_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pxorq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_xor_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pxorq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rol_epi32 (__m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prold512_mask ((__v16si) __A, __B,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rol_epi32 (__m512i __W, __mmask16 __U, __m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prold512_mask ((__v16si) __A, __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rol_epi32 (__mmask16 __U, __m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prold512_mask ((__v16si) __A, __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ror_epi32 (__m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prord512_mask ((__v16si) __A, __B,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ror_epi32 (__m512i __W, __mmask16 __U, __m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prord512_mask ((__v16si) __A, __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_ror_epi32 (__mmask16 __U, __m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prord512_mask ((__v16si) __A, __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rol_epi64 (__m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prolq512_mask ((__v8di) __A, __B,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rol_epi64 (__m512i __W, __mmask8 __U, __m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prolq512_mask ((__v8di) __A, __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rol_epi64 (__mmask8 __U, __m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_prolq512_mask ((__v8di) __A, __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ror_epi64 (__m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prorq512_mask ((__v8di) __A, __B,
       (__v8di)
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ror_epi64 (__m512i __W, __mmask8 __U, __m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prorq512_mask ((__v8di) __A, __B,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_ror_epi64 (__mmask8 __U, __m512i __A, int __B)
{
  return (__m512i) __builtin_ia32_prorq512_mask ((__v8di) __A, __B,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}
# 7685 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_and_si512 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A & (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_and_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A & (__v16su) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_and_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_and_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandd512_mask ((__v16si) __A,
       (__v16si) __B,
       (__v16si)
       _mm512_setzero_si512 (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_and_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A & (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_and_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di) __W, __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_and_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandq512_mask ((__v8di) __A,
       (__v8di) __B,
       (__v8di)
       _mm512_setzero_pd (),
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_andnot_si512 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_andnot_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_andnot_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_andnot_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_andnot_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_andnot_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W, __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_andnot_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pandnq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_pd (),
        __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_test_epi32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ptestmd512 ((__v16si) __A,
      (__v16si) __B,
      (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_test_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ptestmd512 ((__v16si) __A,
      (__v16si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_test_epi64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq512 ((__v8di) __A,
            (__v8di) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_test_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq512 ((__v8di) __A, (__v8di) __B, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_testn_epi32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmd512 ((__v16si) __A,
       (__v16si) __B,
       (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_testn_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmd512 ((__v16si) __A,
       (__v16si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_testn_epi64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq512 ((__v8di) __A,
      (__v8di) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_testn_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq512 ((__v8di) __A,
      (__v8di) __B, __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_ps (__m512 __A)
{
  return (__m512) _mm512_and_epi32 ((__m512i) __A,
        _mm512_set1_epi32 (0x7fffffff));
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) _mm512_mask_and_epi32 ((__m512i) __W, __U, (__m512i) __A,
      _mm512_set1_epi32 (0x7fffffff));
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_pd (__m512d __A)
{
  return (__m512d) _mm512_and_epi64 ((__m512i) __A,
         _mm512_set1_epi64 (0x7fffffffffffffffLL));
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)
  _mm512_mask_and_epi64 ((__m512i) __W, __U, (__m512i) __A,
    _mm512_set1_epi64 (0x7fffffffffffffffLL));
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhdq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhdq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si) __W,
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhdq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di)
            _mm512_undefined_epi32 (),
            (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di) __W,
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckldq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckldq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si) __W,
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckldq512_mask ((__v16si) __A,
           (__v16si) __B,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di)
            _mm512_undefined_epi32 (),
            (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di) __W,
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklqdq512_mask ((__v8di) __A,
            (__v8di) __B,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U);
}



extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_u64 (__m128 __A, const int __R)
{
  return (unsigned long long) __builtin_ia32_vcvtss2usi64 ((__v4sf) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_si64 (__m128 __A, const int __R)
{
  return (long long) __builtin_ia32_vcvtss2si64 ((__v4sf) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_i64 (__m128 __A, const int __R)
{
  return (long long) __builtin_ia32_vcvtss2si64 ((__v4sf) __A, __R);
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_u64 (__m128 __A, const int __R)
{
  return (unsigned long long) __builtin_ia32_vcvttss2usi64 ((__v4sf) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_i64 (__m128 __A, const int __R)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_si64 (__m128 __A, const int __R)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A, __R);
}
# 8116 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_u32 (__m128 __A, const int __R)
{
  return (unsigned) __builtin_ia32_vcvtss2usi32 ((__v4sf) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_si32 (__m128 __A, const int __R)
{
  return (int) __builtin_ia32_vcvtss2si32 ((__v4sf) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_i32 (__m128 __A, const int __R)
{
  return (int) __builtin_ia32_vcvtss2si32 ((__v4sf) __A, __R);
}

extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_u32 (__m128 __A, const int __R)
{
  return (unsigned) __builtin_ia32_vcvttss2usi32 ((__v4sf) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_i32 (__m128 __A, const int __R)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundss_si32 (__m128 __A, const int __R)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A, __R);
}
# 8179 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_u64 (__m128d __A, const int __R)
{
  return (unsigned long long) __builtin_ia32_vcvtsd2usi64 ((__v2df) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_si64 (__m128d __A, const int __R)
{
  return (long long) __builtin_ia32_vcvtsd2si64 ((__v2df) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_i64 (__m128d __A, const int __R)
{
  return (long long) __builtin_ia32_vcvtsd2si64 ((__v2df) __A, __R);
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_u64 (__m128d __A, const int __R)
{
  return (unsigned long long) __builtin_ia32_vcvttsd2usi64 ((__v2df) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_si64 (__m128d __A, const int __R)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A, __R);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_i64 (__m128d __A, const int __R)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A, __R);
}
# 8242 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_u32 (__m128d __A, const int __R)
{
  return (unsigned) __builtin_ia32_vcvtsd2usi32 ((__v2df) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_si32 (__m128d __A, const int __R)
{
  return (int) __builtin_ia32_vcvtsd2si32 ((__v2df) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_i32 (__m128d __A, const int __R)
{
  return (int) __builtin_ia32_vcvtsd2si32 ((__v2df) __A, __R);
}

extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_u32 (__m128d __A, const int __R)
{
  return (unsigned) __builtin_ia32_vcvttsd2usi32 ((__v2df) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_i32 (__m128d __A, const int __R)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtt_roundsd_si32 (__m128d __A, const int __R)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A, __R);
}
# 8303 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movedup_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_movddup512_mask ((__v8df) __A,
         (__v8df)
         _mm512_undefined_pd (),
         (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_movedup_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_movddup512_mask ((__v8df) __A,
         (__v8df) __W,
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_movedup_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_movddup512_mask ((__v8df) __A,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpcklpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpcklpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpcklpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpckhpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpckhpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_unpckhpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpckhps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpckhps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpckhps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_pd (__m256 __A, const int __R)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_pd (__m512d __W, __mmask8 __U, __m256 __A,
       const int __R)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df) __W,
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_pd (__mmask8 __U, __m256 __A, const int __R)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundph_ps (__m256i __A, const int __R)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundph_ps (__m512 __W, __mmask16 __U, __m256i __A,
       const int __R)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf) __W,
          (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundph_ps (__mmask16 __U, __m256i __A, const int __R)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U, __R);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_ph (__m512 __A, const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi)
           _mm256_undefined_si256 (),
           -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_ph (__m512 __A, const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi)
           _mm256_undefined_si256 (),
           -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_ph (__m256i __U, __mmask16 __W, __m512 __A,
       const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi) __U,
           (__mmask16) __W);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_ph (__m256i __U, __mmask16 __W, __m512 __A, const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi) __U,
           (__mmask16) __W);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_ph (__mmask16 __W, __m512 __A, const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __W);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_ph (__mmask16 __W, __m512 __A, const int __I)
{
  return (__m256i) __builtin_ia32_vcvtps2ph512_mask ((__v16sf) __A,
           __I,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __W);
}
# 8593 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_ps (__m512d __A, const int __R)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf)
         _mm256_undefined_ps (),
         (__mmask8) -1, __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_ps (__m256 __W, __mmask8 __U, __m512d __A,
       const int __R)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf) __W,
         (__mmask8) __U, __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_ps (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_ss (__m128 __A, __m128d __B, const int __R)
{
  return (__m128) __builtin_ia32_cvtsd2ss_round ((__v4sf) __A,
       (__v2df) __B,
       __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_sd (__m128d __A, __m128 __B, const int __R)
{
  return (__m128d) __builtin_ia32_cvtss2sd_round ((__v2df) __A,
        (__v4sf) __B,
        __R);
}
# 8657 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_stream_si512 (__m512i * __P, __m512i __A)
{
  __builtin_ia32_movntdq512 ((__v8di *) __P, (__v8di) __A);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_stream_ps (float *__P, __m512 __A)
{
  __builtin_ia32_movntps512 (__P, (__v16sf) __A);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_stream_pd (double *__P, __m512d __A)
{
  __builtin_ia32_movntpd512 (__P, (__v8df) __A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_stream_load_si512 (void *__P)
{
  return __builtin_ia32_movntdqa512 ((__v8di *)__P);
}


typedef enum
{
  _MM_MANT_NORM_1_2,
  _MM_MANT_NORM_p5_2,
  _MM_MANT_NORM_p5_1,
  _MM_MANT_NORM_p75_1p5
} _MM_MANTISSA_NORM_ENUM;

typedef enum
{
  _MM_MANT_SIGN_src,
  _MM_MANT_SIGN_zero,
  _MM_MANT_SIGN_nan
} _MM_MANTISSA_SIGN_ENUM;


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_getexpss128_round ((__v4sf) __A,
          (__v4sf) __B,
          __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_getexpss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_getexpss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_getexpsd128_round ((__v2df) __A,
           (__v2df) __B,
           __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_getexpsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_getexpsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getexp_round_ps (__m512 __A, const int __R)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getexp_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
        const int __R)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf) __W,
         (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getexp_round_ps (__mmask16 __U, __m512 __A, const int __R)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getexp_round_pd (__m512d __A, const int __R)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getexp_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
        const int __R)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df) __W,
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getexp_round_pd (__mmask8 __U, __m512d __A, const int __R)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getmant_round_pd (__m512d __A, _MM_MANTISSA_NORM_ENUM __B,
    _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           _mm512_undefined_pd (),
           (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getmant_round_pd (__m512d __W, __mmask8 __U, __m512d __A,
         _MM_MANTISSA_NORM_ENUM __B,
         _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           (__v8df) __W, __U,
           __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getmant_round_pd (__mmask8 __U, __m512d __A,
          _MM_MANTISSA_NORM_ENUM __B,
          _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           (__v8df)
           _mm512_setzero_pd (),
           __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getmant_round_ps (__m512 __A, _MM_MANTISSA_NORM_ENUM __B,
    _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          _mm512_undefined_ps (),
          (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getmant_round_ps (__m512 __W, __mmask16 __U, __m512 __A,
         _MM_MANTISSA_NORM_ENUM __B,
         _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          (__v16sf) __W, __U,
          __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getmant_round_ps (__mmask16 __U, __m512 __A,
          _MM_MANTISSA_NORM_ENUM __B,
          _MM_MANTISSA_SIGN_ENUM __C, const int __R)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          (__v16sf)
          _mm512_setzero_ps (),
          __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_round_sd (__m128d __A, __m128d __B,
        _MM_MANTISSA_NORM_ENUM __C,
        _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128d) __builtin_ia32_getmantsd_round ((__v2df) __A,
        (__v2df) __B,
        (__D << 2) | __C,
         __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
         __m128d __B, _MM_MANTISSA_NORM_ENUM __C,
         _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128d) __builtin_ia32_getmantsd_mask_round ((__v2df) __A,
          (__v2df) __B,
          (__D << 2) | __C,
                                                    (__v2df) __W,
           __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
          _MM_MANTISSA_NORM_ENUM __C,
          _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128d) __builtin_ia32_getmantsd_mask_round ((__v2df) __A,
       (__v2df) __B,
              (__D << 2) | __C,
                                                        (__v2df)
                                                        _mm_setzero_pd(),
              __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_round_ss (__m128 __A, __m128 __B,
        _MM_MANTISSA_NORM_ENUM __C,
        _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128) __builtin_ia32_getmantss_round ((__v4sf) __A,
        (__v4sf) __B,
        (__D << 2) | __C,
        __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
         __m128 __B, _MM_MANTISSA_NORM_ENUM __C,
         _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128) __builtin_ia32_getmantss_mask_round ((__v4sf) __A,
          (__v4sf) __B,
          (__D << 2) | __C,
                                                    (__v4sf) __W,
           __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
          _MM_MANTISSA_NORM_ENUM __C,
          _MM_MANTISSA_SIGN_ENUM __D, const int __R)
{
  return (__m128) __builtin_ia32_getmantss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
              (__D << 2) | __C,
                                                        (__v4sf)
                                                        _mm_setzero_ps(),
              __U, __R);
}
# 9106 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_roundscale_round_ps (__m512 __A, const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A, __imm,
        (__v16sf)
        _mm512_undefined_ps (),
        -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_roundscale_round_ps (__m512 __A, __mmask16 __B, __m512 __C,
     const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __C, __imm,
        (__v16sf) __A,
        (__mmask16) __B, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_roundscale_round_ps (__mmask16 __A, __m512 __B,
      const int __imm, const int __R)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __B,
        __imm,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __A, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_roundscale_round_pd (__m512d __A, const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A, __imm,
         (__v8df)
         _mm512_undefined_pd (),
         -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_roundscale_round_pd (__m512d __A, __mmask8 __B,
     __m512d __C, const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __C, __imm,
         (__v8df) __A,
         (__mmask8) __B, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_roundscale_round_pd (__mmask8 __A, __m512d __B,
      const int __imm, const int __R)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __B,
         __imm,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __A, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_round_ss (__m128 __A, __m128 __B, const int __imm,
    const int __R)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __A,
       (__v4sf) __B, __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) -1,
       __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_round_ss (__m128 __A, __mmask8 __B, __m128 __C,
         __m128 __D, const int __imm, const int __R)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __C,
       (__v4sf) __D, __imm,
       (__v4sf) __A,
       (__mmask8) __B,
       __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_round_ss (__mmask8 __A, __m128 __B, __m128 __C,
          const int __imm, const int __R)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __B,
       (__v4sf) __C, __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __A,
       __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_round_sd (__m128d __A, __m128d __B, const int __imm,
    const int __R)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __A,
       (__v2df) __B, __imm,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) -1,
       __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_round_sd (__m128d __A, __mmask8 __B, __m128d __C,
         __m128d __D, const int __imm, const int __R)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __C,
       (__v2df) __D, __imm,
       (__v2df) __A,
       (__mmask8) __B,
       __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_round_sd (__mmask8 __A, __m128d __B, __m128d __C,
          const int __imm, const int __R)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __B,
       (__v2df) __C, __imm,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __A,
       __R);
}
# 9329 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_floor_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
        (0x01 | 0x00),
        (__v16sf) __A, -1,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_floor_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
         (0x01 | 0x00),
         (__v8df) __A, -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ceil_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
        (0x02 | 0x00),
        (__v16sf) __A, -1,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_ceil_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
         (0x02 | 0x00),
         (__v8df) __A, -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_floor_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
        (0x01 | 0x00),
        (__v16sf) __W, __U,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_floor_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
         (0x01 | 0x00),
         (__v8df) __W, __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ceil_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
        (0x02 | 0x00),
        (__v16sf) __W, __U,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_ceil_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
         (0x02 | 0x00),
         (__v8df) __W, __U,
         0x04);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_alignr_epi32 (__m512i __A, __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_alignd512_mask ((__v16si) __A,
        (__v16si) __B, __imm,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_alignr_epi32 (__m512i __W, __mmask16 __U, __m512i __A,
     __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_alignd512_mask ((__v16si) __A,
        (__v16si) __B, __imm,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_alignr_epi32 (__mmask16 __U, __m512i __A, __m512i __B,
      const int __imm)
{
  return (__m512i) __builtin_ia32_alignd512_mask ((__v16si) __A,
        (__v16si) __B, __imm,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_alignr_epi64 (__m512i __A, __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_alignq512_mask ((__v8di) __A,
        (__v8di) __B, __imm,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_alignr_epi64 (__m512i __W, __mmask8 __U, __m512i __A,
     __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_alignq512_mask ((__v8di) __A,
        (__v8di) __B, __imm,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_alignr_epi64 (__mmask8 __U, __m512i __A, __m512i __B,
      const int __imm)
{
  return (__m512i) __builtin_ia32_alignq512_mask ((__v8di) __A,
        (__v8di) __B, __imm,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}
# 9508 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epi32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqd512_mask ((__v16si) __A,
           (__v16si) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqd512_mask ((__v16si) __A,
           (__v16si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq512_mask ((__v8di) __A,
          (__v8di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epi64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq512_mask ((__v8di) __A,
          (__v8di) __B,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epi32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtd512_mask ((__v16si) __A,
           (__v16si) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtd512_mask ((__v16si) __A,
           (__v16si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq512_mask ((__v8di) __A,
          (__v8di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epi64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq512_mask ((__v8di) __A,
          (__v8di) __B,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epi32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 5,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epi32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 5,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epu32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 5,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epu32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 5,
          (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epi64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 5,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epi64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 5,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epu64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 5,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epu64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 5,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epi32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 2,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epi32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 2,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epu32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 2,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epu32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 2,
          (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epi64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 2,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epi64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 2,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epu64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 2,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epu64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 2,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epi32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 1,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epi32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 1,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epu32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 1,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epu32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 1,
          (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epi64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 1,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epi64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 1,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epu64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 1,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epu64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 1,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epi32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 4,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epi32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 4,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epu32_mask (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 4,
          (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epu32_mask (__m512i __X, __m512i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
          (__v16si) __Y, 4,
          (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epi64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 4,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epi64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 4,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epu64_mask (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 4,
          (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epu64_mask (__m512i __X, __m512i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
          (__v8di) __Y, 4,
          (__mmask8) -1);
}
# 9875 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftli_mask16 (__mmask16 __A, unsigned int __B)
{
  return (__mmask16) __builtin_ia32_kshiftlihi ((__mmask16) __A,
      (__mmask8) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftri_mask16 (__mmask16 __A, unsigned int __B)
{
  return (__mmask16) __builtin_ia32_kshiftrihi ((__mmask16) __A,
      (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epi64_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
       (__v8di) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epi32_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
        (__v16si) __Y, __P,
        (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epu64_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
        (__v8di) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epu32_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
         (__v16si) __Y, __P,
         (__mmask16) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_round_pd_mask (__m512d __X, __m512d __Y, const int __P,
     const int __R)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, __P,
        (__mmask8) -1, __R);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_round_ps_mask (__m512 __X, __m512 __Y, const int __P, const int __R)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, __P,
         (__mmask16) -1, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epi64_mask (__mmask8 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq512_mask ((__v8di) __X,
       (__v8di) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epi32_mask (__mmask16 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask16) __builtin_ia32_cmpd512_mask ((__v16si) __X,
        (__v16si) __Y, __P,
        (__mmask16) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epu64_mask (__mmask8 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __X,
        (__v8di) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epu32_mask (__mmask16 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __X,
         (__v16si) __Y, __P,
         (__mmask16) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_round_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y,
          const int __P, const int __R)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, __P,
        (__mmask8) __U, __R);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_round_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y,
          const int __P, const int __R)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, __P,
         (__mmask16) __U, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_round_sd_mask (__m128d __X, __m128d __Y, const int __P, const int __R)
{
  return (__mmask8) __builtin_ia32_cmpsd_mask ((__v2df) __X,
            (__v2df) __Y, __P,
            (__mmask8) -1, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_round_sd_mask (__mmask8 __M, __m128d __X, __m128d __Y,
       const int __P, const int __R)
{
  return (__mmask8) __builtin_ia32_cmpsd_mask ((__v2df) __X,
            (__v2df) __Y, __P,
            (__mmask8) __M, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_round_ss_mask (__m128 __X, __m128 __Y, const int __P, const int __R)
{
  return (__mmask8) __builtin_ia32_cmpss_mask ((__v4sf) __X,
            (__v4sf) __Y, __P,
            (__mmask8) -1, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_round_ss_mask (__mmask8 __M, __m128 __X, __m128 __Y,
       const int __P, const int __R)
{
  return (__mmask8) __builtin_ia32_cmpss_mask ((__v4sf) __X,
            (__v4sf) __Y, __P,
            (__mmask8) __M, __R);
}
# 10133 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32gather_ps (__m512i __index, void const *__addr, int __scale)
{
  __m512 __v1_old = _mm512_undefined_ps ();
  __mmask16 __mask = 0xFFFF;

  return (__m512) __builtin_ia32_gathersiv16sf ((__v16sf) __v1_old,
      __addr,
      (__v16si) __index,
      __mask, __scale);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32gather_ps (__m512 __v1_old, __mmask16 __mask,
     __m512i __index, void const *__addr, int __scale)
{
  return (__m512) __builtin_ia32_gathersiv16sf ((__v16sf) __v1_old,
      __addr,
      (__v16si) __index,
      __mask, __scale);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32gather_pd (__m256i __index, void const *__addr, int __scale)
{
  __m512d __v1_old = _mm512_undefined_pd ();
  __mmask8 __mask = 0xFF;

  return (__m512d) __builtin_ia32_gathersiv8df ((__v8df) __v1_old,
      __addr,
      (__v8si) __index, __mask,
      __scale);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32gather_pd (__m512d __v1_old, __mmask8 __mask,
     __m256i __index, void const *__addr, int __scale)
{
  return (__m512d) __builtin_ia32_gathersiv8df ((__v8df) __v1_old,
      __addr,
      (__v8si) __index,
      __mask, __scale);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64gather_ps (__m512i __index, void const *__addr, int __scale)
{
  __m256 __v1_old = _mm256_undefined_ps ();
  __mmask8 __mask = 0xFF;

  return (__m256) __builtin_ia32_gatherdiv16sf ((__v8sf) __v1_old,
      __addr,
      (__v8di) __index, __mask,
      __scale);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64gather_ps (__m256 __v1_old, __mmask8 __mask,
     __m512i __index, void const *__addr, int __scale)
{
  return (__m256) __builtin_ia32_gatherdiv16sf ((__v8sf) __v1_old,
      __addr,
      (__v8di) __index,
      __mask, __scale);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64gather_pd (__m512i __index, void const *__addr, int __scale)
{
  __m512d __v1_old = _mm512_undefined_pd ();
  __mmask8 __mask = 0xFF;

  return (__m512d) __builtin_ia32_gatherdiv8df ((__v8df) __v1_old,
      __addr,
      (__v8di) __index, __mask,
      __scale);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64gather_pd (__m512d __v1_old, __mmask8 __mask,
     __m512i __index, void const *__addr, int __scale)
{
  return (__m512d) __builtin_ia32_gatherdiv8df ((__v8df) __v1_old,
      __addr,
      (__v8di) __index,
      __mask, __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32gather_epi32 (__m512i __index, void const *__addr, int __scale)
{
  __m512i __v1_old = _mm512_undefined_epi32 ();
  __mmask16 __mask = 0xFFFF;

  return (__m512i) __builtin_ia32_gathersiv16si ((__v16si) __v1_old,
       __addr,
       (__v16si) __index,
       __mask, __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32gather_epi32 (__m512i __v1_old, __mmask16 __mask,
        __m512i __index, void const *__addr, int __scale)
{
  return (__m512i) __builtin_ia32_gathersiv16si ((__v16si) __v1_old,
       __addr,
       (__v16si) __index,
       __mask, __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32gather_epi64 (__m256i __index, void const *__addr, int __scale)
{
  __m512i __v1_old = _mm512_undefined_epi32 ();
  __mmask8 __mask = 0xFF;

  return (__m512i) __builtin_ia32_gathersiv8di ((__v8di) __v1_old,
      __addr,
      (__v8si) __index, __mask,
      __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32gather_epi64 (__m512i __v1_old, __mmask8 __mask,
        __m256i __index, void const *__addr,
        int __scale)
{
  return (__m512i) __builtin_ia32_gathersiv8di ((__v8di) __v1_old,
      __addr,
      (__v8si) __index,
      __mask, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64gather_epi32 (__m512i __index, void const *__addr, int __scale)
{
  __m256i __v1_old = _mm256_undefined_si256 ();
  __mmask8 __mask = 0xFF;

  return (__m256i) __builtin_ia32_gatherdiv16si ((__v8si) __v1_old,
       __addr,
       (__v8di) __index,
       __mask, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64gather_epi32 (__m256i __v1_old, __mmask8 __mask,
        __m512i __index, void const *__addr, int __scale)
{
  return (__m256i) __builtin_ia32_gatherdiv16si ((__v8si) __v1_old,
       __addr,
       (__v8di) __index,
       __mask, __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64gather_epi64 (__m512i __index, void const *__addr, int __scale)
{
  __m512i __v1_old = _mm512_undefined_epi32 ();
  __mmask8 __mask = 0xFF;

  return (__m512i) __builtin_ia32_gatherdiv8di ((__v8di) __v1_old,
      __addr,
      (__v8di) __index, __mask,
      __scale);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64gather_epi64 (__m512i __v1_old, __mmask8 __mask,
        __m512i __index, void const *__addr,
        int __scale)
{
  return (__m512i) __builtin_ia32_gatherdiv8di ((__v8di) __v1_old,
      __addr,
      (__v8di) __index,
      __mask, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32scatter_ps (void *__addr, __m512i __index, __m512 __v1, int __scale)
{
  __builtin_ia32_scattersiv16sf (__addr, (__mmask16) 0xFFFF,
     (__v16si) __index, (__v16sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32scatter_ps (void *__addr, __mmask16 __mask,
      __m512i __index, __m512 __v1, int __scale)
{
  __builtin_ia32_scattersiv16sf (__addr, __mask, (__v16si) __index,
     (__v16sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32scatter_pd (void *__addr, __m256i __index, __m512d __v1,
        int __scale)
{
  __builtin_ia32_scattersiv8df (__addr, (__mmask8) 0xFF,
    (__v8si) __index, (__v8df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32scatter_pd (void *__addr, __mmask8 __mask,
      __m256i __index, __m512d __v1, int __scale)
{
  __builtin_ia32_scattersiv8df (__addr, __mask, (__v8si) __index,
    (__v8df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64scatter_ps (void *__addr, __m512i __index, __m256 __v1, int __scale)
{
  __builtin_ia32_scatterdiv16sf (__addr, (__mmask8) 0xFF,
     (__v8di) __index, (__v8sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64scatter_ps (void *__addr, __mmask8 __mask,
      __m512i __index, __m256 __v1, int __scale)
{
  __builtin_ia32_scatterdiv16sf (__addr, __mask, (__v8di) __index,
     (__v8sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64scatter_pd (void *__addr, __m512i __index, __m512d __v1,
        int __scale)
{
  __builtin_ia32_scatterdiv8df (__addr, (__mmask8) 0xFF,
    (__v8di) __index, (__v8df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64scatter_pd (void *__addr, __mmask8 __mask,
      __m512i __index, __m512d __v1, int __scale)
{
  __builtin_ia32_scatterdiv8df (__addr, __mask, (__v8di) __index,
    (__v8df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32scatter_epi32 (void *__addr, __m512i __index,
    __m512i __v1, int __scale)
{
  __builtin_ia32_scattersiv16si (__addr, (__mmask16) 0xFFFF,
     (__v16si) __index, (__v16si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32scatter_epi32 (void *__addr, __mmask16 __mask,
         __m512i __index, __m512i __v1, int __scale)
{
  __builtin_ia32_scattersiv16si (__addr, __mask, (__v16si) __index,
     (__v16si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i32scatter_epi64 (void *__addr, __m256i __index,
    __m512i __v1, int __scale)
{
  __builtin_ia32_scattersiv8di (__addr, (__mmask8) 0xFF,
    (__v8si) __index, (__v8di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i32scatter_epi64 (void *__addr, __mmask8 __mask,
         __m256i __index, __m512i __v1, int __scale)
{
  __builtin_ia32_scattersiv8di (__addr, __mask, (__v8si) __index,
    (__v8di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64scatter_epi32 (void *__addr, __m512i __index,
    __m256i __v1, int __scale)
{
  __builtin_ia32_scatterdiv16si (__addr, (__mmask8) 0xFF,
     (__v8di) __index, (__v8si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64scatter_epi32 (void *__addr, __mmask8 __mask,
         __m512i __index, __m256i __v1, int __scale)
{
  __builtin_ia32_scatterdiv16si (__addr, __mask, (__v8di) __index,
     (__v8si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_i64scatter_epi64 (void *__addr, __m512i __index,
    __m512i __v1, int __scale)
{
  __builtin_ia32_scatterdiv8di (__addr, (__mmask8) 0xFF,
    (__v8di) __index, (__v8di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_i64scatter_epi64 (void *__addr, __mmask8 __mask,
         __m512i __index, __m512i __v1, int __scale)
{
  __builtin_ia32_scatterdiv8di (__addr, __mask, (__v8di) __index,
    (__v8di) __v1, __scale);
}
# 10656 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
            (__v8df) __W,
            (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
            (__v8df)
            _mm512_setzero_pd (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_compressstoredf512_mask ((__v8df *) __P, (__v8df) __A,
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
           (__v16sf) __W,
           (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
           (__v16sf)
           _mm512_setzero_ps (),
           (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_compressstoresf512_mask ((__v16sf *) __P, (__v16sf) __A,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
            (__v8di) __W,
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_compressstoredi512_mask ((__v8di *) __P, (__v8di) __A,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
            (__v16si) __W,
            (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
            (__v16si)
            _mm512_setzero_si512 (),
            (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_compressstoresi512_mask ((__v16si *) __P, (__v16si) __A,
       (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_maskz ((__v8df) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((const __v8df *) __P,
       (__v8df) __W,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_pd (__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_maskz ((const __v8df *) __P,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_maskz ((__v16sf) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((const __v16sf *) __P,
             (__v16sf) __W,
             (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_ps (__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_maskz ((const __v16sf *) __P,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_maskz ((__v8di) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((const __v8di *) __P,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m512i)
  __builtin_ia32_expandloaddi512_maskz ((const __v8di *) __P,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_maskz ((__v16si) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((const __v16si *) __P,
       (__v16si) __W,
       (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_epi32 (__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_maskz ((const __v16si *) __P,
        (__v16si)
        _mm512_setzero_si512
        (), (__mmask16) __U);
}
# 10925 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortest_mask16_u8 (__mmask16 __A, __mmask16 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_kortestchi (__A, __B);
  return (unsigned char) __builtin_ia32_kortestzhi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestz_mask16_u8 (__mmask16 __A, __mmask16 __B)
{
  return (unsigned char) __builtin_ia32_kortestzhi ((__mmask16) __A,
          (__mmask16) __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestc_mask16_u8 (__mmask16 __A, __mmask16 __B)
{
  return (unsigned char) __builtin_ia32_kortestchi ((__mmask16) __A,
          (__mmask16) __B);
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtmask16_u32 (__mmask16 __A)
{
  return (unsigned int) __builtin_ia32_kmovw ((__mmask16 ) __A);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtu32_mask16 (unsigned int __A)
{
  return (__mmask16) __builtin_ia32_kmovw ((__mmask16 ) __A);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_load_mask16 (__mmask16 *__A)
{
  return (__mmask16) __builtin_ia32_kmovw (*(__mmask16 *) __A);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_store_mask16 (__mmask16 *__A, __mmask16 __B)
{
  *(__mmask16 *) __A = __builtin_ia32_kmovw (__B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kand (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kandn (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandnhi ((__mmask16) __A,
          (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_korhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kortestz (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kortestzhi ((__mmask16) __A,
      (__mmask16) __B);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kortestc (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kortestchi ((__mmask16) __A,
      (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kxnor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxnorhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kxor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxorhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_knot (__mmask16 __A)
{
  return (__mmask16) __builtin_ia32_knothi ((__mmask16) __A);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kunpackb (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kunpckhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kunpackb_mask16 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask16) __builtin_ia32_kunpckhi ((__mmask16) __A, (__mmask16) __B);
}


extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_inserti32x4 (__mmask16 __B, __m512i __C, __m128i __D,
     const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x4_mask ((__v16si) __C,
          (__v4si) __D,
          __imm,
          (__v16si)
          _mm512_setzero_si512 (),
          __B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_insertf32x4 (__mmask16 __B, __m512 __C, __m128 __D,
     const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x4_mask ((__v16sf) __C,
         (__v4sf) __D,
         __imm,
         (__v16sf)
         _mm512_setzero_ps (), __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_inserti32x4 (__m512i __A, __mmask16 __B, __m512i __C,
    __m128i __D, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x4_mask ((__v16si) __C,
          (__v4si) __D,
          __imm,
          (__v16si) __A,
          __B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_insertf32x4 (__m512 __A, __mmask16 __B, __m512 __C,
    __m128 __D, const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x4_mask ((__v16sf) __C,
         (__v4sf) __D,
         __imm,
         (__v16sf) __A, __B);
}
# 11120 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epu64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epu64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_undefined_epi32 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epu32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epu32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_undefined_epi32 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si)
        _mm512_setzero_si512 (),
        __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
        (__v16si) __B,
        (__v16si) __W, __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpcklps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpcklps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_unpcklps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_maxsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_maxsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_maxsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_maxss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_maxss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_maxss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_round_sd (__m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_minsd_round ((__v2df) __A,
            (__v2df) __B,
            __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_round_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_minsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_round_sd (__mmask8 __U, __m128d __A, __m128d __B,
      const int __R)
{
  return (__m128d) __builtin_ia32_minsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_round_ss (__m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_minss_round ((__v4sf) __A,
           (__v4sf) __B,
           __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_round_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_minss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_round_ss (__mmask8 __U, __m128 __A, __m128 __B,
      const int __R)
{
  return (__m128) __builtin_ia32_minss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U, __R);
}
# 11568 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_pd (__mmask8 __U, __m512d __A, __m512d __W)
{
  return (__m512d) __builtin_ia32_blendmpd_512_mask ((__v8df) __A,
           (__v8df) __W,
           (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_ps (__mmask16 __U, __m512 __A, __m512 __W)
{
  return (__m512) __builtin_ia32_blendmps_512_mask ((__v16sf) __A,
          (__v16sf) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_epi64 (__mmask8 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_blendmq_512_mask ((__v8di) __A,
          (__v8di) __W,
          (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_epi32 (__mmask16 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_blendmd_512_mask ((__v16si) __A,
          (__v16si) __W,
          (__mmask16) __U);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_round_sd (__m128d __W, __m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_round ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_round_ss (__m128 __W, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_round ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_round_sd (__m128d __W, __m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_round ((__v2df) __W,
         (__v2df) __A,
         -(__v2df) __B,
         __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_round_ss (__m128 __W, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_round ((__v4sf) __W,
        (__v4sf) __A,
        -(__v4sf) __B,
        __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_round_sd (__m128d __W, __m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_round ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_round_ss (__m128 __W, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_round ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_round_sd (__m128d __W, __m128d __A, __m128d __B, const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_round ((__v2df) __W,
         -(__v2df) __A,
         -(__v2df) __B,
         __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_round_ss (__m128 __W, __m128 __A, __m128 __B, const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_round ((__v4sf) __W,
        -(__v4sf) __A,
        -(__v4sf) __B,
        __R);
}
# 11710 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        (__v2df) __A,
        (__v2df) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       (__v4sf) __A,
       (__v4sf) __B,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask3 ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask3 ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        (__v2df) __A,
        -(__v2df) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       (__v4sf) __A,
       -(__v4sf) __B,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubsd3_mask3 ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubss3_mask3 ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         (__v2df) __A,
         -(__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        (__v4sf) __A,
        -(__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        -(__v2df) __A,
        (__v2df) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       -(__v4sf) __A,
       (__v4sf) __B,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask3 ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask3 ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        -(__v2df) __A,
        -(__v2df) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       -(__v4sf) __A,
       -(__v4sf) __B,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubsd3_mask3 ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubss3_mask3 ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         -(__v2df) __A,
         -(__v2df) __B,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        -(__v4sf) __A,
        -(__v4sf) __B,
        (__mmask8) __U,
        0x04);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
    const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        (__v2df) __A,
        (__v2df) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       (__v4sf) __A,
       (__v4sf) __B,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_round_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask3 ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_round_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask3 ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_round_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_round_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
    const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        (__v2df) __A,
        -(__v2df) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       (__v4sf) __A,
       -(__v4sf) __B,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_round_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmsubsd3_mask3 ((__v2df) __W,
         (__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_round_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmsubss3_mask3 ((__v4sf) __W,
        (__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_round_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         (__v2df) __A,
         -(__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_round_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        (__v4sf) __A,
        -(__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
    const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        -(__v2df) __A,
        (__v2df) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       -(__v4sf) __A,
       (__v4sf) __B,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_round_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask3 ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_round_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask3 ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_round_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_round_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
    const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_mask ((__v2df) __W,
        -(__v2df) __A,
        -(__v2df) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
       -(__v4sf) __A,
       -(__v4sf) __B,
       (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_round_sd (__m128d __W, __m128d __A, __m128d __B, __mmask8 __U,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmsubsd3_mask3 ((__v2df) __W,
         -(__v2df) __A,
         (__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_round_ss (__m128 __W, __m128 __A, __m128 __B, __mmask8 __U,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmsubss3_mask3 ((__v4sf) __W,
        -(__v4sf) __A,
        (__v4sf) __B,
        (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_round_sd (__mmask8 __U, __m128d __W, __m128d __A, __m128d __B,
     const int __R)
{
  return (__m128d) __builtin_ia32_vfmaddsd3_maskz ((__v2df) __W,
         -(__v2df) __A,
         -(__v2df) __B,
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_round_ss (__mmask8 __U, __m128 __W, __m128 __A, __m128 __B,
     const int __R)
{
  return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __W,
        -(__v4sf) __A,
        -(__v4sf) __B,
        (__mmask8) __U, __R);
}
# 12313 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_comi_round_ss (__m128 __A, __m128 __B, const int __P, const int __R)
{
  return __builtin_ia32_vcomiss ((__v4sf) __A, (__v4sf) __B, __P, __R);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_comi_round_sd (__m128d __A, __m128d __B, const int __P, const int __R)
{
  return __builtin_ia32_vcomisd ((__v2df) __A, (__v2df) __B, __P, __R);
}







extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sqrt_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df)
        _mm512_undefined_pd (),
        (__mmask8) -1,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sqrt_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df) __W,
        (__mmask8) __U,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sqrt_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U,
        0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sqrt_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf)
       _mm512_undefined_ps (),
       (__mmask16) -1,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sqrt_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf) __W,
       (__mmask16) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sqrt_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_sqrtps512_mask ((__v16sf) __A,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_pd (__m512d __A, __m512d __B)
{
  return (__m512d) ((__v8df)__A + (__v8df)__B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_addpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_addpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_ps (__m512 __A, __m512 __B)
{
  return (__m512) ((__v16sf)__A + (__v16sf)__B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_addps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_addps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_addsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_addsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df)
      _mm_setzero_pd (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_addss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_addss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_pd (__m512d __A, __m512d __B)
{
  return (__m512d) ((__v8df)__A - (__v8df)__B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_subpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_subpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_ps (__m512 __A, __m512 __B)
{
  return (__m512) ((__v16sf)__A - (__v16sf)__B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_subps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_subps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_subsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_subsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df)
      _mm_setzero_pd (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_subss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_subss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_pd (__m512d __A, __m512d __B)
{
  return (__m512d) ((__v8df)__A * (__v8df)__B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_mulpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_mulpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mul_ps (__m512 __A, __m512 __B)
{
  return (__m512) ((__v16sf)__A * (__v16sf)__B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mul_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_mulps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mul_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_mulps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B)
{
  return (__m128d) __builtin_ia32_mulsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_mulsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B)
{
  return (__m128) __builtin_ia32_mulss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_mulss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_div_pd (__m512d __M, __m512d __V)
{
  return (__m512d) ((__v8df)__M / (__v8df)__V);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_div_pd (__m512d __W, __mmask8 __U, __m512d __M, __m512d __V)
{
  return (__m512d) __builtin_ia32_divpd512_mask ((__v8df) __M,
       (__v8df) __V,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_div_pd (__mmask8 __U, __m512d __M, __m512d __V)
{
  return (__m512d) __builtin_ia32_divpd512_mask ((__v8df) __M,
       (__v8df) __V,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_div_ps (__m512 __A, __m512 __B)
{
  return (__m512) ((__v16sf)__A / (__v16sf)__B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_div_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_divps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_div_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_divps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_sd (__m128d __W, __mmask8 __U, __m128d __A,
     __m128d __B)
{
  return (__m128d) __builtin_ia32_divsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_divsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_ss (__m128 __W, __mmask8 __U, __m128 __A,
     __m128 __B)
{
  return (__m128) __builtin_ia32_divss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_divss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_maxsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_maxsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_undefined_pd (),
       (__mmask8) -1,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_undefined_ps (),
      (__mmask16) -1,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U,
      0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_minsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_minsd_mask_round ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U,
       0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_scalef_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_scalef_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_scalef_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_scalef_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_scalef_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __W,
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_scalef_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefsd_mask_round ((__v2df) __A,
          (__v2df) __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefss_mask_round ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmadd_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmadd_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsub_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsub_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask ((__v8df) __A,
          (__v8df) __B,
          (__v8df) __C,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsub_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask3 ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsub_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_maskz ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsub_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsub_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask ((__v16sf) __A,
         (__v16sf) __B,
         (__v16sf) __C,
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsub_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask3 ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsub_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmsubps512_maskz ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df) __C,
             (__mmask8) -1,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df) __C,
             (__mmask8) __U,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask3 ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf) __C,
            (__mmask16) -1,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf) __C,
            (__mmask16) __U,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask3 ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U,
             0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsubadd_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             -(__v8df) __C,
             (__mmask8) -1,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
             (__v8df) __B,
             -(__v8df) __C,
             (__mmask8) __U,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubaddpd512_mask3 ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __C,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
       (__v8df) __B,
       -(__v8df) __C,
       (__mmask8) __U,
       0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsubadd_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            -(__v16sf) __C,
            (__mmask16) -1,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            -(__v16sf) __C,
            (__mmask16) __U,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubaddps512_mask3 ((__v16sf) __A,
             (__v16sf) __B,
             (__v16sf) __C,
             (__mmask16) __U,
             0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
             (__v16sf) __B,
             -(__v16sf) __C,
             (__mmask16) __U,
             0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) -1,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmadd_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmadd_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask3 ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmadd_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_maskz ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) -1,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmadd_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmadd_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask3 ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmadd_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmaddps512_maskz ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmsub_pd (__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) -1,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmsub_pd (__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask ((__v8df) __A,
           (__v8df) __B,
           (__v8df) __C,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmsub_pd (__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask3 ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmsub_pd (__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_maskz ((__v8df) __A,
            (__v8df) __B,
            (__v8df) __C,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmsub_ps (__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) -1,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmsub_ps (__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask ((__v16sf) __A,
          (__v16sf) __B,
          (__v16sf) __C,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmsub_ps (__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask3 ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmsub_ps (__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmsubps512_maskz ((__v16sf) __A,
           (__v16sf) __B,
           (__v16sf) __C,
           (__mmask16) __U,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttpd_epi32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_undefined_si256 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si)
            _mm256_undefined_si256 (),
            (__mmask8) -1,
            0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si) __W,
            (__mmask8) __U,
            0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
            (__v8si)
            _mm256_setzero_si256 (),
            (__mmask8) __U,
            0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_epi32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si)
          _mm256_undefined_si256 (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_undefined_si256 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttps_epi32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si) __W,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttps_epu32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si)
            _mm512_undefined_epi32 (),
            (__mmask16) -1,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si) __W,
            (__mmask16) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttps_epu32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
            (__v16si)
            _mm512_setzero_si512 (),
            (__mmask16) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_epi32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si)
          _mm512_undefined_epi32 (),
          (__mmask16) -1,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si) __W,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_epu32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_undefined_epi32 (),
           (__mmask16) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si) __W,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_epu32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
           (__v16si)
           _mm512_setzero_si512 (),
           (__mmask16) __U,
           0x04);
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsd_f64 (__m512d __A)
{
  return __A[0];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtss_f32 (__m512 __A)
{
  return __A[0];
}


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtu64_ss (__m128 __A, unsigned long long __B)
{
  return (__m128) __builtin_ia32_cvtusi2ss64 ((__v4sf) __A, __B,
           0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtu64_sd (__m128d __A, unsigned long long __B)
{
  return (__m128d) __builtin_ia32_cvtusi2sd64 ((__v2df) __A, __B,
            0x04);
}


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtu32_ss (__m128 __A, unsigned __B)
{
  return (__m128) __builtin_ia32_cvtusi2ss32 ((__v4sf) __A, __B,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi32_ps (__m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf) __W,
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu32_ps (__m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf) __W,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U,
          0x04);
}


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fixupimm_pd (__m512d __A, __m512d __B, __m512i __C, const int __imm)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_mask ((__v8df) __A,
            (__v8df) __B,
            (__v8di) __C,
            __imm,
            (__mmask8) -1,
            0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fixupimm_pd (__m512d __A, __mmask8 __U, __m512d __B,
    __m512i __C, const int __imm)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_mask ((__v8df) __A,
            (__v8df) __B,
            (__v8di) __C,
            __imm,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fixupimm_pd (__mmask8 __U, __m512d __A, __m512d __B,
     __m512i __C, const int __imm)
{
  return (__m512d) __builtin_ia32_fixupimmpd512_maskz ((__v8df) __A,
             (__v8df) __B,
             (__v8di) __C,
             __imm,
             (__mmask8) __U,
             0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fixupimm_ps (__m512 __A, __m512 __B, __m512i __C, const int __imm)
{
  return (__m512) __builtin_ia32_fixupimmps512_mask ((__v16sf) __A,
           (__v16sf) __B,
           (__v16si) __C,
           __imm,
           (__mmask16) -1,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fixupimm_ps (__m512 __A, __mmask16 __U, __m512 __B,
    __m512i __C, const int __imm)
{
  return (__m512) __builtin_ia32_fixupimmps512_mask ((__v16sf) __A,
           (__v16sf) __B,
           (__v16si) __C,
           __imm,
           (__mmask16) __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fixupimm_ps (__mmask16 __U, __m512 __A, __m512 __B,
     __m512i __C, const int __imm)
{
  return (__m512) __builtin_ia32_fixupimmps512_maskz ((__v16sf) __A,
            (__v16sf) __B,
            (__v16si) __C,
            __imm,
            (__mmask16) __U,
            0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_sd (__m128d __A, __m128d __B, __m128i __C, const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmsd_mask ((__v2df) __A,
         (__v2df) __B,
         (__v2di) __C, __imm,
         (__mmask8) -1,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_sd (__m128d __A, __mmask8 __U, __m128d __B,
        __m128i __C, const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmsd_mask ((__v2df) __A,
         (__v2df) __B,
         (__v2di) __C, __imm,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_sd (__mmask8 __U, __m128d __A, __m128d __B,
         __m128i __C, const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmsd_maskz ((__v2df) __A,
          (__v2df) __B,
          (__v2di) __C,
          __imm,
          (__mmask8) __U,
          0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_ss (__m128 __A, __m128 __B, __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmss_mask ((__v4sf) __A,
        (__v4sf) __B,
        (__v4si) __C, __imm,
        (__mmask8) -1,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_ss (__m128 __A, __mmask8 __U, __m128 __B,
        __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmss_mask ((__v4sf) __A,
        (__v4sf) __B,
        (__v4si) __C, __imm,
        (__mmask8) __U,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_ss (__mmask8 __U, __m128 __A, __m128 __B,
         __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmss_maskz ((__v4sf) __A,
         (__v4sf) __B,
         (__v4si) __C, __imm,
         (__mmask8) __U,
         0x04);
}
# 14249 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvtss2usi64 ((__v4sf)
          __A,
          0x04);
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvttss2usi64 ((__v4sf)
           __A,
           0x04);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_i64 (__m128 __A)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A,
        0x04);
}


extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvtss2usi32 ((__v4sf) __A,
       0x04);
}

extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvttss2usi32 ((__v4sf) __A,
        0x04);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttss_i32 (__m128 __A)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A,
         0x04);
}


extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvtsd2usi64 ((__v2df)
          __A,
          0x04);
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvttsd2usi64 ((__v2df)
           __A,
           0x04);
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_i64 (__m128d __A)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A,
        0x04);
}


extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvtsd2usi32 ((__v2df) __A,
       0x04);
}

extern __inline unsigned
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvttsd2usi32 ((__v2df) __A,
        0x04);
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttsd_i32 (__m128d __A)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_pd (__m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_pd (__m512d __W, __mmask8 __U, __m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_pd (__mmask8 __U, __m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtph_ps (__m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf)
          _mm512_undefined_ps (),
          (__mmask16) -1,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtph_ps (__m512 __W, __mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf) __W,
          (__mmask16) __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtph_ps (__mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
          (__v16sf)
          _mm512_setzero_ps (),
          (__mmask16) __U,
          0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_ps (__m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf)
         _mm256_undefined_ps (),
         (__mmask8) -1,
         0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_ps (__m256 __W, __mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf) __W,
         (__mmask8) __U,
         0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_ps (__mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U,
         0x04);
}


extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getexp_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_undefined_ps (),
         (__mmask16) -1,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getexp_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf) __W,
         (__mmask16) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getexp_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getexp_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df)
          _mm512_undefined_pd (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getexp_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getexp_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss128_round ((__v4sf) __A,
          (__v4sf) __B,
          0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss_mask_round ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd128_round ((__v2df) __A,
           (__v2df) __B,
           0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df) __W,
      (__mmask8) __U,
      0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd_mask_round ((__v2df) __A,
      (__v2df) __B,
      (__v2df)
      _mm_setzero_pd (),
      (__mmask8) __U,
      0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getmant_pd (__m512d __A, _MM_MANTISSA_NORM_ENUM __B,
     _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           _mm512_undefined_pd (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getmant_pd (__m512d __W, __mmask8 __U, __m512d __A,
   _MM_MANTISSA_NORM_ENUM __B, _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           (__v8df) __W, __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getmant_pd (__mmask8 __U, __m512d __A,
    _MM_MANTISSA_NORM_ENUM __B, _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512d) __builtin_ia32_getmantpd512_mask ((__v8df) __A,
           (__C << 2) | __B,
           (__v8df)
           _mm512_setzero_pd (),
           __U,
           0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_getmant_ps (__m512 __A, _MM_MANTISSA_NORM_ENUM __B,
     _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          _mm512_undefined_ps (),
          (__mmask16) -1,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_getmant_ps (__m512 __W, __mmask16 __U, __m512 __A,
   _MM_MANTISSA_NORM_ENUM __B, _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          (__v16sf) __W, __U,
          0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_getmant_ps (__mmask16 __U, __m512 __A,
    _MM_MANTISSA_NORM_ENUM __B, _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m512) __builtin_ia32_getmantps512_mask ((__v16sf) __A,
          (__C << 2) | __B,
          (__v16sf)
          _mm512_setzero_ps (),
          __U,
          0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_sd (__m128d __A, __m128d __B, _MM_MANTISSA_NORM_ENUM __C,
  _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128d) __builtin_ia32_getmantsd_round ((__v2df) __A,
         (__v2df) __B,
         (__D << 2) | __C,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
   _MM_MANTISSA_NORM_ENUM __C, _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128d) __builtin_ia32_getmantsd_mask_round ((__v2df) __A,
       (__v2df) __B,
              (__D << 2) | __C,
                                                        (__v2df) __W,
             __U,
           0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_sd (__mmask8 __U, __m128d __A, __m128d __B,
    _MM_MANTISSA_NORM_ENUM __C, _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128d) __builtin_ia32_getmantsd_mask_round ((__v2df) __A,
                                                        (__v2df) __B,
              (__D << 2) | __C,
                                                        (__v2df)
       _mm_setzero_pd(),
              __U,
           0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_ss (__m128 __A, __m128 __B, _MM_MANTISSA_NORM_ENUM __C,
  _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128) __builtin_ia32_getmantss_round ((__v4sf) __A,
        (__v4sf) __B,
        (__D << 2) | __C,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
   _MM_MANTISSA_NORM_ENUM __C, _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128) __builtin_ia32_getmantss_mask_round ((__v4sf) __A,
       (__v4sf) __B,
              (__D << 2) | __C,
                                                        (__v4sf) __W,
             __U,
           0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_ss (__mmask8 __U, __m128 __A, __m128 __B,
    _MM_MANTISSA_NORM_ENUM __C, _MM_MANTISSA_SIGN_ENUM __D)
{
  return (__m128) __builtin_ia32_getmantss_mask_round ((__v4sf) __A,
                                                        (__v4sf) __B,
              (__D << 2) | __C,
                                                        (__v4sf)
       _mm_setzero_ps(),
              __U,
           0x04);
}
# 14860 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_roundscale_ps (__m512 __A, const int __imm)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A, __imm,
        (__v16sf)
        _mm512_undefined_ps (),
        -1,
        0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_roundscale_ps (__m512 __A, __mmask16 __B, __m512 __C,
      const int __imm)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __C, __imm,
        (__v16sf) __A,
        (__mmask16) __B,
        0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_roundscale_ps (__mmask16 __A, __m512 __B, const int __imm)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __B,
        __imm,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __A,
        0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_roundscale_pd (__m512d __A, const int __imm)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A, __imm,
         (__v8df)
         _mm512_undefined_pd (),
         -1,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_roundscale_pd (__m512d __A, __mmask8 __B, __m512d __C,
      const int __imm)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __C, __imm,
         (__v8df) __A,
         (__mmask8) __B,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_roundscale_pd (__mmask8 __A, __m512d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __B,
         __imm,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __A,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_ss (__m128 __A, __m128 __B, const int __imm)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __A,
       (__v4sf) __B, __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) -1,
       0x04);
}


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_ss (__m128 __A, __mmask8 __B, __m128 __C, __m128 __D,
   const int __imm)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __C,
       (__v4sf) __D, __imm,
       (__v4sf) __A,
       (__mmask8) __B,
       0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_ss (__mmask8 __A, __m128 __B, __m128 __C,
    const int __imm)
{
  return (__m128)
    __builtin_ia32_rndscaless_mask_round ((__v4sf) __B,
       (__v4sf) __C, __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __A,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_sd (__m128d __A, __m128d __B, const int __imm)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __A,
       (__v2df) __B, __imm,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) -1,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_sd (__m128d __A, __mmask8 __B, __m128d __C, __m128d __D,
   const int __imm)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __C,
       (__v2df) __D, __imm,
       (__v2df) __A,
       (__mmask8) __B,
       0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_sd (__mmask8 __A, __m128d __B, __m128d __C,
    const int __imm)
{
  return (__m128d)
    __builtin_ia32_rndscalesd_mask_round ((__v2df) __B,
       (__v2df) __C, __imm,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __A,
       0x04);
}
# 15087 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_pd_mask (__m512d __X, __m512d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, __P,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_ps_mask (__m512 __X, __m512 __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, __P,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, __P,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, __P,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_sd_mask (__m128d __X, __m128d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpsd_mask ((__v2df) __X,
            (__v2df) __Y, __P,
            (__mmask8) -1,
            0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_sd_mask (__mmask8 __M, __m128d __X, __m128d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpsd_mask ((__v2df) __X,
            (__v2df) __Y, __P,
            (__mmask8) __M,
            0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_ss_mask (__m128 __X, __m128 __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpss_mask ((__v4sf) __X,
            (__v4sf) __Y, __P,
            (__mmask8) -1,
            0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_ss_mask (__mmask8 __M, __m128 __X, __m128 __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpss_mask ((__v4sf) __X,
            (__v4sf) __Y, __P,
            (__mmask8) __M,
            0x04);
}
# 15209 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x00,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x00,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x01,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x01,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x02,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x02,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpunord_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x03,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpunord_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x03,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x04,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x04,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpnlt_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x05,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpnlt_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x05,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpnle_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x06,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpnle_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x06,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpord_pd_mask (__m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x07,
        (__mmask8) -1,
        0x04);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpord_pd_mask (__mmask8 __U, __m512d __X, __m512d __Y)
{
  return (__mmask8) __builtin_ia32_cmppd512_mask ((__v8df) __X,
        (__v8df) __Y, 0x07,
        (__mmask8) __U,
        0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x00,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x00,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x01,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x01,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x02,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x02,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpunord_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x03,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpunord_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x03,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x04,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x04,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpnlt_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x05,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpnlt_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x05,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpnle_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x06,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpnle_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x06,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpord_ps_mask (__m512 __X, __m512 __Y)
{
  return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x07,
         (__mmask16) -1,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpord_ps_mask (__mmask16 __U, __m512 __X, __m512 __Y)
{
   return (__mmask16) __builtin_ia32_cmpps512_mask ((__v16sf) __X,
         (__v16sf) __Y, 0x07,
         (__mmask16) __U,
         0x04);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kmov (__mmask16 __A)
{
  return __builtin_ia32_kmovw (__A);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd_ps (__m512d __A)
{
  return (__m512) (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd_si512 (__m512d __A)
{
  return (__m512i) (__A);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps_pd (__m512 __A)
{
  return (__m512d) (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps_si512 (__m512 __A)
{
  return (__m512i) (__A);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi512_ps (__m512i __A)
{
  return (__m512) (__A);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi512_pd (__m512i __A)
{
  return (__m512d) (__A);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd512_pd128 (__m512d __A)
{
  return (__m128d)_mm512_extractf32x4_ps((__m512)__A, 0);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps512_ps128 (__m512 __A)
{
  return _mm512_extractf32x4_ps(__A, 0);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi512_si128 (__m512i __A)
{
  return (__m128i)_mm512_extracti32x4_epi32((__m512i)__A, 0);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd512_pd256 (__m512d __A)
{
  return _mm512_extractf64x4_pd(__A, 0);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps512_ps256 (__m512 __A)
{
  return (__m256)_mm512_extractf64x4_pd((__m512d)__A, 0);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi512_si256 (__m512i __A)
{
  return (__m256i)_mm512_extractf64x4_pd((__m512d)__A, 0);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd128_pd512 (__m128d __A)
{
  return (__m512d) __builtin_ia32_pd512_pd((__m128d)__A);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps128_ps512 (__m128 __A)
{
  return (__m512) __builtin_ia32_ps512_ps((__m128)__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi128_si512 (__m128i __A)
{
  return (__m512i) __builtin_ia32_si512_si((__v4si)__A);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castpd256_pd512 (__m256d __A)
{
  return __builtin_ia32_pd512_256pd (__A);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castps256_ps512 (__m256 __A)
{
  return __builtin_ia32_ps512_256ps (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_castsi256_si512 (__m256i __A)
{
  return (__m512i)__builtin_ia32_si512_256si ((__v8si)__A);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextpd128_pd512 (__m128d __A)
{
  return (__m512d) _mm512_insertf32x4 (_mm512_setzero_ps (), (__m128) __A, 0);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextps128_ps512 (__m128 __A)
{
  return _mm512_insertf32x4 (_mm512_setzero_ps (), __A, 0);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextsi128_si512 (__m128i __A)
{
  return _mm512_inserti32x4 (_mm512_setzero_si512 (), __A, 0);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextpd256_pd512 (__m256d __A)
{
  return _mm512_insertf64x4 (_mm512_setzero_pd (), __A, 0);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextps256_ps512 (__m256 __A)
{
  return (__m512) _mm512_insertf64x4 (_mm512_setzero_pd (), (__m256d) __A, 0);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_zextsi256_si512 (__m256i __A)
{
  return _mm512_inserti64x4 (_mm512_setzero_si512 (), __A, 0);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epu32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __A,
           (__v16si) __B, 0,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epu32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __A,
           (__v16si) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epu64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __A,
          (__v8di) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epu64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __A,
          (__v8di) __B, 0,
          (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epu32_mask (__m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __A,
           (__v16si) __B, 6,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epu32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__mmask16) __builtin_ia32_ucmpd512_mask ((__v16si) __A,
           (__v16si) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epu64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __A,
          (__v8di) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epu64_mask (__m512i __A, __m512i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq512_mask ((__v8di) __A,
          (__v8di) __B, 6,
          (__mmask8) -1);
}
# 15784 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_add_epi32 (__m512i __A)
{
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 + __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 + __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 + __T7; return __T8[0] + __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_mul_epi32 (__m512i __A)
{
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 * __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 * __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 * __T7; return __T8[0] * __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_and_epi32 (__m512i __A)
{
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 & __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 & __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 & __T7; return __T8[0] & __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_or_epi32 (__m512i __A)
{
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 | __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 | __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 | __T7; return __T8[0] | __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_add_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi32 (__U, __A);
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 + __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 + __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 + __T7; return __T8[0] + __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_mul_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi32 (_mm512_set1_epi32 (1), __U, __A);
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 * __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 * __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 * __T7; return __T8[0] * __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_and_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi32 (_mm512_set1_epi32 (~0), __U, __A);
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 & __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 & __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 & __T7; return __T8[0] & __T8[1];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_or_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi32 (__U, __A);
  __v8si __T1 = (__v8si) _mm512_extracti64x4_epi64 (__A, 1); __v8si __T2 = (__v8si) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 | __T2); __v4si __T4 = (__v4si) _mm256_extracti128_si256 (__T3, 1); __v4si __T5 = (__v4si) _mm256_extracti128_si256 (__T3, 0); __v4si __T6 = __T4 | __T5; __v4si __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __v4si __T8 = __T6 | __T7; return __T8[0] | __T8[1];
}
# 15860 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_epi32 (__m512i __A)
{
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_min_epi32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_min_epi32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_min_epi32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_min_epi32 (__T8, __T9); return __T10[0];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_epi32 (__m512i __A)
{
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_max_epi32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_max_epi32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_max_epi32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_max_epi32 (__T8, __T9); return __T10[0];
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_epu32 (__m512i __A)
{
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_min_epu32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_min_epu32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_min_epu32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_min_epu32 (__T8, __T9); return __T10[0];
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_epu32 (__m512i __A)
{
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_max_epu32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_max_epu32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_max_epu32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_max_epu32 (__T8, __T9); return __T10[0];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi32 (_mm512_set1_epi32 (0x7fffffff), __U, __A);
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_min_epi32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_min_epi32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_min_epi32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_min_epi32 (__T8, __T9); return __T10[0];
}

extern __inline int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_epi32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi32 (_mm512_set1_epi32 (-0x7fffffff - 1), __U, __A);
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_max_epi32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_max_epi32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_max_epi32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_max_epi32 (__T8, __T9); return __T10[0];
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_epu32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi32 (_mm512_set1_epi32 (~0), __U, __A);
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_min_epu32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_min_epu32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_min_epu32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_min_epu32 (__T8, __T9); return __T10[0];
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_epu32 (__mmask16 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi32 (__U, __A);
  __m256i __T1 = (__m256i) _mm512_extracti64x4_epi64 (__A, 1); __m256i __T2 = (__m256i) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = _mm256_max_epu32 (__T1, __T2); __m128i __T4 = (__m128i) _mm256_extracti128_si256 (__T3, 1); __m128i __T5 = (__m128i) _mm256_extracti128_si256 (__T3, 0); __m128i __T6 = _mm_max_epu32 (__T4, __T5); __m128i __T7 = (__m128i) __builtin_shuffle ((__v4si) __T6, (__v4si) { 2, 3, 0, 1 }); __m128i __T8 = _mm_max_epu32 (__T6, __T7); __m128i __T9 = (__m128i) __builtin_shuffle ((__v4si) __T8, (__v4si) { 1, 0, 1, 0 }); __v4si __T10 = (__v4si) _mm_max_epu32 (__T8, __T9); return __T10[0];
}
# 15932 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_add_ps (__m512 __A)
{
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = __T1 + __T2; __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = __T4 + __T5; __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = __T6 + __T7; return __T8[0] + __T8[1];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_mul_ps (__m512 __A)
{
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = __T1 * __T2; __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = __T4 * __T5; __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = __T6 * __T7; return __T8[0] * __T8[1];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_add_ps (__mmask16 __U, __m512 __A)
{
  __A = _mm512_maskz_mov_ps (__U, __A);
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = __T1 + __T2; __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = __T4 + __T5; __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = __T6 + __T7; return __T8[0] + __T8[1];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_mul_ps (__mmask16 __U, __m512 __A)
{
  __A = _mm512_mask_mov_ps (_mm512_set1_ps (1.0f), __U, __A);
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = __T1 * __T2; __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = __T4 * __T5; __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = __T6 * __T7; return __T8[0] * __T8[1];
}
# 15976 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_ps (__m512 __A)
{
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = _mm256_min_ps (__T1, __T2); __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = _mm_min_ps (__T4, __T5); __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = _mm_min_ps (__T6, __T7); __m128 __T9 = __builtin_shuffle (__T8, (__v4si) { 1, 0, 1, 0 }); __m128 __T10 = _mm_min_ps (__T8, __T9); return __T10[0];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_ps (__m512 __A)
{
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = _mm256_max_ps (__T1, __T2); __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = _mm_max_ps (__T4, __T5); __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = _mm_max_ps (__T6, __T7); __m128 __T9 = __builtin_shuffle (__T8, (__v4si) { 1, 0, 1, 0 }); __m128 __T10 = _mm_max_ps (__T8, __T9); return __T10[0];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_ps (__mmask16 __U, __m512 __A)
{
  __A = _mm512_mask_mov_ps (_mm512_set1_ps (__builtin_inff ()), __U, __A);
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = _mm256_min_ps (__T1, __T2); __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = _mm_min_ps (__T4, __T5); __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = _mm_min_ps (__T6, __T7); __m128 __T9 = __builtin_shuffle (__T8, (__v4si) { 1, 0, 1, 0 }); __m128 __T10 = _mm_min_ps (__T8, __T9); return __T10[0];
}

extern __inline float
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_ps (__mmask16 __U, __m512 __A)
{
  __A = _mm512_mask_mov_ps (_mm512_set1_ps (-__builtin_inff ()), __U, __A);
  __m256 __T1 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 1); __m256 __T2 = (__m256) _mm512_extractf64x4_pd ((__m512d) __A, 0); __m256 __T3 = _mm256_max_ps (__T1, __T2); __m128 __T4 = _mm256_extractf128_ps (__T3, 1); __m128 __T5 = _mm256_extractf128_ps (__T3, 0); __m128 __T6 = _mm_max_ps (__T4, __T5); __m128 __T7 = __builtin_shuffle (__T6, (__v4si) { 2, 3, 0, 1 }); __m128 __T8 = _mm_max_ps (__T6, __T7); __m128 __T9 = __builtin_shuffle (__T8, (__v4si) { 1, 0, 1, 0 }); __m128 __T10 = _mm_max_ps (__T8, __T9); return __T10[0];
}
# 16016 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_add_epi64 (__m512i __A)
{
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 + __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 + __T5; return __T6[0] + __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_mul_epi64 (__m512i __A)
{
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 * __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 * __T5; return __T6[0] * __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_and_epi64 (__m512i __A)
{
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 & __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 & __T5; return __T6[0] & __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_or_epi64 (__m512i __A)
{
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 | __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 | __T5; return __T6[0] | __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_add_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi64 (__U, __A);
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 + __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 + __T5; return __T6[0] + __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_mul_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi64 (_mm512_set1_epi64 (1LL), __U, __A);
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 * __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 * __T5; return __T6[0] * __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_and_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi64 (_mm512_set1_epi64 (~0LL), __U, __A);
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 & __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 & __T5; return __T6[0] & __T6[1];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_or_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi64 (__U, __A);
  __v4di __T1 = (__v4di) _mm512_extracti64x4_epi64 (__A, 1); __v4di __T2 = (__v4di) _mm512_extracti64x4_epi64 (__A, 0); __m256i __T3 = (__m256i) (__T1 | __T2); __v2di __T4 = (__v2di) _mm256_extracti128_si256 (__T3, 1); __v2di __T5 = (__v2di) _mm256_extracti128_si256 (__T3, 0); __v2di __T6 = __T4 | __T5; return __T6[0] | __T6[1];
}
# 16090 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_epi64 (__m512i __A)
{
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_min_epi64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_min_epi64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_min_epi64 (__T4, __T5); return __T6[0];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_epi64 (__m512i __A)
{
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_max_epi64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_max_epi64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_max_epi64 (__T4, __T5); return __T6[0];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi64 (_mm512_set1_epi64 (0x7fffffffffffffffLL),
          __U, __A);
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_min_epi64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_min_epi64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_min_epi64 (__T4, __T5); return __T6[0];
}

extern __inline long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_epi64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi64 (_mm512_set1_epi64 (-0x7fffffffffffffffLL - 1),
          __U, __A);
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_max_epi64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_max_epi64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_max_epi64 (__T4, __T5); return __T6[0];
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_epu64 (__m512i __A)
{
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_min_epu64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_min_epu64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_min_epu64 (__T4, __T5); return __T6[0];
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_epu64 (__m512i __A)
{
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_max_epu64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_max_epu64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_max_epu64 (__T4, __T5); return __T6[0];
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_epu64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_mask_mov_epi64 (_mm512_set1_epi64 (~0LL), __U, __A);
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_min_epu64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_min_epu64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_min_epu64 (__T4, __T5); return __T6[0];
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_epu64 (__mmask8 __U, __m512i __A)
{
  __A = _mm512_maskz_mov_epi64 (__U, __A);
  __m512i __T1 = _mm512_shuffle_i64x2 (__A, __A, 0x4e); __m512i __T2 = _mm512_max_epu64 (__A, __T1); __m512i __T3 = (__m512i) __builtin_shuffle ((__v8di) __T2, (__v8di) { 2, 3, 0, 1, 6, 7, 4, 5 }); __m512i __T4 = _mm512_max_epu64 (__T2, __T3); __m512i __T5 = (__m512i) __builtin_shuffle ((__v8di) __T4, (__v8di) { 1, 0, 3, 2, 5, 4, 7, 6 }); __v8di __T6 = (__v8di) _mm512_max_epu64 (__T4, __T5); return __T6[0];
}
# 16162 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_add_pd (__m512d __A)
{
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = __T1 + __T2; __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = __T4 + __T5; return __T6[0] + __T6[1];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_mul_pd (__m512d __A)
{
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = __T1 * __T2; __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = __T4 * __T5; return __T6[0] * __T6[1];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_add_pd (__mmask8 __U, __m512d __A)
{
  __A = _mm512_maskz_mov_pd (__U, __A);
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = __T1 + __T2; __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = __T4 + __T5; return __T6[0] + __T6[1];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_mul_pd (__mmask8 __U, __m512d __A)
{
  __A = _mm512_mask_mov_pd (_mm512_set1_pd (1.0), __U, __A);
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = __T1 * __T2; __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = __T4 * __T5; return __T6[0] * __T6[1];
}
# 16204 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512fintrin.h" 3 4
extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_min_pd (__m512d __A)
{
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = _mm256_min_pd (__T1, __T2); __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = _mm_min_pd (__T4, __T5); __m128d __T7 = (__m128d) __builtin_shuffle (__T6, (__v2di) { 1, 0 }); __m128d __T8 = _mm_min_pd (__T6, __T7); return __T8[0];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_max_pd (__m512d __A)
{
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = _mm256_max_pd (__T1, __T2); __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = _mm_max_pd (__T4, __T5); __m128d __T7 = (__m128d) __builtin_shuffle (__T6, (__v2di) { 1, 0 }); __m128d __T8 = _mm_max_pd (__T6, __T7); return __T8[0];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_min_pd (__mmask8 __U, __m512d __A)
{
  __A = _mm512_mask_mov_pd (_mm512_set1_pd (__builtin_inf ()), __U, __A);
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = _mm256_min_pd (__T1, __T2); __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = _mm_min_pd (__T4, __T5); __m128d __T7 = (__m128d) __builtin_shuffle (__T6, (__v2di) { 1, 0 }); __m128d __T8 = _mm_min_pd (__T6, __T7); return __T8[0];
}

extern __inline double
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_max_pd (__mmask8 __U, __m512d __A)
{
  __A = _mm512_mask_mov_pd (_mm512_set1_pd (-__builtin_inf ()), __U, __A);
  __m256d __T1 = (__m256d) _mm512_extractf64x4_pd (__A, 1); __m256d __T2 = (__m256d) _mm512_extractf64x4_pd (__A, 0); __m256d __T3 = _mm256_max_pd (__T1, __T2); __m128d __T4 = _mm256_extractf128_pd (__T3, 1); __m128d __T5 = _mm256_extractf128_pd (__T3, 0); __m128d __T6 = _mm_max_pd (__T4, __T5); __m128d __T7 = (__m128d) __builtin_shuffle (__T6, (__v2di) { 1, 0 }); __m128d __T8 = _mm_max_pd (__T6, __T7); return __T8[0];
}





#pragma GCC pop_options
# 56 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512erintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512erintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512er")




typedef double __v8df __attribute__ ((__vector_size__ (64)));
typedef float __v16sf __attribute__ ((__vector_size__ (64)));



typedef float __m512 __attribute__ ((__vector_size__ (64), __may_alias__));
typedef double __m512d __attribute__ ((__vector_size__ (64), __may_alias__));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;


extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_exp2a23_round_pd (__m512d __A, int __R)
{
  __m512d __W;
  return (__m512d) __builtin_ia32_exp2pd_mask ((__v8df) __A,
            (__v8df) __W,
            (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_exp2a23_round_pd (__m512d __W, __mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_exp2pd_mask ((__v8df) __A,
            (__v8df) __W,
            (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_exp2a23_round_pd (__mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_exp2pd_mask ((__v8df) __A,
            (__v8df) _mm512_setzero_pd (),
            (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_exp2a23_round_ps (__m512 __A, int __R)
{
  __m512 __W;
  return (__m512) __builtin_ia32_exp2ps_mask ((__v16sf) __A,
           (__v16sf) __W,
           (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_exp2a23_round_ps (__m512 __W, __mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_exp2ps_mask ((__v16sf) __A,
           (__v16sf) __W,
           (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_exp2a23_round_ps (__mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_exp2ps_mask ((__v16sf) __A,
           (__v16sf) _mm512_setzero_ps (),
           (__mmask16) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rcp28_round_pd (__m512d __A, int __R)
{
  __m512d __W;
  return (__m512d) __builtin_ia32_rcp28pd_mask ((__v8df) __A,
      (__v8df) __W,
      (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rcp28_round_pd (__m512d __W, __mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_rcp28pd_mask ((__v8df) __A,
      (__v8df) __W,
      (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rcp28_round_pd (__mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_rcp28pd_mask ((__v8df) __A,
      (__v8df) _mm512_setzero_pd (),
      (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rcp28_round_ps (__m512 __A, int __R)
{
  __m512 __W;
  return (__m512) __builtin_ia32_rcp28ps_mask ((__v16sf) __A,
            (__v16sf) __W,
            (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rcp28_round_ps (__m512 __W, __mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_rcp28ps_mask ((__v16sf) __A,
            (__v16sf) __W,
            (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rcp28_round_ps (__mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_rcp28ps_mask ((__v16sf) __A,
            (__v16sf) _mm512_setzero_ps (),
            (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp28_round_sd (__m128d __A, __m128d __B, int __R)
{
  return (__m128d) __builtin_ia32_rcp28sd_round ((__v2df) __B,
       (__v2df) __A,
       __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp28_round_ss (__m128 __A, __m128 __B, int __R)
{
  return (__m128) __builtin_ia32_rcp28ss_round ((__v4sf) __B,
      (__v4sf) __A,
      __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rsqrt28_round_pd (__m512d __A, int __R)
{
  __m512d __W;
  return (__m512d) __builtin_ia32_rsqrt28pd_mask ((__v8df) __A,
        (__v8df) __W,
        (__mmask8) -1, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rsqrt28_round_pd (__m512d __W, __mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_rsqrt28pd_mask ((__v8df) __A,
        (__v8df) __W,
        (__mmask8) __U, __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rsqrt28_round_pd (__mmask8 __U, __m512d __A, int __R)
{
  return (__m512d) __builtin_ia32_rsqrt28pd_mask ((__v8df) __A,
        (__v8df) _mm512_setzero_pd (),
        (__mmask8) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_rsqrt28_round_ps (__m512 __A, int __R)
{
  __m512 __W;
  return (__m512) __builtin_ia32_rsqrt28ps_mask ((__v16sf) __A,
       (__v16sf) __W,
       (__mmask16) -1, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_rsqrt28_round_ps (__m512 __W, __mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_rsqrt28ps_mask ((__v16sf) __A,
       (__v16sf) __W,
       (__mmask16) __U, __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_rsqrt28_round_ps (__mmask16 __U, __m512 __A, int __R)
{
  return (__m512) __builtin_ia32_rsqrt28ps_mask ((__v16sf) __A,
       (__v16sf) _mm512_setzero_ps (),
       (__mmask16) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt28_round_sd (__m128d __A, __m128d __B, int __R)
{
  return (__m128d) __builtin_ia32_rsqrt28sd_round ((__v2df) __B,
         (__v2df) __A,
         __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt28_round_ss (__m128 __A, __m128 __B, int __R)
{
  return (__m128) __builtin_ia32_rsqrt28ss_round ((__v4sf) __B,
        (__v4sf) __A,
        __R);
}
# 391 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512erintrin.h" 3 4
#pragma GCC pop_options
# 58 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512pfintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512pfintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512pf")




typedef long long __v8di __attribute__ ((__vector_size__ (64)));
typedef int __v16si __attribute__ ((__vector_size__ (64)));



typedef long long __m512i __attribute__ ((__vector_size__ (64), __may_alias__));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;


extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i32gather_pd (__m256i __index, void const *__addr,
         int __scale, int __hint)
{
  __builtin_ia32_gatherpfdpd ((__mmask8) 0xFF, (__v8si) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i32gather_ps (__m512i __index, void const *__addr,
         int __scale, int __hint)
{
  __builtin_ia32_gatherpfdps ((__mmask16) 0xFFFF, (__v16si) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i32gather_pd (__m256i __index, __mmask8 __mask,
       void const *__addr, int __scale, int __hint)
{
  __builtin_ia32_gatherpfdpd (__mask, (__v8si) __index, __addr, __scale,
         __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i32gather_ps (__m512i __index, __mmask16 __mask,
       void const *__addr, int __scale, int __hint)
{
  __builtin_ia32_gatherpfdps (__mask, (__v16si) __index, __addr, __scale,
         __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i64gather_pd (__m512i __index, void const *__addr,
         int __scale, int __hint)
{
  __builtin_ia32_gatherpfqpd ((__mmask8) 0xFF, (__v8di) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i64gather_ps (__m512i __index, void const *__addr,
         int __scale, int __hint)
{
  __builtin_ia32_gatherpfqps ((__mmask8) 0xFF, (__v8di) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i64gather_pd (__m512i __index, __mmask8 __mask,
       void const *__addr, int __scale, int __hint)
{
  __builtin_ia32_gatherpfqpd (__mask, (__v8di) __index, __addr, __scale,
         __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i64gather_ps (__m512i __index, __mmask8 __mask,
       void const *__addr, int __scale, int __hint)
{
  __builtin_ia32_gatherpfqps (__mask, (__v8di) __index, __addr, __scale,
         __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i32scatter_pd (void *__addr, __m256i __index, int __scale,
          int __hint)
{
  __builtin_ia32_scatterpfdpd ((__mmask8) 0xFF, (__v8si) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i32scatter_ps (void *__addr, __m512i __index, int __scale,
          int __hint)
{
  __builtin_ia32_scatterpfdps ((__mmask16) 0xFFFF, (__v16si) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i32scatter_pd (void *__addr, __mmask8 __mask,
        __m256i __index, int __scale, int __hint)
{
  __builtin_ia32_scatterpfdpd (__mask, (__v8si) __index, __addr, __scale,
          __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i32scatter_ps (void *__addr, __mmask16 __mask,
        __m512i __index, int __scale, int __hint)
{
  __builtin_ia32_scatterpfdps (__mask, (__v16si) __index, __addr, __scale,
          __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i64scatter_pd (void *__addr, __m512i __index, int __scale,
          int __hint)
{
  __builtin_ia32_scatterpfqpd ((__mmask8) 0xFF, (__v8di) __index,__addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_prefetch_i64scatter_ps (void *__addr, __m512i __index, int __scale,
          int __hint)
{
  __builtin_ia32_scatterpfqps ((__mmask8) 0xFF, (__v8di) __index, __addr,
         __scale, __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i64scatter_pd (void *__addr, __mmask8 __mask,
        __m512i __index, int __scale, int __hint)
{
  __builtin_ia32_scatterpfqpd (__mask, (__v8di) __index, __addr, __scale,
          __hint);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_prefetch_i64scatter_ps (void *__addr, __mmask8 __mask,
        __m512i __index, int __scale, int __hint)
{
  __builtin_ia32_scatterpfqps (__mask, (__v8di) __index, __addr, __scale,
          __hint);
}
# 266 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512pfintrin.h" 3 4
#pragma GCC pop_options
# 60 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512cdintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512cdintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512cd")




typedef long long __v8di __attribute__ ((__vector_size__ (64)));
typedef int __v16si __attribute__ ((__vector_size__ (64)));



typedef long long __m512i __attribute__ ((__vector_size__ (64), __may_alias__));
typedef double __m512d __attribute__ ((__vector_size__ (64), __may_alias__));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_conflict_epi32 (__m512i __A)
{
  return (__m512i)
  __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
            (__v16si) _mm512_setzero_si512 (),
            (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_conflict_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_conflict_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)
  __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
            (__v16si) _mm512_setzero_si512 (),
            (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_conflict_epi64 (__m512i __A)
{
  return (__m512i)
  __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
            (__v8di) _mm512_setzero_si512 (),
            (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_conflict_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_conflict_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)
  __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
            (__v8di) _mm512_setzero_si512 (),
            (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_lzcnt_epi64 (__m512i __A)
{
  return (__m512i)
  __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
        (__v8di) _mm512_setzero_si512 (),
        (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_lzcnt_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
           (__v8di) __W,
           (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_lzcnt_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)
  __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
        (__v8di) _mm512_setzero_si512 (),
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_lzcnt_epi32 (__m512i __A)
{
  return (__m512i)
  __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
        (__v16si) _mm512_setzero_si512 (),
        (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_lzcnt_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
           (__v16si) __W,
           (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_lzcnt_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)
  __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
        (__v16si) _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m512i) __builtin_ia32_broadcastmb512 (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m512i) __builtin_ia32_broadcastmw512 (__A);
}



#pragma GCC pop_options
# 62 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vl")




typedef unsigned int __mmask32;

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_movapd256_mask ((__v4df) __A,
        (__v4df) __W,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_movapd256_mask ((__v4df) __A,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_movapd128_mask ((__v2df) __A,
        (__v2df) __W,
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_movapd128_mask ((__v2df) __A,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_load_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((__v4df *) __P,
         (__v4df) __W,
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((__v4df *) __P,
         (__v4df)
         _mm256_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((__v2df *) __P,
         (__v2df) __W,
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((__v2df *) __P,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_store_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeapd256_mask ((__v4df *) __P,
       (__v4df) __A,
       (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeapd128_mask ((__v2df *) __P,
       (__v2df) __A,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movaps256_mask ((__v8sf) __A,
       (__v8sf) __W,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movaps256_mask ((__v8sf) __A,
       (__v8sf)
       _mm256_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movaps128_mask ((__v4sf) __A,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movaps128_mask ((__v4sf) __A,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_load_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((__v8sf *) __P,
        (__v8sf) __W,
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((__v8sf *) __P,
        (__v8sf)
        _mm256_setzero_ps (),
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((__v4sf *) __P,
        (__v4sf) __W,
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((__v4sf *) __P,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_store_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeaps256_mask ((__v8sf *) __P,
       (__v8sf) __A,
       (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeaps128_mask ((__v4sf *) __P,
       (__v4sf) __A,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdqa64_256_mask ((__v4di) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdqa64_256_mask ((__v4di) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdqa64_128_mask ((__v2di) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdqa64_128_mask ((__v2di) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_load_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((__v4di *) __P,
       (__v4di) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((__v4di *) __P,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((__v2di *) __P,
       (__v2di) __W,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((__v2di *) __P,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8)
       __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_store_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa64store256_mask ((__v4di *) __P,
     (__v4di) __A,
     (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa64store128_mask ((__v2di *) __P,
     (__v2di) __A,
     (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdqa32_256_mask ((__v8si) __A,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdqa32_256_mask ((__v8si) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdqa32_128_mask ((__v4si) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdqa32_128_mask ((__v4si) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_load_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((__v8si *) __P,
       (__v8si) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((__v8si *) __P,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_load_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((__v4si *) __P,
       (__v4si) __W,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((__v4si *) __P,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8)
       __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_store_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa32store256_mask ((__v8si *) __P,
     (__v8si) __A,
     (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_store_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa32store128_mask ((__v4si *) __P,
     (__v4si) __A,
     (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_addpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_addpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_addpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_addpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_addps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_addps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_addps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_addps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_subpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_subpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_subpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_subpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_subps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_subps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_subps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_subps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_store_epi64 (void *__P, __m256i __A)
{
  *(__m256i *) __P = __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_store_epi64 (void *__P, __m128i __A)
{
  *(__m128i *) __P = __A;
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const double *) __P,
         (__v4df) __W,
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const double *) __P,
         (__v4df)
         _mm256_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const double *) __P,
         (__v2df) __W,
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const double *) __P,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeupd256_mask ((double *) __P,
       (__v4df) __A,
       (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeupd128_mask ((double *) __P,
       (__v2df) __A,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const float *) __P,
        (__v8sf) __W,
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const float *) __P,
        (__v8sf)
        _mm256_setzero_ps (),
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const float *) __P,
        (__v4sf) __W,
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const float *) __P,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeups256_mask ((float *) __P,
       (__v8sf) __A,
       (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeups128_mask ((float *) __P,
       (__v4sf) __A,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const long long *) __P,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const long long *) __P,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const long long *) __P,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const long long *) __P,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu_epi64 (void *__P, __m256i __A)
{
  *(__m256i_u *) __P = (__m256i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqudi256_mask ((long long *) __P,
         (__v4di) __A,
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_epi64 (void *__P, __m128i __A)
{
  *(__m128i_u *) __P = (__m128i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqudi128_mask ((long long *) __P,
         (__v2di) __A,
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const int *) __P,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const int *) __P,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const int *) __P,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const int *) __P,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_storeu_epi32 (void *__P, __m256i __A)
{
  *(__m256i_u *) __P = (__m256i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqusi256_mask ((int *) __P,
         (__v8si) __A,
         (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_storeu_epi32 (void *__P, __m128i __A)
{
  *(__m128i_u *) __P = (__m128i_u) __A;
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqusi128_mask ((int *) __P,
         (__v4si) __A,
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_abs_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsd256_mask ((__v8si) __A,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_abs_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsd256_mask ((__v8si) __A,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_abs_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsd128_mask ((__v4si) __A,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_abs_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsd128_mask ((__v4si) __A,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_abs_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_abs_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_abs_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_abs_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_abs_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_abs_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtpd_epu32 (__m256d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtpd_epu32 (__mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_epu32 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtpd_epu32 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttps_epi32 (__m256i __W, __mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvttps2dq256_mask ((__v8sf) __A,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttps_epi32 (__mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvttps2dq256_mask ((__v8sf) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttps_epi32 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2dq128_mask ((__v4sf) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttps_epi32 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2dq128_mask ((__v4sf) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttps_epu32 (__m256 __A)
{
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
            (__v8si)
            _mm256_setzero_si256 (),
            (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttps_epu32 (__m256i __W, __mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
            (__v8si) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttps_epu32 (__mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
            (__v8si)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttps_epu32 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttps_epu32 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
            (__v4si) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttps_epu32 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2dq256_mask ((__v4df) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttpd_epi32 (__mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2dq256_mask ((__v4df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttpd_epi32 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttpd_epu32 (__m256d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
            (__v4si) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttpd_epu32 (__mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttpd_epu32 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
            (__v4si) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttpd_epu32 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2dq256_mask ((__v4df) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtpd_epi32 (__mmask8 __U, __m256d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2dq256_mask ((__v4df) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtpd_epi32 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_pd (__m256d __W, __mmask8 __U, __m128i __A)
{
  return (__m256d) __builtin_ia32_cvtdq2pd256_mask ((__v4si) __A,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A)
{
  return (__m256d) __builtin_ia32_cvtdq2pd256_mask ((__v4si) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_pd (__m128d __W, __mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtdq2pd128_mask ((__v4si) __A,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtdq2pd128_mask ((__v4si) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu32_pd (__m128i __A)
{
  return (__m256d) __builtin_ia32_cvtudq2pd256_mask ((__v4si) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu32_pd (__m256d __W, __mmask8 __U, __m128i __A)
{
  return (__m256d) __builtin_ia32_cvtudq2pd256_mask ((__v4si) __A,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A)
{
  return (__m256d) __builtin_ia32_cvtudq2pd256_mask ((__v4si) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu32_pd (__m128i __A)
{
  return (__m128d) __builtin_ia32_cvtudq2pd128_mask ((__v4si) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu32_pd (__m128d __W, __mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtudq2pd128_mask ((__v4si) __A,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtudq2pd128_mask ((__v4si) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_ps (__m256 __W, __mmask8 __U, __m256i __A)
{
  return (__m256) __builtin_ia32_cvtdq2ps256_mask ((__v8si) __A,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi32_ps (__mmask8 __U, __m256i __A)
{
  return (__m256) __builtin_ia32_cvtdq2ps256_mask ((__v8si) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtdq2ps128_mask ((__v4si) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi32_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtdq2ps128_mask ((__v4si) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu32_ps (__m256i __A)
{
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu32_ps (__m256 __W, __mmask8 __U, __m256i __A)
{
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu32_ps (__mmask8 __U, __m256i __A)
{
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu32_ps (__m128i __A)
{
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu32_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu32_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_pd (__m256d __W, __mmask8 __U, __m128 __A)
{
  return (__m256d) __builtin_ia32_cvtps2pd256_mask ((__v4sf) __A,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_pd (__mmask8 __U, __m128 __A)
{
  return (__m256d) __builtin_ia32_cvtps2pd256_mask ((__v4sf) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_pd (__m128d __W, __mmask8 __U, __m128 __A)
{
  return (__m128d) __builtin_ia32_cvtps2pd128_mask ((__v4sf) __A,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_pd (__mmask8 __U, __m128 __A)
{
  return (__m128d) __builtin_ia32_cvtps2pd128_mask ((__v4sf) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
        (__v16qi) __O, __M);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdb256mem_mask ((__v16qi*) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
        (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
        (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
         (__v8hi)__O,
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
         (__v8hi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
         (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
          (__v8hi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
          (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
          (__v8hi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
          (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
        (__v16qi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
          (__v16qi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
        (__v8hi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
        (__v8hi)__O,
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
        (__v8hi)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
        (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
        (__v8hi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
         (__v8hi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
         (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
         (__v8hi)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
         (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
         (__v8hi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
          (__v8hi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
          (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
          (__v8hi)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
          (__v8hi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
        (__v4si)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
        (__v4si) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
        (__v4si)
        _mm_undefined_si128 (),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
        (__v4si) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
         (__v4si)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
         (__v4si) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
         (__v4si)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
         (__v4si)
         _mm_undefined_si128 (),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
         (__v4si)__O,
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
         (__v4si)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
          (__v4si)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
          (__v4si) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
          (__v4si)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
          (__v4si)
          _mm_undefined_si128 (),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
          (__v4si) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
          (__v4si)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastss_ps (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastss256_mask ((__v4sf) __A,
            (__v8sf) __O,
            __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastss256_mask ((__v4sf) __A,
            (__v8sf)
            _mm256_setzero_ps (),
            __M);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcastss_ps (__m128 __O, __mmask8 __M, __m128 __A)
{
  return (__m128) __builtin_ia32_broadcastss128_mask ((__v4sf) __A,
            (__v4sf) __O,
            __M);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m128) __builtin_ia32_broadcastss128_mask ((__v4sf) __A,
            (__v4sf)
            _mm_setzero_ps (),
            __M);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastsd_pd (__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d) __builtin_ia32_broadcastsd256_mask ((__v2df) __A,
             (__v4df) __O,
             __M);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m256d) __builtin_ia32_broadcastsd256_mask ((__v2df) __A,
             (__v4df)
             _mm256_setzero_pd (),
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastd_epi32 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastd256_mask ((__v4si) __A,
             (__v8si) __O,
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastd256_mask ((__v4si) __A,
             (__v8si)
             _mm256_setzero_si256 (),
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_set1_epi32 (__m256i __O, __mmask8 __M, int __A)
{
  return (__m256i) __builtin_ia32_pbroadcastd256_gpr_mask (__A, (__v8si) __O,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_set1_epi32 (__mmask8 __M, int __A)
{
  return (__m256i) __builtin_ia32_pbroadcastd256_gpr_mask (__A,
          (__v8si)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcastd_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastd128_mask ((__v4si) __A,
             (__v4si) __O,
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastd128_mask ((__v4si) __A,
             (__v4si)
             _mm_setzero_si128 (),
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_set1_epi32 (__m128i __O, __mmask8 __M, int __A)
{
  return (__m128i) __builtin_ia32_pbroadcastd128_gpr_mask (__A, (__v4si) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_set1_epi32 (__mmask8 __M, int __A)
{
  return (__m128i)
  __builtin_ia32_pbroadcastd128_gpr_mask (__A,
       (__v4si) _mm_setzero_si128 (),
       __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastq_epi64 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastq256_mask ((__v2di) __A,
             (__v4di) __O,
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastq256_mask ((__v2di) __A,
             (__v4di)
             _mm256_setzero_si256 (),
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_set1_epi64 (__m256i __O, __mmask8 __M, long long __A)
{
  return (__m256i) __builtin_ia32_pbroadcastq256_gpr_mask (__A, (__v4di) __O,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m256i) __builtin_ia32_pbroadcastq256_gpr_mask (__A,
          (__v4di)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcastq_epi64 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastq128_mask ((__v2di) __A,
             (__v2di) __O,
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastq128_mask ((__v2di) __A,
             (__v2di)
             _mm_setzero_si128 (),
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_set1_epi64 (__m128i __O, __mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_pbroadcastq128_gpr_mask (__A, (__v2di) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m128i)
  __builtin_ia32_pbroadcastq128_gpr_mask (__A,
       (__v2di) _mm_setzero_si128 (),
       __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_f32x4 (__m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x4_256_mask ((__v4sf) __A,
                (__v8sf)_mm256_undefined_pd (),
         (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_f32x4 (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x4_256_mask ((__v4sf) __A,
         (__v8sf) __O,
         __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_f32x4 (__mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x4_256_mask ((__v4sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_i32x4 (__m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x4_256_mask ((__v4si)
          __A,
                 (__v8si)_mm256_undefined_si256 (),
          (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_i32x4 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x4_256_mask ((__v4si)
          __A,
          (__v8si)
          __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_i32x4 (__mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x4_256_mask ((__v4si)
          __A,
          (__v8si)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi8_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbd256_mask ((__v16qi) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi8_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbd256_mask ((__v16qi) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi8_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbd128_mask ((__v16qi) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi8_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbd128_mask ((__v16qi) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi8_epi64 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbq256_mask ((__v16qi) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbq256_mask ((__v16qi) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi8_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbq128_mask ((__v16qi) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbq128_mask ((__v16qi) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi16_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxwd256_mask ((__v8hi) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi16_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxwd256_mask ((__v8hi) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi16_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxwd128_mask ((__v8hi) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi16_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxwd128_mask ((__v8hi) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi16_epi64 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxwq256_mask ((__v8hi) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxwq256_mask ((__v8hi) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi16_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxwq128_mask ((__v8hi) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxwq128_mask ((__v8hi) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi32_epi64 (__m256i __W, __mmask8 __U, __m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxdq256_mask ((__v4si) __X,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi32_epi64 (__mmask8 __U, __m128i __X)
{
  return (__m256i) __builtin_ia32_pmovsxdq256_mask ((__v4si) __X,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi32_epi64 (__m128i __W, __mmask8 __U, __m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxdq128_mask ((__v4si) __X,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi32_epi64 (__mmask8 __U, __m128i __X)
{
  return (__m128i) __builtin_ia32_pmovsxdq128_mask ((__v4si) __X,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu8_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbd256_mask ((__v16qi) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu8_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbd256_mask ((__v16qi) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu8_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbd128_mask ((__v16qi) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu8_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbd128_mask ((__v16qi) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu8_epi64 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbq256_mask ((__v16qi) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbq256_mask ((__v16qi) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu8_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbq128_mask ((__v16qi) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbq128_mask ((__v16qi) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu16_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxwd256_mask ((__v8hi) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu16_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxwd256_mask ((__v8hi) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu16_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxwd128_mask ((__v8hi) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu16_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxwd128_mask ((__v8hi) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu16_epi64 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxwq256_mask ((__v8hi) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxwq256_mask ((__v8hi) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu16_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxwq128_mask ((__v8hi) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu16_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxwq128_mask ((__v8hi) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu32_epi64 (__m256i __W, __mmask8 __U, __m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxdq256_mask ((__v4si) __X,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu32_epi64 (__mmask8 __U, __m128i __X)
{
  return (__m256i) __builtin_ia32_pmovzxdq256_mask ((__v4si) __X,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu32_epi64 (__m128i __W, __mmask8 __U, __m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxdq128_mask ((__v4si) __X,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu32_epi64 (__mmask8 __U, __m128i __X)
{
  return (__m128i) __builtin_ia32_pmovzxdq128_mask ((__v4si) __X,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rcp14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rcp14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rcp14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rcp14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rcp14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rcp14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rcp14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rcp14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rcp14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rcp14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rcp14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rsqrt14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rsqrt14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rsqrt14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rsqrt14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rsqrt14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rsqrt14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rsqrt14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rsqrt14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rsqrt14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rsqrt14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rsqrt14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sqrt_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_sqrtpd256_mask ((__v4df) __A,
        (__v4df) __W,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sqrt_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_sqrtpd256_mask ((__v4df) __A,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sqrt_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_sqrtpd128_mask ((__v2df) __A,
        (__v2df) __W,
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sqrt_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_sqrtpd128_mask ((__v2df) __A,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sqrt_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_sqrtps256_mask ((__v8sf) __A,
       (__v8sf) __W,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sqrt_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_sqrtps256_mask ((__v8sf) __A,
       (__v8sf)
       _mm256_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sqrt_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_sqrtps128_mask ((__v4sf) __A,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sqrt_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_sqrtps128_mask ((__v4sf) __A,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_paddd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_paddq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_psubd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_psubq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_paddd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_paddq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psubd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psubq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_getexp_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_getexp_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_getexp_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_getexp_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_getexp_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_getexp_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getexp_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getexp_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getexp_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srl_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psrld256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srl_epi32 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psrld256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srl_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psrld128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srl_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrld128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srl_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psrlq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srl_epi64 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psrlq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srl_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srl_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_and_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pandd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_and_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pandd256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_scalef_pd (__m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_scalef_pd (__m256d __W, __mmask8 __U, __m256d __A,
         __m256d __B)
{
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_scalef_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_scalef_ps (__m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_scalef_ps (__m256 __W, __mmask8 __U, __m256 __A,
         __m256 __B)
{
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_scalef_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_pd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_scalef_pd (__m128d __W, __mmask8 __U, __m128d __A,
      __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_scalef_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_scalef_ps (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_scalef_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_scalef_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmadd_pd (__m256d __A, __mmask8 __U, __m256d __B,
        __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df) __C,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmadd_pd (__m256d __A, __m256d __B, __m256d __C,
         __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask3 ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmadd_pd (__mmask8 __U, __m256d __A, __m256d __B,
         __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_maskz ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_pd (__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df) __C,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_pd (__m128d __A, __m128d __B, __m128d __C,
      __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask3 ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_pd (__mmask8 __U, __m128d __A, __m128d __B,
      __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_maskz ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmadd_ps (__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf) __C,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmadd_ps (__m256 __A, __m256 __B, __m256 __C,
         __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask3 ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmadd_ps (__mmask8 __U, __m256 __A, __m256 __B,
         __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_maskz ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmadd_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf) __C,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmadd_ps (__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask3 ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmadd_ps (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_maskz ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmsub_pd (__m256d __A, __mmask8 __U, __m256d __B,
        __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmsubpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df) __C,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmsub_pd (__m256d __A, __m256d __B, __m256d __C,
         __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmsubpd256_mask3 ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmsub_pd (__mmask8 __U, __m256d __A, __m256d __B,
         __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmsubpd256_maskz ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_pd (__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmsubpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df) __C,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_pd (__m128d __A, __m128d __B, __m128d __C,
      __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubpd128_mask3 ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_pd (__mmask8 __U, __m128d __A, __m128d __B,
      __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmsubpd128_maskz ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmsub_ps (__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmsubps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf) __C,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmsub_ps (__m256 __A, __m256 __B, __m256 __C,
         __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmsubps256_mask3 ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmsub_ps (__mmask8 __U, __m256 __A, __m256 __B,
         __m256 __C)
{
  return (__m256) __builtin_ia32_vfmsubps256_maskz ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsub_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmsubps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf) __C,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsub_ps (__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubps128_mask3 ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsub_ps (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmsubps128_maskz ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmaddsub_pd (__m256d __A, __mmask8 __U, __m256d __B,
    __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask ((__v4df) __A,
             (__v4df) __B,
             (__v4df) __C,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmaddsub_pd (__m256d __A, __m256d __B, __m256d __C,
     __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask3 ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __C,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmaddsub_pd (__mmask8 __U, __m256d __A, __m256d __B,
     __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_maskz ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __C,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmaddsub_pd (__m128d __A, __mmask8 __U, __m128d __B,
        __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask ((__v2df) __A,
             (__v2df) __B,
             (__v2df) __C,
             (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmaddsub_pd (__m128d __A, __m128d __B, __m128d __C,
         __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask3 ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __C,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmaddsub_pd (__mmask8 __U, __m128d __A, __m128d __B,
         __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_maskz ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __C,
       (__mmask8)
       __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmaddsub_ps (__m256 __A, __mmask8 __U, __m256 __B,
    __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask ((__v8sf) __A,
            (__v8sf) __B,
            (__v8sf) __C,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmaddsub_ps (__m256 __A, __m256 __B, __m256 __C,
     __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask3 ((__v8sf) __A,
             (__v8sf) __B,
             (__v8sf) __C,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmaddsub_ps (__mmask8 __U, __m256 __A, __m256 __B,
     __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_maskz ((__v8sf) __A,
             (__v8sf) __B,
             (__v8sf) __C,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmaddsub_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf) __C,
            (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmaddsub_ps (__m128 __A, __m128 __B, __m128 __C,
         __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask3 ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf) __C,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmaddsub_ps (__mmask8 __U, __m128 __A, __m128 __B,
         __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_maskz ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf) __C,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmsubadd_pd (__m256d __A, __mmask8 __U, __m256d __B,
    __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask ((__v4df) __A,
             (__v4df) __B,
             -(__v4df) __C,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmsubadd_pd (__m256d __A, __m256d __B, __m256d __C,
     __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmsubaddpd256_mask3 ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __C,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmsubadd_pd (__mmask8 __U, __m256d __A, __m256d __B,
     __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_maskz ((__v4df) __A,
       (__v4df) __B,
       -(__v4df) __C,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsubadd_pd (__m128d __A, __mmask8 __U, __m128d __B,
        __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask ((__v2df) __A,
             (__v2df) __B,
             -(__v2df) __C,
             (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsubadd_pd (__m128d __A, __m128d __B, __m128d __C,
         __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubaddpd128_mask3 ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __C,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsubadd_pd (__mmask8 __U, __m128d __A, __m128d __B,
         __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_maskz ((__v2df) __A,
       (__v2df) __B,
       -(__v2df) __C,
       (__mmask8)
       __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fmsubadd_ps (__m256 __A, __mmask8 __U, __m256 __B,
    __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask ((__v8sf) __A,
            (__v8sf) __B,
            -(__v8sf) __C,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fmsubadd_ps (__m256 __A, __m256 __B, __m256 __C,
     __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmsubaddps256_mask3 ((__v8sf) __A,
             (__v8sf) __B,
             (__v8sf) __C,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fmsubadd_ps (__mmask8 __U, __m256 __A, __m256 __B,
     __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_maskz ((__v8sf) __A,
             (__v8sf) __B,
             -(__v8sf) __C,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fmsubadd_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask ((__v4sf) __A,
            (__v4sf) __B,
            -(__v4sf) __C,
            (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fmsubadd_ps (__m128 __A, __m128 __B, __m128 __C,
         __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubaddps128_mask3 ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf) __C,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fmsubadd_ps (__mmask8 __U, __m128 __A, __m128 __B,
         __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_maskz ((__v4sf) __A,
             (__v4sf) __B,
             -(__v4sf) __C,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fnmadd_pd (__m256d __A, __mmask8 __U, __m256d __B,
         __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmaddpd256_mask ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fnmadd_pd (__m256d __A, __m256d __B, __m256d __C,
   __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfnmaddpd256_mask3 ((__v4df) __A,
            (__v4df) __B,
            (__v4df) __C,
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fnmadd_pd (__mmask8 __U, __m256d __A, __m256d __B,
   __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmaddpd256_maskz ((__v4df) __A,
            (__v4df) __B,
            (__v4df) __C,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_pd (__m128d __A, __mmask8 __U, __m128d __B,
      __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmaddpd128_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_pd (__m128d __A, __m128d __B, __m128d __C,
       __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfnmaddpd128_mask3 ((__v2df) __A,
            (__v2df) __B,
            (__v2df) __C,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_pd (__mmask8 __U, __m128d __A, __m128d __B,
       __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmaddpd128_maskz ((__v2df) __A,
            (__v2df) __B,
            (__v2df) __C,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fnmadd_ps (__m256 __A, __mmask8 __U, __m256 __B,
         __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmaddps256_mask ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fnmadd_ps (__m256 __A, __m256 __B, __m256 __C,
   __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfnmaddps256_mask3 ((__v8sf) __A,
           (__v8sf) __B,
           (__v8sf) __C,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fnmadd_ps (__mmask8 __U, __m256 __A, __m256 __B,
   __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmaddps256_maskz ((__v8sf) __A,
           (__v8sf) __B,
           (__v8sf) __C,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmadd_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmaddps128_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmadd_ps (__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfnmaddps128_mask3 ((__v4sf) __A,
           (__v4sf) __B,
           (__v4sf) __C,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmadd_ps (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmaddps128_maskz ((__v4sf) __A,
           (__v4sf) __B,
           (__v4sf) __C,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fnmsub_pd (__m256d __A, __mmask8 __U, __m256d __B,
         __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmsubpd256_mask ((__v4df) __A,
           (__v4df) __B,
           (__v4df) __C,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fnmsub_pd (__m256d __A, __m256d __B, __m256d __C,
   __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfnmsubpd256_mask3 ((__v4df) __A,
            (__v4df) __B,
            (__v4df) __C,
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fnmsub_pd (__mmask8 __U, __m256d __A, __m256d __B,
   __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmsubpd256_maskz ((__v4df) __A,
            (__v4df) __B,
            (__v4df) __C,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_pd (__m128d __A, __mmask8 __U, __m128d __B,
      __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmsubpd128_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __C,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_pd (__m128d __A, __m128d __B, __m128d __C,
       __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfnmsubpd128_mask3 ((__v2df) __A,
            (__v2df) __B,
            (__v2df) __C,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_pd (__mmask8 __U, __m128d __A, __m128d __B,
       __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmsubpd128_maskz ((__v2df) __A,
            (__v2df) __B,
            (__v2df) __C,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fnmsub_ps (__m256 __A, __mmask8 __U, __m256 __B,
         __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmsubps256_mask ((__v8sf) __A,
          (__v8sf) __B,
          (__v8sf) __C,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask3_fnmsub_ps (__m256 __A, __m256 __B, __m256 __C,
   __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfnmsubps256_mask3 ((__v8sf) __A,
           (__v8sf) __B,
           (__v8sf) __C,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fnmsub_ps (__mmask8 __U, __m256 __A, __m256 __B,
   __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmsubps256_maskz ((__v8sf) __A,
           (__v8sf) __B,
           (__v8sf) __C,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fnmsub_ps (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmsubps128_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask3_fnmsub_ps (__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfnmsubps128_mask3 ((__v4sf) __A,
           (__v4sf) __B,
           (__v4sf) __C,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fnmsub_ps (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmsubps128_maskz ((__v4sf) __A,
           (__v4sf) __B,
           (__v4sf) __C,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_and_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pandd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_and_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pandd128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_andnot_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
     __m256i __B)
{
  return (__m256i) __builtin_ia32_pandnd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_andnot_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pandnd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_andnot_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_pandnd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_andnot_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pandnd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_or_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pord256_mask ((__v8si) __A,
      (__v8si) __B,
      (__v8si) __W,
      (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_or_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pord256_mask ((__v8si) __A,
      (__v8si) __B,
      (__v8si)
      _mm256_setzero_si256 (),
      (__mmask8) __U);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_or_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8su)__A | (__v8su)__B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_or_epi32 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pord128_mask ((__v4si) __A,
      (__v4si) __B,
      (__v4si) __W,
      (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_or_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pord128_mask ((__v4si) __A,
      (__v4si) __B,
      (__v4si)
      _mm_setzero_si128 (),
      (__mmask8) __U);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4su)__A | (__v4su)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_xor_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pxord256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_xor_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pxord256_mask ((__v8si) __A,
       (__v8si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_xor_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v8su)__A ^ (__v8su)__B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_xor_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pxord128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_xor_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pxord128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v4su)__A ^ (__v4su)__B);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m128d __A)
{
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtpd_ps (__mmask8 __U, __m128d __A)
{
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m256d __A)
{
  return (__m128) __builtin_ia32_cvtpd2ps256_mask ((__v4df) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtpd_ps (__mmask8 __U, __m256d __A)
{
  return (__m128) __builtin_ia32_cvtpd2ps256_mask ((__v4df) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_epi32 (__m256i __W, __mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvtps2dq256_mask ((__v8sf) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_epi32 (__mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvtps2dq256_mask ((__v8sf) __A,
          (__v8si)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_epi32 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2dq128_mask ((__v4sf) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_epi32 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2dq128_mask ((__v4sf) __A,
          (__v4si)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_epu32 (__m256 __A)
{
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_epu32 (__m256i __W, __mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_epu32 (__mmask8 __U, __m256 __A)
{
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_epu32 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_epu32 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_epu32 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_movedup_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_movddup256_mask ((__v4df) __A,
         (__v4df) __W,
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_movedup_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_movddup256_mask ((__v4df) __A,
         (__v4df)
         _mm256_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_movedup_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_movddup128_mask ((__v2df) __A,
         (__v2df) __W,
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_movedup_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_movddup128_mask ((__v2df) __A,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_movehdup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movshdup256_mask ((__v8sf) __A,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_movehdup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movshdup256_mask ((__v8sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_movehdup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movshdup128_mask ((__v4sf) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_movehdup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movshdup128_mask ((__v4sf) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_moveldup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movsldup256_mask ((__v8sf) __A,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_moveldup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_movsldup256_mask ((__v8sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_moveldup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movsldup128_mask ((__v4sf) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_moveldup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_movsldup128_mask ((__v4sf) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhdq128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhdq128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhdq256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhdq256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhqdq128_mask ((__v2di) __A,
            (__v2di) __B,
            (__v2di) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhqdq128_mask ((__v2di) __A,
            (__v2di) __B,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhqdq256_mask ((__v4di) __A,
            (__v4di) __B,
            (__v4di) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhqdq256_mask ((__v4di) __A,
            (__v4di) __B,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckldq128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckldq128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckldq256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckldq256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklqdq128_mask ((__v2di) __A,
            (__v2di) __B,
            (__v2di) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklqdq128_mask ((__v2di) __A,
            (__v2di) __B,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklqdq256_mask ((__v4di) __A,
            (__v4di) __B,
            (__v4di) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklqdq256_mask ((__v4di) __A,
            (__v4di) __B,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epu32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __A,
         (__v4si) __B, 0,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqd128_mask ((__v4si) __A,
          (__v4si) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epu32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __A,
         (__v4si) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqd128_mask ((__v4si) __A,
          (__v4si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epu32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __A,
         (__v8si) __B, 0,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqd256_mask ((__v8si) __A,
          (__v8si) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epu32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __A,
         (__v8si) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqd256_mask ((__v8si) __A,
          (__v8si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epu64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __A,
         (__v2di) __B, 0,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq128_mask ((__v2di) __A,
          (__v2di) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epu64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __A,
         (__v2di) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq128_mask ((__v2di) __A,
          (__v2di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epu64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __A,
         (__v4di) __B, 0,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq256_mask ((__v4di) __A,
          (__v4di) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epu64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __A,
         (__v4di) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqq256_mask ((__v4di) __A,
          (__v4di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epu32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __A,
         (__v4si) __B, 6,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtd128_mask ((__v4si) __A,
          (__v4si) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epu32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __A,
         (__v4si) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtd128_mask ((__v4si) __A,
          (__v4si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epu32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __A,
         (__v8si) __B, 6,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtd256_mask ((__v8si) __A,
          (__v8si) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epu32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __A,
         (__v8si) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtd256_mask ((__v8si) __A,
          (__v8si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epu64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __A,
         (__v2di) __B, 6,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq128_mask ((__v2di) __A,
          (__v2di) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epu64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __A,
         (__v2di) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq128_mask ((__v2di) __A,
          (__v2di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epu64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __A,
         (__v4di) __B, 6,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq256_mask ((__v4di) __A,
          (__v4di) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epu64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __A,
         (__v4di) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtq256_mask ((__v4di) __A,
          (__v4di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_test_epi32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmd128 ((__v4si) __A,
            (__v4si) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_test_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmd128 ((__v4si) __A,
            (__v4si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_test_epi32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestmd256 ((__v8si) __A,
            (__v8si) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_test_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestmd256 ((__v8si) __A,
            (__v8si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_test_epi64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq128 ((__v2di) __A,
            (__v2di) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_test_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq128 ((__v2di) __A,
            (__v2di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_test_epi64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq256 ((__v4di) __A,
            (__v4di) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_test_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestmq256 ((__v4di) __A,
            (__v4di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_testn_epi32_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmd128 ((__v4si) __A,
      (__v4si) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_testn_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmd128 ((__v4si) __A,
      (__v4si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testn_epi32_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmd256 ((__v8si) __A,
      (__v8si) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_testn_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmd256 ((__v8si) __A,
      (__v8si) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_testn_epi64_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq128 ((__v2di) __A,
      (__v2di) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_testn_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq128 ((__v2di) __A,
      (__v2di) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testn_epi64_mask (__m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq256 ((__v4di) __A,
      (__v4di) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_testn_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmq256 ((__v4di) __A,
      (__v4di) __B, __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
            (__v4df) __W,
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
            (__v4df)
            _mm256_setzero_pd (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_compressstoredf256_mask ((__v4df *) __P,
       (__v4df) __A,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
            (__v2df) __W,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
            (__v2df)
            _mm_setzero_pd (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_compressstoredf128_mask ((__v2df *) __P,
       (__v2df) __A,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
           (__v8sf) __W,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
           (__v8sf)
           _mm256_setzero_ps (),
           (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_compressstoresf256_mask ((__v8sf *) __P,
       (__v8sf) __A,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
           (__v4sf) __W,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
           (__v4sf)
           _mm_setzero_ps (),
           (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_compressstoresf128_mask ((__v4sf *) __P,
       (__v4sf) __A,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
            (__v4di) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_compressstoredi256_mask ((__v4di *) __P,
       (__v4di) __A,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
            (__v2di) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_compressstoredi128_mask ((__v2di *) __P,
       (__v2di) __A,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
            (__v8si) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
            (__v8si)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_compressstoresi256_mask ((__v8si *) __P,
       (__v8si) __A,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
            (__v4si) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
            (__v4si)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_compressstoresi128_mask ((__v4si *) __P,
       (__v4si) __A,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_expanddf256_maskz ((__v4df) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((__v4df *) __P,
       (__v4df) __W,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_expandloaddf256_maskz ((__v4df *) __P,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8)
        __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_expanddf128_maskz ((__v2df) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((__v2df *) __P,
       (__v2df) __W,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_expandloaddf128_maskz ((__v2df *) __P,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8)
        __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_expandsf256_maskz ((__v8sf) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_expandloadsf256_mask ((__v8sf *) __P,
             (__v8sf) __W,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_expandloadsf256_maskz ((__v8sf *) __P,
       (__v8sf)
       _mm256_setzero_ps (),
       (__mmask8)
       __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_expandsf128_maskz ((__v4sf) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_expandloadsf128_mask ((__v4sf *) __P,
             (__v4sf) __W,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_expandloadsf128_maskz ((__v4sf *) __P,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_expanddi256_maskz ((__v4di) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_epi64 (__m256i __W, __mmask8 __U,
          void const *__P)
{
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((__v4di *) __P,
       (__v4di) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloaddi256_maskz ((__v4di *) __P,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_expanddi128_maskz ((__v2di) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((__v2di *) __P,
       (__v2di) __W,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloaddi128_maskz ((__v2di *) __P,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_expandsi256_maskz ((__v8si) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_epi32 (__m256i __W, __mmask8 __U,
          void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((__v8si *) __P,
       (__v8si) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadsi256_maskz ((__v8si *) __P,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_expandsi128_maskz ((__v4si) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((__v4si *) __P,
       (__v4si) __W,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadsi128_maskz ((__v4si *) __P,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_pd (__m256d __A, __m256i __I, __m256d __B)
{
  return (__m256d) __builtin_ia32_vpermt2varpd256_mask ((__v4di) __I
                 ,
       (__v4df) __A,
       (__v4df) __B,
       (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_pd (__m256d __A, __mmask8 __U, __m256i __I,
        __m256d __B)
{
  return (__m256d) __builtin_ia32_vpermt2varpd256_mask ((__v4di) __I
                 ,
       (__v4df) __A,
       (__v4df) __B,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_pd (__m256d __A, __m256i __I, __mmask8 __U,
         __m256d __B)
{
  return (__m256d) __builtin_ia32_vpermi2varpd256_mask ((__v4df) __A,
       (__v4di) __I
                 ,
       (__v4df) __B,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_pd (__mmask8 __U, __m256d __A, __m256i __I,
         __m256d __B)
{
  return (__m256d) __builtin_ia32_vpermt2varpd256_maskz ((__v4di) __I
                  ,
        (__v4df) __A,
        (__v4df) __B,
        (__mmask8)
        __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_ps (__m256 __A, __m256i __I, __m256 __B)
{
  return (__m256) __builtin_ia32_vpermt2varps256_mask ((__v8si) __I
                       ,
             (__v8sf) __A,
             (__v8sf) __B,
             (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_ps (__m256 __A, __mmask8 __U, __m256i __I,
        __m256 __B)
{
  return (__m256) __builtin_ia32_vpermt2varps256_mask ((__v8si) __I
                       ,
             (__v8sf) __A,
             (__v8sf) __B,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_ps (__m256 __A, __m256i __I, __mmask8 __U,
         __m256 __B)
{
  return (__m256) __builtin_ia32_vpermi2varps256_mask ((__v8sf) __A,
             (__v8si) __I
                       ,
             (__v8sf) __B,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_ps (__mmask8 __U, __m256 __A, __m256i __I,
         __m256 __B)
{
  return (__m256) __builtin_ia32_vpermt2varps256_maskz ((__v8si) __I
                 ,
       (__v8sf) __A,
       (__v8sf) __B,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_epi64 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varq128_mask ((__v2di) __I
                       ,
             (__v2di) __A,
             (__v2di) __B,
             (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_epi64 (__m128i __A, __mmask8 __U, __m128i __I,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varq128_mask ((__v2di) __I
                       ,
             (__v2di) __A,
             (__v2di) __B,
             (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_epi64 (__m128i __A, __m128i __I, __mmask8 __U,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2varq128_mask ((__v2di) __A,
             (__v2di) __I
                       ,
             (__v2di) __B,
             (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_epi64 (__mmask8 __U, __m128i __A, __m128i __I,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varq128_maskz ((__v2di) __I
                 ,
       (__v2di) __A,
       (__v2di) __B,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_epi32 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2vard128_mask ((__v4si) __I
                       ,
             (__v4si) __A,
             (__v4si) __B,
             (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_epi32 (__m128i __A, __mmask8 __U, __m128i __I,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2vard128_mask ((__v4si) __I
                       ,
             (__v4si) __A,
             (__v4si) __B,
             (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_epi32 (__m128i __A, __m128i __I, __mmask8 __U,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2vard128_mask ((__v4si) __A,
             (__v4si) __I
                       ,
             (__v4si) __B,
             (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_epi32 (__mmask8 __U, __m128i __A, __m128i __I,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2vard128_maskz ((__v4si) __I
                 ,
       (__v4si) __A,
       (__v4si) __B,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_epi64 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varq256_mask ((__v4di) __I
                       ,
             (__v4di) __A,
             (__v4di) __B,
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_epi64 (__m256i __A, __mmask8 __U, __m256i __I,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varq256_mask ((__v4di) __I
                       ,
             (__v4di) __A,
             (__v4di) __B,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_epi64 (__m256i __A, __m256i __I,
     __mmask8 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2varq256_mask ((__v4di) __A,
             (__v4di) __I
                       ,
             (__v4di) __B,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_epi64 (__mmask8 __U, __m256i __A,
     __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varq256_maskz ((__v4di) __I
                 ,
       (__v4di) __A,
       (__v4di) __B,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_epi32 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2vard256_mask ((__v8si) __I
                       ,
             (__v8si) __A,
             (__v8si) __B,
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_epi32 (__m256i __A, __mmask8 __U, __m256i __I,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2vard256_mask ((__v8si) __I
                       ,
             (__v8si) __A,
             (__v8si) __B,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_epi32 (__m256i __A, __m256i __I,
     __mmask8 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2vard256_mask ((__v8si) __A,
             (__v8si) __I
                       ,
             (__v8si) __B,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_epi32 (__mmask8 __U, __m256i __A,
     __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2vard256_maskz ((__v8si) __I
                 ,
       (__v8si) __A,
       (__v8si) __B,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_pd (__m128d __A, __m128i __I, __m128d __B)
{
  return (__m128d) __builtin_ia32_vpermt2varpd128_mask ((__v2di) __I
                 ,
       (__v2df) __A,
       (__v2df) __B,
       (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_pd (__m128d __A, __mmask8 __U, __m128i __I,
     __m128d __B)
{
  return (__m128d) __builtin_ia32_vpermt2varpd128_mask ((__v2di) __I
                 ,
       (__v2df) __A,
       (__v2df) __B,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_pd (__m128d __A, __m128i __I, __mmask8 __U,
      __m128d __B)
{
  return (__m128d) __builtin_ia32_vpermi2varpd128_mask ((__v2df) __A,
       (__v2di) __I
                 ,
       (__v2df) __B,
       (__mmask8)
       __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_pd (__mmask8 __U, __m128d __A, __m128i __I,
      __m128d __B)
{
  return (__m128d) __builtin_ia32_vpermt2varpd128_maskz ((__v2di) __I
                  ,
        (__v2df) __A,
        (__v2df) __B,
        (__mmask8)
        __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_ps (__m128 __A, __m128i __I, __m128 __B)
{
  return (__m128) __builtin_ia32_vpermt2varps128_mask ((__v4si) __I
                       ,
             (__v4sf) __A,
             (__v4sf) __B,
             (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_ps (__m128 __A, __mmask8 __U, __m128i __I,
     __m128 __B)
{
  return (__m128) __builtin_ia32_vpermt2varps128_mask ((__v4si) __I
                       ,
             (__v4sf) __A,
             (__v4sf) __B,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_ps (__m128 __A, __m128i __I, __mmask8 __U,
      __m128 __B)
{
  return (__m128) __builtin_ia32_vpermi2varps128_mask ((__v4sf) __A,
             (__v4si) __I
                       ,
             (__v4sf) __B,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_ps (__mmask8 __U, __m128 __A, __m128i __I,
      __m128 __B)
{
  return (__m128) __builtin_ia32_vpermt2varps128_maskz ((__v4si) __I
                 ,
       (__v4sf) __A,
       (__v4sf) __B,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srav_epi64 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psravq128_mask ((__v2di) __X,
        (__v2di) __Y,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srav_epi64 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psravq128_mask ((__v2di) __X,
        (__v2di) __Y,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srav_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psravq128_mask ((__v2di) __X,
        (__v2di) __Y,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sllv_epi32 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sllv_epi32 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sllv_epi32 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sllv_epi32 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sllv_epi64 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv4di_mask ((__v4di) __X,
       (__v4di) __Y,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sllv_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psllv4di_mask ((__v4di) __X,
       (__v4di) __Y,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sllv_epi64 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv2di_mask ((__v2di) __X,
       (__v2di) __Y,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sllv_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psllv2di_mask ((__v2di) __X,
       (__v2di) __Y,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srav_epi32 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrav8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srav_epi32 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrav8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srav_epi32 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrav4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srav_epi32 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrav4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srlv_epi32 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srlv_epi32 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv8si_mask ((__v8si) __X,
       (__v8si) __Y,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srlv_epi32 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srlv_epi32 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv4si_mask ((__v4si) __X,
       (__v4si) __Y,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srlv_epi64 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv4di_mask ((__v4di) __X,
       (__v4di) __Y,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srlv_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psrlv4di_mask ((__v4di) __X,
       (__v4di) __Y,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srlv_epi64 (__m128i __W, __mmask8 __U, __m128i __X,
       __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv2di_mask ((__v2di) __X,
       (__v2di) __Y,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srlv_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_psrlv2di_mask ((__v2di) __X,
       (__v2di) __Y,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rolv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rolv_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rolv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rolv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rolv_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rolv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rorv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rorv_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rorv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rorv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rorv_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rorv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rolv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rolv_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rolv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rolv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rolv_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rolv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rorv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rorv_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rorv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rorv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rorv_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rorv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srav_epi64 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psravq256_mask ((__v4di) __X,
        (__v4di) __Y,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srav_epi64 (__m256i __W, __mmask8 __U, __m256i __X,
   __m256i __Y)
{
  return (__m256i) __builtin_ia32_psravq256_mask ((__v4di) __X,
        (__v4di) __Y,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srav_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_psravq256_mask ((__v4di) __X,
        (__v4di) __Y,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_and_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pandq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di) __W, __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_and_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pandq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di)
       _mm256_setzero_pd (),
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_and_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pandq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W, __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_and_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pandq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_pd (),
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_andnot_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
     __m256i __B)
{
  return (__m256i) __builtin_ia32_pandnq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W, __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_andnot_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pandnq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_pd (),
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_andnot_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_pandnq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W, __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_andnot_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pandnq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_pd (),
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_or_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_porq256_mask ((__v4di) __A,
      (__v4di) __B,
      (__v4di) __W,
      (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_or_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_porq256_mask ((__v4di) __A,
      (__v4di) __B,
      (__v4di)
      _mm256_setzero_si256 (),
      (__mmask8) __U);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_or_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A | (__v4du)__B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_or_epi64 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_porq128_mask ((__v2di) __A,
      (__v2di) __B,
      (__v2di) __W,
      (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_or_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_porq128_mask ((__v2di) __A,
      (__v2di) __B,
      (__v2di)
      _mm_setzero_si128 (),
      (__mmask8) __U);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_or_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A | (__v2du)__B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_xor_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pxorq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_xor_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pxorq256_mask ((__v4di) __A,
       (__v4di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_xor_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du)__A ^ (__v4du)__B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_xor_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pxorq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_xor_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pxorq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_xor_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du)__A ^ (__v2du)__B);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_maxpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_maxpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_maxps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_maxps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_divps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_divps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_div_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_divpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_div_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_divpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_minpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_div_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_divpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_minpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_minps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_div_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_divpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_div_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_divps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_minps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_div_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_divps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_mulps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_minps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_mulps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_maxps_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_minpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_minpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_maxpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_maxpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_mulpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_mulpd_mask ((__v2df) __A,
           (__v2df) __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mul_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_mulps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mul_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_mulps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mul_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_mulpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mul_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_mulpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epi64 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epi64 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epi64 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epi64 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epu64 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_max_epu64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epu64 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_min_epu64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epu64 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epu64 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epi32 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epi32 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epi32 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epi32 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsd256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epu32 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxud256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epu32 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxud256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epu32 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminud256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epu32 (__m256i __W, __mmask8 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminud256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epi64 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epi64 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epi64 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epi64 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epu64 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_max_epu64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epu64 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_min_epu64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epu64 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epu64 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epi32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epi32 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epi32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epi32 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsd128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epu32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxud128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epu32 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxud128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epu32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminud128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epu32 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminud128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W, __M);
}


#pragma GCC push_options
#pragma GCC target("avx512vl,avx512cd")



extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_broadcastmb128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_broadcastmb256 (__A);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m128i) __builtin_ia32_broadcastmw128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m256i) __builtin_ia32_broadcastmw256 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_lzcnt_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_lzcnt_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_lzcnt_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_lzcnt_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_lzcnt_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_lzcnt_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_conflict_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_conflict_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
        (__v4di) __W,
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_conflict_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_conflict_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_conflict_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
        (__v8si) __W,
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_conflict_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_lzcnt_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_lzcnt_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
           (__v4si) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_lzcnt_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
           (__v4si)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_lzcnt_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_lzcnt_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_lzcnt_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_conflict_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_conflict_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
        (__v2di) __W,
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_conflict_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_conflict_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_conflict_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
        (__v4si) __W,
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_conflict_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}


#pragma GCC pop_options


extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_pd (__m256d __W, __mmask8 __U, __m256d __A,
    __m256d __B)
{
  return (__m256d) __builtin_ia32_unpcklpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_unpcklpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_pd (__m128d __W, __mmask8 __U, __m128d __A,
        __m128d __B)
{
  return (__m128d) __builtin_ia32_unpcklpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_unpcklpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_ps (__m256 __W, __mmask8 __U, __m256 __A,
    __m256 __B)
{
  return (__m256) __builtin_ia32_unpcklps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_pd (__m256d __W, __mmask8 __U, __m256d __A,
    __m256d __B)
{
  return (__m256d) __builtin_ia32_unpckhpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_unpckhpd256_mask ((__v4df) __A,
          (__v4df) __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_pd (__m128d __W, __mmask8 __U, __m128d __A,
        __m128d __B)
{
  return (__m128d) __builtin_ia32_unpckhpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_unpckhpd128_mask ((__v2df) __A,
          (__v2df) __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_ps (__m256 __W, __mmask8 __U, __m256 __A,
    __m256 __B)
{
  return (__m256) __builtin_ia32_unpckhps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_unpckhps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpckhps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpckhps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtph_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_unpcklps256_mask ((__v8sf) __A,
         (__v8sf) __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtph_ps (__m256 __W, __mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpcklps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_unpcklps128_mask ((__v4sf) __A,
         (__v4sf) __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sra_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psrad256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sra_epi32 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psrad256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sra_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psrad128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sra_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrad128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sra_epi64 (__m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psraq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sra_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psraq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sra_epi64 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psraq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sra_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psraq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sra_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psraq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sra_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psraq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sll_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pslld128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sll_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pslld128_mask ((__v4si) __A,
       (__v4si) __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sll_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psllq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sll_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psllq128_mask ((__v2di) __A,
       (__v2di) __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sll_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_pslld256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sll_epi32 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_pslld256_mask ((__v8si) __A,
       (__v4si) __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sll_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psllq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sll_epi64 (__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psllq256_mask ((__v4di) __A,
       (__v2di) __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_ps (__m256 __W, __mmask8 __U, __m256i __X,
       __m256 __Y)
{
  return (__m256) __builtin_ia32_permvarsf256_mask ((__v8sf) __Y,
          (__v8si) __X,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_ps (__mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256) __builtin_ia32_permvarsf256_mask ((__v8sf) __Y,
          (__v8si) __X,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutexvar_pd (__m256i __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
           (__v4di) __X,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_pd (__m256d __W, __mmask8 __U, __m256i __X,
       __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
           (__v4di) __X,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_pd (__mmask8 __U, __m256i __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
           (__v4di) __X,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutevar_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256i __C)
{
  return (__m256d) __builtin_ia32_vpermilvarpd256_mask ((__v4df) __A,
       (__v4di) __C,
       (__v4df) __W,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutevar_pd (__mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d) __builtin_ia32_vpermilvarpd256_mask ((__v4df) __A,
       (__v4di) __C,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8)
       __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutevar_ps (__m256 __W, __mmask8 __U, __m256 __A,
      __m256i __C)
{
  return (__m256) __builtin_ia32_vpermilvarps256_mask ((__v8sf) __A,
             (__v8si) __C,
             (__v8sf) __W,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutevar_ps (__mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256) __builtin_ia32_vpermilvarps256_mask ((__v8sf) __A,
             (__v8si) __C,
             (__v8sf)
             _mm256_setzero_ps (),
             (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutevar_pd (__m128d __W, __mmask8 __U, __m128d __A,
   __m128i __C)
{
  return (__m128d) __builtin_ia32_vpermilvarpd_mask ((__v2df) __A,
           (__v2di) __C,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutevar_pd (__mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d) __builtin_ia32_vpermilvarpd_mask ((__v2df) __A,
           (__v2di) __C,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutevar_ps (__m128 __W, __mmask8 __U, __m128 __A,
   __m128i __C)
{
  return (__m128) __builtin_ia32_vpermilvarps_mask ((__v4sf) __A,
          (__v4si) __C,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutevar_ps (__mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128) __builtin_ia32_vpermilvarps_mask ((__v4sf) __A,
          (__v4si) __C,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mullo_epi32 (__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulld256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_epi64 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
           (__v4di) __X,
           (__v4di)
           _mm256_setzero_si256 (),
           __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mullo_epi32 (__m256i __W, __mmask8 __M, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulld256_mask ((__v8si) __A,
        (__v8si) __B,
        (__v8si) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mullo_epi32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulld128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mullo_epi32 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulld128_mask ((__v4si) __A,
        (__v4si) __B,
        (__v4si) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mul_epi32 (__m256i __W, __mmask8 __M, __m256i __X,
         __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmuldq256_mask ((__v8si) __X,
        (__v8si) __Y,
        (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mul_epi32 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmuldq256_mask ((__v8si) __X,
        (__v8si) __Y,
        (__v4di)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_epi32 (__m128i __W, __mmask8 __M, __m128i __X,
      __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmuldq128_mask ((__v4si) __X,
        (__v4si) __Y,
        (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_epi32 (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmuldq128_mask ((__v4si) __X,
        (__v4si) __Y,
        (__v2di)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutexvar_epi64 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
           (__v4di) __X,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_epi64 (__m256i __W, __mmask8 __M, __m256i __X,
          __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
           (__v4di) __X,
           (__v4di) __W,
           __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mul_epu32 (__m256i __W, __mmask8 __M, __m256i __X,
         __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmuludq256_mask ((__v8si) __X,
         (__v8si) __Y,
         (__v4di) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_epi32 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
           (__v8si) __X,
           (__v8si)
           _mm256_setzero_si256 (),
           __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mul_epu32 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmuludq256_mask ((__v8si) __X,
         (__v8si) __Y,
         (__v4di)
         _mm256_setzero_si256 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mul_epu32 (__m128i __W, __mmask8 __M, __m128i __X,
      __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmuludq128_mask ((__v4si) __X,
         (__v4si) __Y,
         (__v2di) __W, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mul_epu32 (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmuludq128_mask ((__v4si) __X,
         (__v4si) __Y,
         (__v2di)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutexvar_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
           (__v8si) __X,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_epi32 (__m256i __W, __mmask8 __M, __m256i __X,
          __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
           (__v8si) __X,
           (__v8si) __W,
           __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epu32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 4,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epu32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 4,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epu32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 1,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epu32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 1,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epu32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 5,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epu32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 5,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epu32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 2,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epu32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, 2,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epu64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 4,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epu64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 4,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epu64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 1,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epu64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 1,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epu64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 5,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epu64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 5,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epu64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 2,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epu64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, 2,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epi32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 4,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epi32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 4,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epi32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 1,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epi32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 1,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epi32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 5,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epi32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 5,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epi32_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 2,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epi32_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, 2,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epi64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 4,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epi64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 4,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epi64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 1,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epi64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 1,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epi64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 5,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epi64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 5,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epi64_mask (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 2,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epi64_mask (__m256i __X, __m256i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, 2,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epu32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 4,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epu32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 4,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epu32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 1,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epu32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 1,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epu32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 5,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epu32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 5,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epu32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 2,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epu32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, 2,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epu64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 4,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epu64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 4,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epu64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 1,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epu64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 1,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epu64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 5,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epu64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 5,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epu64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 2,
        (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epu64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, 2,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epi32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 4,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epi32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 4,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epi32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 1,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 1,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epi32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 5,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epi32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 5,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epi32_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 2,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epi32_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, 2,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epi64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 4,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epi64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 4,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epi64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 1,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 1,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epi64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 5,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epi64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 5,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epi64_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 2,
       (__mmask8) __M);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epi64_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, 2,
       (__mmask8) -1);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex_epi64 (__m256i __X, const int __I)
{
  return (__m256i) __builtin_ia32_permdi256_mask ((__v4di) __X,
           __I,
           (__v4di)
           _mm256_setzero_si256(),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex_epi64 (__m256i __W, __mmask8 __M,
       __m256i __X, const int __I)
{
  return (__m256i) __builtin_ia32_permdi256_mask ((__v4di) __X,
        __I,
        (__v4di) __W,
        (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex_epi64 (__mmask8 __M, __m256i __X, const int __I)
{
  return (__m256i) __builtin_ia32_permdi256_mask ((__v4di) __X,
        __I,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __M);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_pd (__m256d __W, __mmask8 __U, __m256d __A,
   __m256d __B, const int __imm)
{
  return (__m256d) __builtin_ia32_shufpd256_mask ((__v4df) __A,
        (__v4df) __B, __imm,
        (__v4df) __W,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_pd (__mmask8 __U, __m256d __A, __m256d __B,
    const int __imm)
{
  return (__m256d) __builtin_ia32_shufpd256_mask ((__v4df) __A,
        (__v4df) __B, __imm,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shuffle_pd (__m128d __W, __mmask8 __U, __m128d __A,
       __m128d __B, const int __imm)
{
  return (__m128d) __builtin_ia32_shufpd128_mask ((__v2df) __A,
        (__v2df) __B, __imm,
        (__v2df) __W,
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shuffle_pd (__mmask8 __U, __m128d __A, __m128d __B,
        const int __imm)
{
  return (__m128d) __builtin_ia32_shufpd128_mask ((__v2df) __A,
        (__v2df) __B, __imm,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_ps (__m256 __W, __mmask8 __U, __m256 __A,
   __m256 __B, const int __imm)
{
  return (__m256) __builtin_ia32_shufps256_mask ((__v8sf) __A,
       (__v8sf) __B, __imm,
       (__v8sf) __W,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_ps (__mmask8 __U, __m256 __A, __m256 __B,
    const int __imm)
{
  return (__m256) __builtin_ia32_shufps256_mask ((__v8sf) __A,
       (__v8sf) __B, __imm,
       (__v8sf)
       _mm256_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shuffle_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
       const int __imm)
{
  return (__m128) __builtin_ia32_shufps128_mask ((__v4sf) __A,
       (__v4sf) __B, __imm,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shuffle_ps (__mmask8 __U, __m128 __A, __m128 __B,
        const int __imm)
{
  return (__m128) __builtin_ia32_shufps128_mask ((__v4sf) __A,
       (__v4sf) __B, __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_inserti32x4 (__m256i __A, __m128i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_inserti32x4_256_mask ((__v8si) __A,
       (__v4si) __B,
       __imm,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_inserti32x4 (__m256i __W, __mmask8 __U, __m256i __A,
    __m128i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_inserti32x4_256_mask ((__v8si) __A,
       (__v4si) __B,
       __imm,
       (__v8si) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_inserti32x4 (__mmask8 __U, __m256i __A, __m128i __B,
     const int __imm)
{
  return (__m256i) __builtin_ia32_inserti32x4_256_mask ((__v8si) __A,
       (__v4si) __B,
       __imm,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8)
       __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insertf32x4 (__m256 __A, __m128 __B, const int __imm)
{
  return (__m256) __builtin_ia32_insertf32x4_256_mask ((__v8sf) __A,
             (__v4sf) __B,
             __imm,
             (__v8sf)
             _mm256_setzero_ps (),
             (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_insertf32x4 (__m256 __W, __mmask8 __U, __m256 __A,
    __m128 __B, const int __imm)
{
  return (__m256) __builtin_ia32_insertf32x4_256_mask ((__v8sf) __A,
             (__v4sf) __B,
             __imm,
             (__v8sf) __W,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_insertf32x4 (__mmask8 __U, __m256 __A, __m128 __B,
     const int __imm)
{
  return (__m256) __builtin_ia32_insertf32x4_256_mask ((__v8sf) __A,
             (__v4sf) __B,
             __imm,
             (__v8sf)
             _mm256_setzero_ps (),
             (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extracti32x4_epi32 (__m256i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_256_mask ((__v8si) __A,
        __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_extracti32x4_epi32 (__m128i __W, __mmask8 __U, __m256i __A,
    const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_256_mask ((__v8si) __A,
        __imm,
        (__v4si) __W,
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_extracti32x4_epi32 (__mmask8 __U, __m256i __A,
     const int __imm)
{
  return (__m128i) __builtin_ia32_extracti32x4_256_mask ((__v8si) __A,
        __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extractf32x4_ps (__m256 __A, const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_256_mask ((__v8sf) __A,
       __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_extractf32x4_ps (__m128 __W, __mmask8 __U, __m256 __A,
        const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_256_mask ((__v8sf) __A,
       __imm,
       (__v4sf) __W,
       (__mmask8)
       __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_extractf32x4_ps (__mmask8 __U, __m256 __A,
         const int __imm)
{
  return (__m128) __builtin_ia32_extractf32x4_256_mask ((__v8sf) __A,
       __imm,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_i64x2 (__m256i __A, __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i64x2_256_mask ((__v4di) __A,
             (__v4di) __B,
             __imm,
             (__v4di)
             _mm256_setzero_si256 (),
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_i64x2 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i64x2_256_mask ((__v4di) __A,
             (__v4di) __B,
             __imm,
             (__v4di) __W,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_i64x2 (__mmask8 __U, __m256i __A, __m256i __B,
       const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i64x2_256_mask ((__v4di) __A,
             (__v4di) __B,
             __imm,
             (__v4di)
             _mm256_setzero_si256 (),
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_i32x4 (__m256i __A, __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i32x4_256_mask ((__v8si) __A,
             (__v8si) __B,
             __imm,
             (__v8si)
             _mm256_setzero_si256 (),
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_i32x4 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i32x4_256_mask ((__v8si) __A,
             (__v8si) __B,
             __imm,
             (__v8si) __W,
             (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_i32x4 (__mmask8 __U, __m256i __A, __m256i __B,
       const int __imm)
{
  return (__m256i) __builtin_ia32_shuf_i32x4_256_mask ((__v8si) __A,
             (__v8si) __B,
             __imm,
             (__v8si)
             _mm256_setzero_si256 (),
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_f64x2 (__m256d __A, __m256d __B, const int __imm)
{
  return (__m256d) __builtin_ia32_shuf_f64x2_256_mask ((__v4df) __A,
             (__v4df) __B,
             __imm,
             (__v4df)
             _mm256_setzero_pd (),
             (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_f64x2 (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B, const int __imm)
{
  return (__m256d) __builtin_ia32_shuf_f64x2_256_mask ((__v4df) __A,
             (__v4df) __B,
             __imm,
             (__v4df) __W,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_f64x2 (__mmask8 __U, __m256d __A, __m256d __B,
       const int __imm)
{
  return (__m256d) __builtin_ia32_shuf_f64x2_256_mask ((__v4df) __A,
             (__v4df) __B,
             __imm,
             (__v4df)
             _mm256_setzero_pd (),
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shuffle_f32x4 (__m256 __A, __m256 __B, const int __imm)
{
  return (__m256) __builtin_ia32_shuf_f32x4_256_mask ((__v8sf) __A,
            (__v8sf) __B,
            __imm,
            (__v8sf)
            _mm256_setzero_ps (),
            (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_f32x4 (__m256 __W, __mmask8 __U, __m256 __A,
      __m256 __B, const int __imm)
{
  return (__m256) __builtin_ia32_shuf_f32x4_256_mask ((__v8sf) __A,
            (__v8sf) __B,
            __imm,
            (__v8sf) __W,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_f32x4 (__mmask8 __U, __m256 __A, __m256 __B,
       const int __imm)
{
  return (__m256) __builtin_ia32_shuf_f32x4_256_mask ((__v8sf) __A,
            (__v8sf) __B,
            __imm,
            (__v8sf)
            _mm256_setzero_ps (),
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fixupimm_pd (__m256d __A, __m256d __B, __m256i __C,
      const int __imm)
{
  return (__m256d) __builtin_ia32_fixupimmpd256_mask ((__v4df) __A,
            (__v4df) __B,
            (__v4di) __C,
            __imm,
            (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fixupimm_pd (__m256d __A, __mmask8 __U, __m256d __B,
    __m256i __C, const int __imm)
{
  return (__m256d) __builtin_ia32_fixupimmpd256_mask ((__v4df) __A,
            (__v4df) __B,
            (__v4di) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fixupimm_pd (__mmask8 __U, __m256d __A, __m256d __B,
     __m256i __C, const int __imm)
{
  return (__m256d) __builtin_ia32_fixupimmpd256_maskz ((__v4df) __A,
             (__v4df) __B,
             (__v4di) __C,
             __imm,
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fixupimm_ps (__m256 __A, __m256 __B, __m256i __C,
      const int __imm)
{
  return (__m256) __builtin_ia32_fixupimmps256_mask ((__v8sf) __A,
           (__v8sf) __B,
           (__v8si) __C,
           __imm,
           (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fixupimm_ps (__m256 __A, __mmask8 __U, __m256 __B,
    __m256i __C, const int __imm)
{
  return (__m256) __builtin_ia32_fixupimmps256_mask ((__v8sf) __A,
           (__v8sf) __B,
           (__v8si) __C,
           __imm,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_fixupimm_ps (__mmask8 __U, __m256 __A, __m256 __B,
     __m256i __C, const int __imm)
{
  return (__m256) __builtin_ia32_fixupimmps256_maskz ((__v8sf) __A,
            (__v8sf) __B,
            (__v8si) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_pd (__m128d __A, __m128d __B, __m128i __C,
   const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmpd128_mask ((__v2df) __A,
            (__v2df) __B,
            (__v2di) __C,
            __imm,
            (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_pd (__m128d __A, __mmask8 __U, __m128d __B,
        __m128i __C, const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmpd128_mask ((__v2df) __A,
            (__v2df) __B,
            (__v2di) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_pd (__mmask8 __U, __m128d __A, __m128d __B,
         __m128i __C, const int __imm)
{
  return (__m128d) __builtin_ia32_fixupimmpd128_maskz ((__v2df) __A,
             (__v2df) __B,
             (__v2di) __C,
             __imm,
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fixupimm_ps (__m128 __A, __m128 __B, __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmps128_mask ((__v4sf) __A,
           (__v4sf) __B,
           (__v4si) __C,
           __imm,
           (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fixupimm_ps (__m128 __A, __mmask8 __U, __m128 __B,
        __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmps128_mask ((__v4sf) __A,
           (__v4sf) __B,
           (__v4si) __C,
           __imm,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_fixupimm_ps (__mmask8 __U, __m128 __A, __m128 __B,
         __m128i __C, const int __imm)
{
  return (__m128) __builtin_ia32_fixupimmps128_maskz ((__v4sf) __A,
            (__v4sf) __B,
            (__v4si) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srli_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psrldi256_mask ((__v8si) __A, __imm,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srli_epi32 (__mmask8 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psrldi256_mask ((__v8si) __A, __imm,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srli_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psrldi128_mask ((__v4si) __A, __imm,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srli_epi32 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psrldi128_mask ((__v4si) __A, __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srli_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psrlqi256_mask ((__v4di) __A, __imm,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srli_epi64 (__mmask8 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psrlqi256_mask ((__v4di) __A, __imm,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srli_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psrlqi128_mask ((__v2di) __A, __imm,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srli_epi64 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psrlqi128_mask ((__v2di) __A, __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_ternarylogic_epi64 (__m256i __A, __m256i __B, __m256i __C,
      const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogq256_mask ((__v4di) __A,
           (__v4di) __B,
           (__v4di) __C, __imm,
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_ternarylogic_epi64 (__m256i __A, __mmask8 __U,
    __m256i __B, __m256i __C,
    const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogq256_mask ((__v4di) __A,
           (__v4di) __B,
           (__v4di) __C, __imm,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_ternarylogic_epi64 (__mmask8 __U, __m256i __A,
     __m256i __B, __m256i __C,
     const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogq256_maskz ((__v4di) __A,
            (__v4di) __B,
            (__v4di) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_ternarylogic_epi32 (__m256i __A, __m256i __B, __m256i __C,
      const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogd256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si) __C, __imm,
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_ternarylogic_epi32 (__m256i __A, __mmask8 __U,
    __m256i __B, __m256i __C,
    const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogd256_mask ((__v8si) __A,
           (__v8si) __B,
           (__v8si) __C, __imm,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_ternarylogic_epi32 (__mmask8 __U, __m256i __A,
     __m256i __B, __m256i __C,
     const int __imm)
{
  return (__m256i) __builtin_ia32_pternlogd256_maskz ((__v8si) __A,
            (__v8si) __B,
            (__v8si) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_ternarylogic_epi64 (__m128i __A, __m128i __B, __m128i __C,
   const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogq128_mask ((__v2di) __A,
           (__v2di) __B,
           (__v2di) __C, __imm,
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_ternarylogic_epi64 (__m128i __A, __mmask8 __U,
        __m128i __B, __m128i __C, const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogq128_mask ((__v2di) __A,
           (__v2di) __B,
           (__v2di) __C, __imm,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_ternarylogic_epi64 (__mmask8 __U, __m128i __A,
         __m128i __B, __m128i __C, const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogq128_maskz ((__v2di) __A,
            (__v2di) __B,
            (__v2di) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_ternarylogic_epi32 (__m128i __A, __m128i __B, __m128i __C,
   const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogd128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si) __C, __imm,
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_ternarylogic_epi32 (__m128i __A, __mmask8 __U,
        __m128i __B, __m128i __C, const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogd128_mask ((__v4si) __A,
           (__v4si) __B,
           (__v4si) __C, __imm,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_ternarylogic_epi32 (__mmask8 __U, __m128i __A,
         __m128i __B, __m128i __C, const int __imm)
{
  return (__m128i) __builtin_ia32_pternlogd128_maskz ((__v4si) __A,
            (__v4si) __B,
            (__v4si) __C,
            __imm,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_roundscale_ps (__m256 __A, const int __imm)
{
  return (__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf) __A,
            __imm,
            (__v8sf)
            _mm256_setzero_ps (),
            (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_roundscale_ps (__m256 __W, __mmask8 __U, __m256 __A,
      const int __imm)
{
  return (__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf) __A,
            __imm,
            (__v8sf) __W,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_roundscale_ps (__mmask8 __U, __m256 __A, const int __imm)
{
  return (__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf) __A,
            __imm,
            (__v8sf)
            _mm256_setzero_ps (),
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_roundscale_pd (__m256d __A, const int __imm)
{
  return (__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df) __A,
             __imm,
             (__v4df)
             _mm256_setzero_pd (),
             (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_roundscale_pd (__m256d __W, __mmask8 __U, __m256d __A,
      const int __imm)
{
  return (__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df) __A,
             __imm,
             (__v4df) __W,
             (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_roundscale_pd (__mmask8 __U, __m256d __A, const int __imm)
{
  return (__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df) __A,
             __imm,
             (__v4df)
             _mm256_setzero_pd (),
             (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_ps (__m128 __A, const int __imm)
{
  return (__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf) __A,
            __imm,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_ps (__m128 __W, __mmask8 __U, __m128 __A,
   const int __imm)
{
  return (__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf) __A,
            __imm,
            (__v4sf) __W,
            (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_ps (__mmask8 __U, __m128 __A, const int __imm)
{
  return (__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf) __A,
            __imm,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_roundscale_pd (__m128d __A, const int __imm)
{
  return (__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df) __A,
             __imm,
             (__v2df)
             _mm_setzero_pd (),
             (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_roundscale_pd (__m128d __W, __mmask8 __U, __m128d __A,
   const int __imm)
{
  return (__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df) __A,
             __imm,
             (__v2df) __W,
             (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_roundscale_pd (__mmask8 __U, __m128d __A, const int __imm)
{
  return (__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df) __A,
             __imm,
             (__v2df)
             _mm_setzero_pd (),
             (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_getmant_ps (__m256 __A, _MM_MANTISSA_NORM_ENUM __B,
     _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256) __builtin_ia32_getmantps256_mask ((__v8sf) __A,
          (__C << 2) | __B,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_getmant_ps (__m256 __W, __mmask8 __U, __m256 __A,
   _MM_MANTISSA_NORM_ENUM __B,
   _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256) __builtin_ia32_getmantps256_mask ((__v8sf) __A,
          (__C << 2) | __B,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_getmant_ps (__mmask8 __U, __m256 __A,
    _MM_MANTISSA_NORM_ENUM __B,
    _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256) __builtin_ia32_getmantps256_mask ((__v8sf) __A,
          (__C << 2) | __B,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_ps (__m128 __A, _MM_MANTISSA_NORM_ENUM __B,
  _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128) __builtin_ia32_getmantps128_mask ((__v4sf) __A,
          (__C << 2) | __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_ps (__m128 __W, __mmask8 __U, __m128 __A,
       _MM_MANTISSA_NORM_ENUM __B,
       _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128) __builtin_ia32_getmantps128_mask ((__v4sf) __A,
          (__C << 2) | __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_ps (__mmask8 __U, __m128 __A,
        _MM_MANTISSA_NORM_ENUM __B,
        _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128) __builtin_ia32_getmantps128_mask ((__v4sf) __A,
          (__C << 2) | __B,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_getmant_pd (__m256d __A, _MM_MANTISSA_NORM_ENUM __B,
     _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256d) __builtin_ia32_getmantpd256_mask ((__v4df) __A,
           (__C << 2) | __B,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_getmant_pd (__m256d __W, __mmask8 __U, __m256d __A,
   _MM_MANTISSA_NORM_ENUM __B,
   _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256d) __builtin_ia32_getmantpd256_mask ((__v4df) __A,
           (__C << 2) | __B,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_getmant_pd (__mmask8 __U, __m256d __A,
    _MM_MANTISSA_NORM_ENUM __B,
    _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m256d) __builtin_ia32_getmantpd256_mask ((__v4df) __A,
           (__C << 2) | __B,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_getmant_pd (__m128d __A, _MM_MANTISSA_NORM_ENUM __B,
  _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128d) __builtin_ia32_getmantpd128_mask ((__v2df) __A,
           (__C << 2) | __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_getmant_pd (__m128d __W, __mmask8 __U, __m128d __A,
       _MM_MANTISSA_NORM_ENUM __B,
       _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128d) __builtin_ia32_getmantpd128_mask ((__v2df) __A,
           (__C << 2) | __B,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_getmant_pd (__mmask8 __U, __m128d __A,
        _MM_MANTISSA_NORM_ENUM __B,
        _MM_MANTISSA_SIGN_ENUM __C)
{
  return (__m128d) __builtin_ia32_getmantpd128_mask ((__v2df) __A,
           (__C << 2) | __B,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i32gather_ps (__m256 __v1_old, __mmask8 __mask,
      __m256i __index, void const *__addr,
      int __scale)
{
  return (__m256) __builtin_ia32_gather3siv8sf ((__v8sf) __v1_old,
      __addr,
      (__v8si) __index,
      __mask, __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i32gather_ps (__m128 __v1_old, __mmask8 __mask,
   __m128i __index, void const *__addr,
   int __scale)
{
  return (__m128) __builtin_ia32_gather3siv4sf ((__v4sf) __v1_old,
      __addr,
      (__v4si) __index,
      __mask, __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i32gather_pd (__m256d __v1_old, __mmask8 __mask,
      __m128i __index, void const *__addr,
      int __scale)
{
  return (__m256d) __builtin_ia32_gather3siv4df ((__v4df) __v1_old,
       __addr,
       (__v4si) __index,
       __mask, __scale);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i32gather_pd (__m128d __v1_old, __mmask8 __mask,
   __m128i __index, void const *__addr,
   int __scale)
{
  return (__m128d) __builtin_ia32_gather3siv2df ((__v2df) __v1_old,
       __addr,
       (__v4si) __index,
       __mask, __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i64gather_ps (__m128 __v1_old, __mmask8 __mask,
      __m256i __index, void const *__addr,
      int __scale)
{
  return (__m128) __builtin_ia32_gather3div8sf ((__v4sf) __v1_old,
      __addr,
      (__v4di) __index,
      __mask, __scale);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i64gather_ps (__m128 __v1_old, __mmask8 __mask,
   __m128i __index, void const *__addr,
   int __scale)
{
  return (__m128) __builtin_ia32_gather3div4sf ((__v4sf) __v1_old,
      __addr,
      (__v2di) __index,
      __mask, __scale);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i64gather_pd (__m256d __v1_old, __mmask8 __mask,
      __m256i __index, void const *__addr,
      int __scale)
{
  return (__m256d) __builtin_ia32_gather3div4df ((__v4df) __v1_old,
       __addr,
       (__v4di) __index,
       __mask, __scale);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i64gather_pd (__m128d __v1_old, __mmask8 __mask,
   __m128i __index, void const *__addr,
   int __scale)
{
  return (__m128d) __builtin_ia32_gather3div2df ((__v2df) __v1_old,
       __addr,
       (__v2di) __index,
       __mask, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i32gather_epi32 (__m256i __v1_old, __mmask8 __mask,
         __m256i __index, void const *__addr,
         int __scale)
{
  return (__m256i) __builtin_ia32_gather3siv8si ((__v8si) __v1_old,
       __addr,
       (__v8si) __index,
       __mask, __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i32gather_epi32 (__m128i __v1_old, __mmask8 __mask,
      __m128i __index, void const *__addr,
      int __scale)
{
  return (__m128i) __builtin_ia32_gather3siv4si ((__v4si) __v1_old,
       __addr,
       (__v4si) __index,
       __mask, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i32gather_epi64 (__m256i __v1_old, __mmask8 __mask,
         __m128i __index, void const *__addr,
         int __scale)
{
  return (__m256i) __builtin_ia32_gather3siv4di ((__v4di) __v1_old,
       __addr,
       (__v4si) __index,
       __mask, __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i32gather_epi64 (__m128i __v1_old, __mmask8 __mask,
      __m128i __index, void const *__addr,
      int __scale)
{
  return (__m128i) __builtin_ia32_gather3siv2di ((__v2di) __v1_old,
       __addr,
       (__v4si) __index,
       __mask, __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i64gather_epi32 (__m128i __v1_old, __mmask8 __mask,
         __m256i __index, void const *__addr,
         int __scale)
{
  return (__m128i) __builtin_ia32_gather3div8si ((__v4si) __v1_old,
       __addr,
       (__v4di) __index,
       __mask, __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i64gather_epi32 (__m128i __v1_old, __mmask8 __mask,
      __m128i __index, void const *__addr,
      int __scale)
{
  return (__m128i) __builtin_ia32_gather3div4si ((__v4si) __v1_old,
       __addr,
       (__v2di) __index,
       __mask, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mmask_i64gather_epi64 (__m256i __v1_old, __mmask8 __mask,
         __m256i __index, void const *__addr,
         int __scale)
{
  return (__m256i) __builtin_ia32_gather3div4di ((__v4di) __v1_old,
       __addr,
       (__v4di) __index,
       __mask, __scale);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mmask_i64gather_epi64 (__m128i __v1_old, __mmask8 __mask,
      __m128i __index, void const *__addr,
      int __scale)
{
  return (__m128i) __builtin_ia32_gather3div2di ((__v2di) __v1_old,
       __addr,
       (__v2di) __index,
       __mask, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32scatter_ps (void *__addr, __m256i __index,
        __m256 __v1, const int __scale)
{
  __builtin_ia32_scattersiv8sf (__addr, (__mmask8) 0xFF,
    (__v8si) __index, (__v8sf) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32scatter_ps (void *__addr, __mmask8 __mask,
      __m256i __index, __m256 __v1,
      const int __scale)
{
  __builtin_ia32_scattersiv8sf (__addr, __mask, (__v8si) __index,
    (__v8sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32scatter_ps (void *__addr, __m128i __index, __m128 __v1,
     const int __scale)
{
  __builtin_ia32_scattersiv4sf (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v4sf) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32scatter_ps (void *__addr, __mmask8 __mask,
   __m128i __index, __m128 __v1,
   const int __scale)
{
  __builtin_ia32_scattersiv4sf (__addr, __mask, (__v4si) __index,
    (__v4sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32scatter_pd (void *__addr, __m128i __index,
        __m256d __v1, const int __scale)
{
  __builtin_ia32_scattersiv4df (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v4df) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32scatter_pd (void *__addr, __mmask8 __mask,
      __m128i __index, __m256d __v1,
      const int __scale)
{
  __builtin_ia32_scattersiv4df (__addr, __mask, (__v4si) __index,
    (__v4df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32scatter_pd (void *__addr, __m128i __index,
     __m128d __v1, const int __scale)
{
  __builtin_ia32_scattersiv2df (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v2df) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32scatter_pd (void *__addr, __mmask8 __mask,
   __m128i __index, __m128d __v1,
   const int __scale)
{
  __builtin_ia32_scattersiv2df (__addr, __mask, (__v4si) __index,
    (__v2df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64scatter_ps (void *__addr, __m256i __index,
        __m128 __v1, const int __scale)
{
  __builtin_ia32_scatterdiv8sf (__addr, (__mmask8) 0xFF,
    (__v4di) __index, (__v4sf) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64scatter_ps (void *__addr, __mmask8 __mask,
      __m256i __index, __m128 __v1,
      const int __scale)
{
  __builtin_ia32_scatterdiv8sf (__addr, __mask, (__v4di) __index,
    (__v4sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64scatter_ps (void *__addr, __m128i __index, __m128 __v1,
     const int __scale)
{
  __builtin_ia32_scatterdiv4sf (__addr, (__mmask8) 0xFF,
    (__v2di) __index, (__v4sf) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64scatter_ps (void *__addr, __mmask8 __mask,
   __m128i __index, __m128 __v1,
   const int __scale)
{
  __builtin_ia32_scatterdiv4sf (__addr, __mask, (__v2di) __index,
    (__v4sf) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64scatter_pd (void *__addr, __m256i __index,
        __m256d __v1, const int __scale)
{
  __builtin_ia32_scatterdiv4df (__addr, (__mmask8) 0xFF,
    (__v4di) __index, (__v4df) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64scatter_pd (void *__addr, __mmask8 __mask,
      __m256i __index, __m256d __v1,
      const int __scale)
{
  __builtin_ia32_scatterdiv4df (__addr, __mask, (__v4di) __index,
    (__v4df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64scatter_pd (void *__addr, __m128i __index,
     __m128d __v1, const int __scale)
{
  __builtin_ia32_scatterdiv2df (__addr, (__mmask8) 0xFF,
    (__v2di) __index, (__v2df) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64scatter_pd (void *__addr, __mmask8 __mask,
   __m128i __index, __m128d __v1,
   const int __scale)
{
  __builtin_ia32_scatterdiv2df (__addr, __mask, (__v2di) __index,
    (__v2df) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32scatter_epi32 (void *__addr, __m256i __index,
    __m256i __v1, const int __scale)
{
  __builtin_ia32_scattersiv8si (__addr, (__mmask8) 0xFF,
    (__v8si) __index, (__v8si) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32scatter_epi32 (void *__addr, __mmask8 __mask,
         __m256i __index, __m256i __v1,
         const int __scale)
{
  __builtin_ia32_scattersiv8si (__addr, __mask, (__v8si) __index,
    (__v8si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32scatter_epi32 (void *__addr, __m128i __index,
        __m128i __v1, const int __scale)
{
  __builtin_ia32_scattersiv4si (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v4si) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32scatter_epi32 (void *__addr, __mmask8 __mask,
      __m128i __index, __m128i __v1,
      const int __scale)
{
  __builtin_ia32_scattersiv4si (__addr, __mask, (__v4si) __index,
    (__v4si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i32scatter_epi64 (void *__addr, __m128i __index,
    __m256i __v1, const int __scale)
{
  __builtin_ia32_scattersiv4di (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v4di) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i32scatter_epi64 (void *__addr, __mmask8 __mask,
         __m128i __index, __m256i __v1,
         const int __scale)
{
  __builtin_ia32_scattersiv4di (__addr, __mask, (__v4si) __index,
    (__v4di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i32scatter_epi64 (void *__addr, __m128i __index,
        __m128i __v1, const int __scale)
{
  __builtin_ia32_scattersiv2di (__addr, (__mmask8) 0xFF,
    (__v4si) __index, (__v2di) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i32scatter_epi64 (void *__addr, __mmask8 __mask,
      __m128i __index, __m128i __v1,
      const int __scale)
{
  __builtin_ia32_scattersiv2di (__addr, __mask, (__v4si) __index,
    (__v2di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64scatter_epi32 (void *__addr, __m256i __index,
    __m128i __v1, const int __scale)
{
  __builtin_ia32_scatterdiv8si (__addr, (__mmask8) 0xFF,
    (__v4di) __index, (__v4si) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64scatter_epi32 (void *__addr, __mmask8 __mask,
         __m256i __index, __m128i __v1,
         const int __scale)
{
  __builtin_ia32_scatterdiv8si (__addr, __mask, (__v4di) __index,
    (__v4si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64scatter_epi32 (void *__addr, __m128i __index,
        __m128i __v1, const int __scale)
{
  __builtin_ia32_scatterdiv4si (__addr, (__mmask8) 0xFF,
    (__v2di) __index, (__v4si) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64scatter_epi32 (void *__addr, __mmask8 __mask,
      __m128i __index, __m128i __v1,
      const int __scale)
{
  __builtin_ia32_scatterdiv4si (__addr, __mask, (__v2di) __index,
    (__v4si) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_i64scatter_epi64 (void *__addr, __m256i __index,
    __m256i __v1, const int __scale)
{
  __builtin_ia32_scatterdiv4di (__addr, (__mmask8) 0xFF,
    (__v4di) __index, (__v4di) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_i64scatter_epi64 (void *__addr, __mmask8 __mask,
         __m256i __index, __m256i __v1,
         const int __scale)
{
  __builtin_ia32_scatterdiv4di (__addr, __mask, (__v4di) __index,
    (__v4di) __v1, __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_i64scatter_epi64 (void *__addr, __m128i __index,
        __m128i __v1, const int __scale)
{
  __builtin_ia32_scatterdiv2di (__addr, (__mmask8) 0xFF,
    (__v2di) __index, (__v2di) __v1,
    __scale);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_i64scatter_epi64 (void *__addr, __mmask8 __mask,
      __m128i __index, __m128i __v1,
      const int __scale)
{
  __builtin_ia32_scatterdiv2di (__addr, __mask, (__v2di) __index,
    (__v2di) __v1, __scale);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
      _MM_PERM_ENUM __mask)
{
  return (__m256i) __builtin_ia32_pshufd256_mask ((__v8si) __A, __mask,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_epi32 (__mmask8 __U, __m256i __A,
       _MM_PERM_ENUM __mask)
{
  return (__m256i) __builtin_ia32_pshufd256_mask ((__v8si) __A, __mask,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shuffle_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
   _MM_PERM_ENUM __mask)
{
  return (__m128i) __builtin_ia32_pshufd128_mask ((__v4si) __A, __mask,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shuffle_epi32 (__mmask8 __U, __m128i __A,
    _MM_PERM_ENUM __mask)
{
  return (__m128i) __builtin_ia32_pshufd128_mask ((__v4si) __A, __mask,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rol_epi32 (__m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prold256_mask ((__v8si) __A, __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rol_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         const int __B)
{
  return (__m256i) __builtin_ia32_prold256_mask ((__v8si) __A, __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rol_epi32 (__mmask8 __U, __m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prold256_mask ((__v8si) __A, __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rol_epi32 (__m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prold128_mask ((__v4si) __A, __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rol_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      const int __B)
{
  return (__m128i) __builtin_ia32_prold128_mask ((__v4si) __A, __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rol_epi32 (__mmask8 __U, __m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prold128_mask ((__v4si) __A, __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_ror_epi32 (__m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prord256_mask ((__v8si) __A, __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_ror_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
         const int __B)
{
  return (__m256i) __builtin_ia32_prord256_mask ((__v8si) __A, __B,
       (__v8si) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_ror_epi32 (__mmask8 __U, __m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prord256_mask ((__v8si) __A, __B,
       (__v8si)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_ror_epi32 (__m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prord128_mask ((__v4si) __A, __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_ror_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
      const int __B)
{
  return (__m128i) __builtin_ia32_prord128_mask ((__v4si) __A, __B,
       (__v4si) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_ror_epi32 (__mmask8 __U, __m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prord128_mask ((__v4si) __A, __B,
       (__v4si)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_rol_epi64 (__m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prolq256_mask ((__v4di) __A, __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_rol_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         const int __B)
{
  return (__m256i) __builtin_ia32_prolq256_mask ((__v4di) __A, __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_rol_epi64 (__mmask8 __U, __m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prolq256_mask ((__v4di) __A, __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_rol_epi64 (__m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prolq128_mask ((__v2di) __A, __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_rol_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      const int __B)
{
  return (__m128i) __builtin_ia32_prolq128_mask ((__v2di) __A, __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_rol_epi64 (__mmask8 __U, __m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prolq128_mask ((__v2di) __A, __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_ror_epi64 (__m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prorq256_mask ((__v4di) __A, __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_ror_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
         const int __B)
{
  return (__m256i) __builtin_ia32_prorq256_mask ((__v4di) __A, __B,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_ror_epi64 (__mmask8 __U, __m256i __A, const int __B)
{
  return (__m256i) __builtin_ia32_prorq256_mask ((__v4di) __A, __B,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_ror_epi64 (__m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prorq128_mask ((__v2di) __A, __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_ror_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
      const int __B)
{
  return (__m128i) __builtin_ia32_prorq128_mask ((__v2di) __A, __B,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_ror_epi64 (__mmask8 __U, __m128i __A, const int __B)
{
  return (__m128i) __builtin_ia32_prorq128_mask ((__v2di) __A, __B,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_alignr_epi32 (__m128i __A, __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_alignd128_mask ((__v4si) __A,
        (__v4si) __B, __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_alignr_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_alignd128_mask ((__v4si) __A,
        (__v4si) __B, __imm,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_alignr_epi32 (__mmask8 __U, __m128i __A, __m128i __B,
   const int __imm)
{
  return (__m128i) __builtin_ia32_alignd128_mask ((__v4si) __A,
        (__v4si) __B, __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_alignr_epi64 (__m128i __A, __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_alignq128_mask ((__v2di) __A,
        (__v2di) __B, __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_alignr_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_alignq128_mask ((__v2di) __A,
        (__v2di) __B, __imm,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_alignr_epi64 (__mmask8 __U, __m128i __A, __m128i __B,
   const int __imm)
{
  return (__m128i) __builtin_ia32_alignq128_mask ((__v2di) __A,
        (__v2di) __B, __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_alignr_epi32 (__m256i __A, __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_alignd256_mask ((__v8si) __A,
        (__v8si) __B, __imm,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_alignr_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
     __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_alignd256_mask ((__v8si) __A,
        (__v8si) __B, __imm,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_alignr_epi32 (__mmask8 __U, __m256i __A, __m256i __B,
      const int __imm)
{
  return (__m256i) __builtin_ia32_alignd256_mask ((__v8si) __A,
        (__v8si) __B, __imm,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_alignr_epi64 (__m256i __A, __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_alignq256_mask ((__v4di) __A,
        (__v4di) __B, __imm,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_alignr_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
     __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_alignq256_mask ((__v4di) __A,
        (__v4di) __B, __imm,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_alignr_epi64 (__mmask8 __U, __m256i __A, __m256i __B,
      const int __imm)
{
  return (__m256i) __builtin_ia32_alignq256_mask ((__v4di) __A,
        (__v4di) __B, __imm,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_ph (__m128i __W, __mmask8 __U, __m128 __A,
     const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf) __A, __I,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_ph (__mmask8 __U, __m128 __A, const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf) __A, __I,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_ph (__m128i __W, __mmask8 __U, __m256 __A,
        const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf) __A, __I,
           (__v8hi) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_ph (__mmask8 __U, __m256 __A, const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf) __A, __I,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srai_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psradi256_mask ((__v8si) __A, __imm,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srai_epi32 (__mmask8 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psradi256_mask ((__v8si) __A, __imm,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srai_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psradi128_mask ((__v4si) __A, __imm,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srai_epi32 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psradi128_mask ((__v4si) __A, __imm,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srai_epi64 (__m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psraqi256_mask ((__v4di) __A, __imm,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srai_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psraqi256_mask ((__v4di) __A, __imm,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srai_epi64 (__mmask8 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psraqi256_mask ((__v4di) __A, __imm,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srai_epi64 (__m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psraqi128_mask ((__v2di) __A, __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srai_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psraqi128_mask ((__v2di) __A, __imm,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srai_epi64 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psraqi128_mask ((__v2di) __A, __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_slli_epi32 (__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_pslldi128_mask ((__v4si) __A, __B,
        (__v4si) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_slli_epi32 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_pslldi128_mask ((__v4si) __A, __B,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_slli_epi64 (__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_psllqi128_mask ((__v2di) __A, __B,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_slli_epi64 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_psllqi128_mask ((__v2di) __A, __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_slli_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
   int __B)
{
  return (__m256i) __builtin_ia32_pslldi256_mask ((__v8si) __A, __B,
        (__v8si) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_slli_epi32 (__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i) __builtin_ia32_pslldi256_mask ((__v8si) __A, __B,
        (__v8si)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_slli_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
   int __B)
{
  return (__m256i) __builtin_ia32_psllqi256_mask ((__v4di) __A, __B,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_slli_epi64 (__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i) __builtin_ia32_psllqi256_mask ((__v4di) __A, __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex_pd (__m256d __W, __mmask8 __U, __m256d __X,
    const int __imm)
{
  return (__m256d) __builtin_ia32_permdf256_mask ((__v4df) __X, __imm,
        (__v4df) __W,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex_pd (__mmask8 __U, __m256d __X, const int __imm)
{
  return (__m256d) __builtin_ia32_permdf256_mask ((__v4df) __X, __imm,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permute_pd (__m256d __W, __mmask8 __U, __m256d __X,
   const int __C)
{
  return (__m256d) __builtin_ia32_vpermilpd256_mask ((__v4df) __X, __C,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permute_pd (__mmask8 __U, __m256d __X, const int __C)
{
  return (__m256d) __builtin_ia32_vpermilpd256_mask ((__v4df) __X, __C,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permute_pd (__m128d __W, __mmask8 __U, __m128d __X,
       const int __C)
{
  return (__m128d) __builtin_ia32_vpermilpd_mask ((__v2df) __X, __C,
        (__v2df) __W,
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permute_pd (__mmask8 __U, __m128d __X, const int __C)
{
  return (__m128d) __builtin_ia32_vpermilpd_mask ((__v2df) __X, __C,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permute_ps (__m256 __W, __mmask8 __U, __m256 __X,
   const int __C)
{
  return (__m256) __builtin_ia32_vpermilps256_mask ((__v8sf) __X, __C,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permute_ps (__mmask8 __U, __m256 __X, const int __C)
{
  return (__m256) __builtin_ia32_vpermilps256_mask ((__v8sf) __X, __C,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permute_ps (__m128 __W, __mmask8 __U, __m128 __X,
       const int __C)
{
  return (__m128) __builtin_ia32_vpermilps_mask ((__v4sf) __X, __C,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permute_ps (__mmask8 __U, __m128 __X, const int __C)
{
  return (__m128) __builtin_ia32_vpermilps_mask ((__v4sf) __X, __C,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_pd (__mmask8 __U, __m256d __A, __m256d __W)
{
  return (__m256d) __builtin_ia32_blendmpd_256_mask ((__v4df) __A,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_ps (__mmask8 __U, __m256 __A, __m256 __W)
{
  return (__m256) __builtin_ia32_blendmps_256_mask ((__v8sf) __A,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_epi64 (__mmask8 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_blendmq_256_mask ((__v4di) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_epi32 (__mmask8 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_blendmd_256_mask ((__v8si) __A,
          (__v8si) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_pd (__mmask8 __U, __m128d __A, __m128d __W)
{
  return (__m128d) __builtin_ia32_blendmpd_128_mask ((__v2df) __A,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_ps (__mmask8 __U, __m128 __A, __m128 __W)
{
  return (__m128) __builtin_ia32_blendmps_128_mask ((__v4sf) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_epi64 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_blendmq_128_mask ((__v2di) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_epi32 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_blendmd_128_mask ((__v4si) __A,
          (__v4si) __W,
          (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epi64_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epi32_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epu64_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epu32_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_pd_mask (__m256d __X, __m256d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd256_mask ((__v4df) __X,
        (__v4df) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_ps_mask (__m256 __X, __m256 __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpps256_mask ((__v8sf) __X,
        (__v8sf) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epi64_mask (__mmask8 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq256_mask ((__v4di) __X,
       (__v4di) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epi32_mask (__mmask8 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_cmpd256_mask ((__v8si) __X,
       (__v8si) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epu64_mask (__mmask8 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq256_mask ((__v4di) __X,
        (__v4di) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epu32_mask (__mmask8 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpd256_mask ((__v8si) __X,
        (__v8si) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_pd_mask (__mmask8 __U, __m256d __X, __m256d __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd256_mask ((__v4df) __X,
        (__v4df) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_ps_mask (__mmask8 __U, __m256 __X, __m256 __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_cmpps256_mask ((__v8sf) __X,
        (__v8sf) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epi64_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epi32_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epu64_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epu32_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_pd_mask (__m128d __X, __m128d __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd128_mask ((__v2df) __X,
        (__v2df) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_ps_mask (__m128 __X, __m128 __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpps128_mask ((__v4sf) __X,
        (__v4sf) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epi64_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_cmpq128_mask ((__v2di) __X,
       (__v2di) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epi32_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_cmpd128_mask ((__v4si) __X,
       (__v4si) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epu64_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpq128_mask ((__v2di) __X,
        (__v2di) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epu32_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpd128_mask ((__v4si) __X,
        (__v4si) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_pd_mask (__mmask8 __U, __m128d __X, __m128d __Y,
        const int __P)
{
  return (__mmask8) __builtin_ia32_cmppd128_mask ((__v2df) __X,
        (__v2df) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_ps_mask (__mmask8 __U, __m128 __X, __m128 __Y,
        const int __P)
{
  return (__mmask8) __builtin_ia32_cmpps128_mask ((__v4sf) __X,
        (__v4sf) __Y, __P,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex_pd (__m256d __X, const int __M)
{
  return (__m256d) __builtin_ia32_permdf256_mask ((__v4df) __X, __M,
        (__v4df)
        _mm256_undefined_pd (),
        (__mmask8) -1);
}
# 13735 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlintrin.h" 3 4
#pragma GCC pop_options
# 64 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bwintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bwintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512bw")




typedef short __v32hi __attribute__ ((__vector_size__ (64)));
typedef char __v64qi __attribute__ ((__vector_size__ (64)));

typedef unsigned long long __mmask64;

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktest_mask32_u8 (__mmask32 __A, __mmask32 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_ktestcsi (__A, __B);
  return (unsigned char) __builtin_ia32_ktestzsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktest_mask64_u8 (__mmask64 __A, __mmask64 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_ktestcdi (__A, __B);
  return (unsigned char) __builtin_ia32_ktestzdi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestz_mask32_u8 (__mmask32 __A, __mmask32 __B)
{
  return (unsigned char) __builtin_ia32_ktestzsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestz_mask64_u8 (__mmask64 __A, __mmask64 __B)
{
  return (unsigned char) __builtin_ia32_ktestzdi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestc_mask32_u8 (__mmask32 __A, __mmask32 __B)
{
  return (unsigned char) __builtin_ia32_ktestcsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestc_mask64_u8 (__mmask64 __A, __mmask64 __B)
{
  return (unsigned char) __builtin_ia32_ktestcdi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortest_mask32_u8 (__mmask32 __A, __mmask32 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_kortestcsi (__A, __B);
  return (unsigned char) __builtin_ia32_kortestzsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortest_mask64_u8 (__mmask64 __A, __mmask64 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_kortestcdi (__A, __B);
  return (unsigned char) __builtin_ia32_kortestzdi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestz_mask32_u8 (__mmask32 __A, __mmask32 __B)
{
  return (unsigned char) __builtin_ia32_kortestzsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestz_mask64_u8 (__mmask64 __A, __mmask64 __B)
{
  return (unsigned char) __builtin_ia32_kortestzdi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestc_mask32_u8 (__mmask32 __A, __mmask32 __B)
{
  return (unsigned char) __builtin_ia32_kortestcsi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestc_mask64_u8 (__mmask64 __A, __mmask64 __B)
{
  return (unsigned char) __builtin_ia32_kortestcdi (__A, __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kadd_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kaddsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kadd_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kadddi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtmask32_u32 (__mmask32 __A)
{
  return (unsigned int) __builtin_ia32_kmovd ((__mmask32) __A);
}

extern __inline unsigned long long
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtmask64_u64 (__mmask64 __A)
{
  return (unsigned long long) __builtin_ia32_kmovq ((__mmask64) __A);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtu32_mask32 (unsigned int __A)
{
  return (__mmask32) __builtin_ia32_kmovd ((__mmask32) __A);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtu64_mask64 (unsigned long long __A)
{
  return (__mmask64) __builtin_ia32_kmovq ((__mmask64) __A);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_load_mask32 (__mmask32 *__A)
{
  return (__mmask32) __builtin_ia32_kmovd (*__A);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_load_mask64 (__mmask64 *__A)
{
  return (__mmask64) __builtin_ia32_kmovq (*(__mmask64 *) __A);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_store_mask32 (__mmask32 *__A, __mmask32 __B)
{
  *(__mmask32 *) __A = __builtin_ia32_kmovd (__B);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_store_mask64 (__mmask64 *__A, __mmask64 __B)
{
  *(__mmask64 *) __A = __builtin_ia32_kmovq (__B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_knot_mask32 (__mmask32 __A)
{
  return (__mmask32) __builtin_ia32_knotsi ((__mmask32) __A);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_knot_mask64 (__mmask64 __A)
{
  return (__mmask64) __builtin_ia32_knotdi ((__mmask64) __A);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kor_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_korsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kor_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kordi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxnor_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kxnorsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxnor_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kxnordi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxor_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kxorsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxor_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kxordi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kand_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kandsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kand_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kanddi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kandn_mask32 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kandnsi ((__mmask32) __A, (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kandn_mask64 (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kandndi ((__mmask64) __A, (__mmask64) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdquhi512_mask ((__v32hi) __A,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdquhi512_mask ((__v32hi) __A,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_epi16 (__m512i __W, __mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const short *) __P,
           (__v32hi) __W,
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_epi16 (__mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const short *) __P,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_epi16 (void *__P, __mmask32 __U, __m512i __A)
{
  __builtin_ia32_storedquhi512_mask ((short *) __P,
         (__v32hi) __A,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mov_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdquqi512_mask ((__v64qi) __A,
          (__v64qi) __W,
          (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mov_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_movdquqi512_mask ((__v64qi) __A,
          (__v64qi)
          _mm512_setzero_si512 (),
          (__mmask64) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kunpackw (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kunpcksi ((__mmask32) __A,
           (__mmask32) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kunpackw_mask32 (__mmask16 __A, __mmask16 __B)
{
  return (__mmask32) __builtin_ia32_kunpcksi ((__mmask32) __A,
           (__mmask32) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_kunpackd (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kunpckdi ((__mmask64) __A,
           (__mmask64) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kunpackd_mask64 (__mmask32 __A, __mmask32 __B)
{
  return (__mmask64) __builtin_ia32_kunpckdi ((__mmask64) __A,
           (__mmask64) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const char *) __P,
           (__v64qi) __W,
           (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const char *) __P,
           (__v64qi)
           _mm512_setzero_si512 (),
           (__mmask64) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_storeu_epi8 (void *__P, __mmask64 __U, __m512i __A)
{
  __builtin_ia32_storedquqi512_mask ((char *) __P,
         (__v64qi) __A,
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sad_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psadbw512 ((__v64qi) __A,
          (__v64qi) __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi16_epi8 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
        (__v32qi) _mm256_undefined_si256(),
        (__mmask32) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovwb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
        (__v32qi) __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi16_epi8 (__mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
        (__v32qi)
        _mm256_setzero_si256 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtsepi16_epi8 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
         (__v32qi)_mm256_undefined_si256(),
         (__mmask32) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtsepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
         (__v32qi)__O,
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtsepi16_epi8 (__mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
         (__v32qi)
         _mm256_setzero_si256 (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtusepi16_epi8 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
          (__v32qi)_mm256_undefined_si256(),
          (__mmask32) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
          (__v32qi) __O,
          __M);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovuswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtusepi16_epi8 (__mmask32 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
          (__v32qi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastb_epi8 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastb512_mask ((__v16qi) __A,
             (__v64qi)_mm512_undefined_epi32(),
             (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastb_epi8 (__m512i __O, __mmask64 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastb512_mask ((__v16qi) __A,
             (__v64qi) __O,
             __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastb_epi8 (__mmask64 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastb512_mask ((__v16qi) __A,
             (__v64qi)
             _mm512_setzero_si512 (),
             __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_set1_epi8 (__m512i __O, __mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_pbroadcastb512_gpr_mask (__A,
          (__v64qi) __O,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_set1_epi8 (__mmask64 __M, char __A)
{
  return (__m512i)
  __builtin_ia32_pbroadcastb512_gpr_mask (__A,
       (__v64qi)
       _mm512_setzero_si512 (),
       __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcastw_epi16 (__m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastw512_mask ((__v8hi) __A,
             (__v32hi)_mm512_undefined_epi32(),
             (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcastw_epi16 (__m512i __O, __mmask32 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastw512_mask ((__v8hi) __A,
             (__v32hi) __O,
             __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcastw_epi16 (__mmask32 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_pbroadcastw512_mask ((__v8hi) __A,
             (__v32hi)
             _mm512_setzero_si512 (),
             __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_set1_epi16 (__m512i __O, __mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_pbroadcastw512_gpr_mask (__A,
          (__v32hi) __O,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_set1_epi16 (__mmask32 __M, short __A)
{
  return (__m512i)
  __builtin_ia32_pbroadcastw512_gpr_mask (__A,
       (__v32hi)
       _mm512_setzero_si512 (),
       __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mulhrs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mulhrs_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
     __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mulhrs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mulhi_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mulhi_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mulhi_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mulhi_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mulhi_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi) __W,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mulhi_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mullo_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v32hu) __A * (__v32hu) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mullo_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_pmullw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mullo_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmullw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi8_epi16 (__m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbw512_mask ((__v32qi) __A,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi8_epi16 (__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbw512_mask ((__v32qi) __A,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi8_epi16 (__mmask32 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovsxbw512_mask ((__v32qi) __A,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu8_epi16 (__m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbw512_mask ((__v32qi) __A,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu8_epi16 (__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbw512_mask ((__v32qi) __A,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu8_epi16 (__mmask32 __U, __m256i __A)
{
  return (__m512i) __builtin_ia32_pmovzxbw512_mask ((__v32qi) __A,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
           (__v32hi) __A,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_epi16 (__mmask32 __M, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
           (__v32hi) __A,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
           (__v32hi) __A,
           (__v32hi) __W,
           (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_epi16 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_mask ((__v32hi) __I
                 ,
       (__v32hi) __A,
       (__v32hi) __B,
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_epi16 (__m512i __A, __mmask32 __U,
    __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_mask ((__v32hi) __I
                 ,
       (__v32hi) __A,
       (__v32hi) __B,
       (__mmask32)
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_epi16 (__m512i __A, __m512i __I,
     __mmask32 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varhi512_mask ((__v32hi) __A,
       (__v32hi) __I
                 ,
       (__v32hi) __B,
       (__mmask32)
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_epi16 (__mmask32 __U, __m512i __A,
     __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_maskz ((__v32hi) __I
                  ,
        (__v32hi) __A,
        (__v32hi) __B,
        (__mmask32)
        __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_avg_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_avg_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi) __W,
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_avg_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v64qu) __A + (__v64qu) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_paddb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi) __W,
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v64qu) __A - (__v64qu) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_psubb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi) __W,
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_avg_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_avg_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_avg_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pavgw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_subs_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_subs_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_subs_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_subs_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi)
         _mm512_setzero_si512 (),
         (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_subs_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi) __W,
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_subs_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi)
         _mm512_setzero_si512 (),
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_adds_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_adds_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_adds_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_adds_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi)
         _mm512_setzero_si512 (),
         (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_adds_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi) __W,
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_adds_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
         (__v64qi) __B,
         (__v64qi)
         _mm512_setzero_si512 (),
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sub_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v32hu) __A - (__v32hu) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sub_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_psubw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sub_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_subs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_subs_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_subs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_subs_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_subs_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi) __W,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_subs_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_add_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v32hu) __A + (__v32hu) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_add_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_paddw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_add_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddw512_mask ((__v32hi) __A,
       (__v32hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_adds_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_adds_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_adds_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_adds_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_adds_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi) __W,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_adds_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srl_epi16 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srl_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srl_epi16 (__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psrlw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_packs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packsswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi)
          _mm512_setzero_si512 (),
          (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sll_epi16 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psllw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sll_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m128i __B)
{
  return (__m512i) __builtin_ia32_psllw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sll_epi16 (__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psllw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maddubs_epi16 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
           (__v64qi) __Y,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_maddubs_epi16 (__m512i __W, __mmask32 __U, __m512i __X,
      __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
           (__v64qi) __Y,
           (__v32hi) __W,
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_maddubs_epi16 (__mmask32 __U, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
           (__v64qi) __Y,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_madd_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v16si)
         _mm512_setzero_si512 (),
         (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_madd_epi16 (__m512i __W, __mmask16 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v16si) __W,
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_madd_epi16 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
         (__v32hi) __B,
         (__v16si)
         _mm512_setzero_si512 (),
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi)
           _mm512_setzero_si512 (),
           (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi) __W,
           (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi)
           _mm512_setzero_si512 (),
           (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpackhi_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpackhi_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi) __W,
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpackhi_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpckhwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi)
           _mm512_setzero_si512 (),
           (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi) __W,
           (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklbw512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__v64qi)
           _mm512_setzero_si512 (),
           (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_unpacklo_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_unpacklo_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi) __W,
           (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_unpacklo_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_punpcklwd512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__v32hi)
           _mm512_setzero_si512 (),
           (__mmask32) __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epu8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __A,
          (__v64qi) __B, 0,
          (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epi8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_pcmpeqb512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epu8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __A,
          (__v64qi) __B, 0,
          __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_pcmpeqb512_mask ((__v64qi) __A,
           (__v64qi) __B,
           __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epu16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __A,
          (__v32hi) __B, 0,
          (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpeq_epi16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_pcmpeqw512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epu16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __A,
          (__v32hi) __B, 0,
          __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpeq_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_pcmpeqw512_mask ((__v32hi) __A,
           (__v32hi) __B,
           __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epu8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __A,
          (__v64qi) __B, 6,
          (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epi8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_pcmpgtb512_mask ((__v64qi) __A,
           (__v64qi) __B,
           (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epu8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __A,
          (__v64qi) __B, 6,
          __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_pcmpgtb512_mask ((__v64qi) __A,
           (__v64qi) __B,
           __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epu16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __A,
          (__v32hi) __B, 6,
          (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpgt_epi16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_pcmpgtw512_mask ((__v32hi) __A,
           (__v32hi) __B,
           (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epu16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __A,
          (__v32hi) __B, 6,
          __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpgt_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_pcmpgtw512_mask ((__v32hi) __A,
           (__v32hi) __B,
           __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movepi8_mask (__m512i __A)
{
  return (__mmask64) __builtin_ia32_cvtb2mask512 ((__v64qi) __A);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movepi16_mask (__m512i __A)
{
  return (__mmask32) __builtin_ia32_cvtw2mask512 ((__v32hi) __A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movm_epi8 (__mmask64 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2b512 (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movm_epi16 (__mmask32 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2w512 (__A);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_test_epi8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ptestmb512 ((__v64qi) __A,
      (__v64qi) __B,
      (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_test_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ptestmb512 ((__v64qi) __A,
      (__v64qi) __B, __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_test_epi16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ptestmw512 ((__v32hi) __A,
      (__v32hi) __B,
      (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_test_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ptestmw512 ((__v32hi) __A,
      (__v32hi) __B, __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_testn_epi8_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ptestnmb512 ((__v64qi) __A,
       (__v64qi) __B,
       (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_testn_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_ptestnmb512 ((__v64qi) __A,
       (__v64qi) __B, __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_testn_epi16_mask (__m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ptestnmw512 ((__v32hi) __A,
       (__v32hi) __B,
       (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_testn_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__mmask32) __builtin_ia32_ptestnmw512 ((__v32hi) __A,
       (__v32hi) __B, __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shuffle_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pshufb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shuffle_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
     __m512i __B)
{
  return (__m512i) __builtin_ia32_pshufb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shuffle_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pshufb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epu16 (__m512i __W, __mmask32 __M, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epu8 (__m512i __W, __mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epu8 (__m512i __W, __mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_min_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_min_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi)
        _mm512_setzero_si512 (),
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_min_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
        (__v64qi) __B,
        (__v64qi) __W,
        (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_max_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_max_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_max_epu16 (__m512i __W, __mmask32 __M, __m512i __A,
         __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sra_epi16 (__m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psraw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sra_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
         __m128i __B)
{
  return (__m512i) __builtin_ia32_psraw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sra_epi16 (__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i) __builtin_ia32_psraw512_mask ((__v32hi) __A,
       (__v8hi) __B,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srav_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psrav32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srav_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_psrav32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srav_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psrav32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srlv_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psrlv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srlv_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_psrlv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srlv_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psrlv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_sllv_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psllv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_sllv_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B)
{
  return (__m512i) __builtin_ia32_psllv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_sllv_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psllv32hi_mask ((__v32hi) __A,
        (__v32hi) __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_packs_epi16 (__m512i __W, __mmask64 __M, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_packsswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi) __W,
          (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_packs_epi16 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packsswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_packus_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packuswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi)
          _mm512_setzero_si512 (),
          (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_packus_epi16 (__m512i __W, __mmask64 __M, __m512i __A,
     __m512i __B)
{
  return (__m512i) __builtin_ia32_packuswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi) __W,
          (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_packus_epi16 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packuswb512_mask ((__v32hi) __A,
          (__v32hi) __B,
          (__v64qi)
          _mm512_setzero_si512 (),
          (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_epi8 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
       (__v64qi) __W,
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_abs_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
       (__v64qi)
       _mm512_setzero_si512 (),
       (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_abs_epi16 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_abs_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_abs_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
       (__v32hi)
       _mm512_setzero_si512 (),
       (__mmask32) __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epu8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 4,
         (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epu8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 1,
         (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epu8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 5,
         (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epu8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 2,
         (__mmask64) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epu16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 4,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epu16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 1,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epu16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 5,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epu16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 2,
         (__mmask32) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epi8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 4,
        (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epi8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 1,
        (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epi8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 5,
        (__mmask64) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epi8_mask (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 2,
        (__mmask64) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpneq_epi16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 4,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmplt_epi16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 1,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmpge_epi16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 5,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmple_epi16_mask (__mmask32 __M, __m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 2,
        (__mmask32) __M);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epu8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 4,
         (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epu8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 1,
         (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epu8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 5,
         (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epu8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, 2,
         (__mmask64) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epu16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 4,
         (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epu16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 1,
         (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epu16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 5,
         (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epu16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, 2,
         (__mmask32) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epi8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 4,
        (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epi8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 1,
        (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epi8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 5,
        (__mmask64) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epi8_mask (__m512i __X, __m512i __Y)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, 2,
        (__mmask64) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpneq_epi16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 4,
        (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmplt_epi16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 1,
        (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmpge_epi16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 5,
        (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmple_epi16_mask (__m512i __X, __m512i __Y)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, 2,
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_packs_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packssdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_packs_epi32 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packssdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_packs_epi32 (__m512i __W, __mmask32 __M, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_packssdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi) __W,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_packus_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packusdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_packus_epi32 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packusdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_packus_epi32 (__m512i __W, __mmask32 __M, __m512i __A,
     __m512i __B)
{
  return (__m512i) __builtin_ia32_packusdw512_mask ((__v16si) __A,
          (__v16si) __B,
          (__v32hi) __W,
          __M);
}


extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftli_mask32 (__mmask32 __A, unsigned int __B)
{
  return (__mmask32) __builtin_ia32_kshiftlisi ((__mmask32) __A,
      (__mmask8) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftli_mask64 (__mmask64 __A, unsigned int __B)
{
  return (__mmask64) __builtin_ia32_kshiftlidi ((__mmask64) __A,
      (__mmask8) __B);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftri_mask32 (__mmask32 __A, unsigned int __B)
{
  return (__mmask32) __builtin_ia32_kshiftrisi ((__mmask32) __A,
      (__mmask8) __B);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftri_mask64 (__mmask64 __A, unsigned int __B)
{
  return (__mmask64) __builtin_ia32_kshiftridi ((__mmask64) __A,
      (__mmask8) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_alignr_epi8 (__m512i __A, __m512i __B, const int __N)
{
  return (__m512i) __builtin_ia32_palignr512 ((__v8di) __A,
           (__v8di) __B, __N * 8);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_alignr_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
    __m512i __B, const int __N)
{
  return (__m512i) __builtin_ia32_palignr512_mask ((__v8di) __A,
         (__v8di) __B,
         __N * 8,
         (__v8di) __W,
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_alignr_epi8 (__mmask64 __U, __m512i __A, __m512i __B,
     const int __N)
{
  return (__m512i) __builtin_ia32_palignr512_mask ((__v8di) __A,
         (__v8di) __B,
         __N * 8,
         (__v8di)
         _mm512_setzero_si512 (),
         (__mmask64) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dbsad_epu8 (__m512i __A, __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_dbpsadbw512_mask ((__v64qi) __A,
          (__v64qi) __B,
          __imm,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dbsad_epu8 (__m512i __W, __mmask32 __U, __m512i __A,
   __m512i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_dbpsadbw512_mask ((__v64qi) __A,
          (__v64qi) __B,
          __imm,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dbsad_epu8 (__mmask32 __U, __m512i __A, __m512i __B,
    const int __imm)
{
  return (__m512i) __builtin_ia32_dbpsadbw512_mask ((__v64qi) __A,
          (__v64qi) __B,
          __imm,
          (__v32hi)
          _mm512_setzero_si512 (),
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srli_epi16 (__m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_psrlwi512_mask ((__v32hi) __A, __imm,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srli_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   const int __imm)
{
  return (__m512i) __builtin_ia32_psrlwi512_mask ((__v32hi) __A, __imm,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srli_epi16 (__mmask32 __U, __m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_psrlwi512_mask ((__v32hi) __A, __imm,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_slli_epi16 (__m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_psllwi512_mask ((__v32hi) __A, __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_slli_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   const int __B)
{
  return (__m512i) __builtin_ia32_psllwi512_mask ((__v32hi) __A, __B,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_slli_epi16 (__mmask32 __U, __m512i __A, const int __B)
{
  return (__m512i) __builtin_ia32_psllwi512_mask ((__v32hi) __A, __B,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shufflehi_epi16 (__m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_pshufhw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shufflehi_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
        const int __imm)
{
  return (__m512i) __builtin_ia32_pshufhw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi) __W,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shufflehi_epi16 (__mmask32 __U, __m512i __A,
         const int __imm)
{
  return (__m512i) __builtin_ia32_pshufhw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shufflelo_epi16 (__m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_pshuflw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shufflelo_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
        const int __imm)
{
  return (__m512i) __builtin_ia32_pshuflw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi) __W,
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shufflelo_epi16 (__mmask32 __U, __m512i __A,
         const int __imm)
{
  return (__m512i) __builtin_ia32_pshuflw512_mask ((__v32hi) __A,
         __imm,
         (__v32hi)
         _mm512_setzero_si512 (),
         (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_srai_epi16 (__m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_psrawi512_mask ((__v32hi) __A, __imm,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_srai_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
   const int __imm)
{
  return (__m512i) __builtin_ia32_psrawi512_mask ((__v32hi) __A, __imm,
        (__v32hi) __W,
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_srai_epi16 (__mmask32 __U, __m512i __A, const int __imm)
{
  return (__m512i) __builtin_ia32_psrawi512_mask ((__v32hi) __A, __imm,
        (__v32hi)
        _mm512_setzero_si512 (),
        (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_epi16 (__mmask32 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_blendmw_512_mask ((__v32hi) __A,
          (__v32hi) __W,
          (__mmask32) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_blend_epi8 (__mmask64 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_blendmb_512_mask ((__v64qi) __A,
          (__v64qi) __W,
          (__mmask64) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epi16_mask (__mmask32 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, __P,
        (__mmask32) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epi16_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask32) __builtin_ia32_cmpw512_mask ((__v32hi) __X,
        (__v32hi) __Y, __P,
        (__mmask32) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epi8_mask (__mmask64 __U, __m512i __X, __m512i __Y,
      const int __P)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, __P,
        (__mmask64) __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epi8_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask64) __builtin_ia32_cmpb512_mask ((__v64qi) __X,
        (__v64qi) __Y, __P,
        (__mmask64) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epu16_mask (__mmask32 __U, __m512i __X, __m512i __Y,
       const int __P)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, __P,
         (__mmask32) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epu16_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask32) __builtin_ia32_ucmpw512_mask ((__v32hi) __X,
         (__v32hi) __Y, __P,
         (__mmask32) -1);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cmp_epu8_mask (__mmask64 __U, __m512i __X, __m512i __Y,
      const int __P)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, __P,
         (__mmask64) __U);
}

extern __inline __mmask64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cmp_epu8_mask (__m512i __X, __m512i __Y, const int __P)
{
  return (__mmask64) __builtin_ia32_ucmpb512_mask ((__v64qi) __X,
         (__v64qi) __Y, __P,
         (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_bslli_epi128 (__m512i __A, const int __N)
{
  return (__m512i) __builtin_ia32_pslldq512 (__A, __N * 8);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_bsrli_epi128 (__m512i __A, const int __N)
{
  return (__m512i) __builtin_ia32_psrldq512 (__A, __N * 8);
}
# 3298 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bwintrin.h" 3 4
#pragma GCC pop_options
# 66 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512dqintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512dqintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512dq")



extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktest_mask8_u8 (__mmask8 __A, __mmask8 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_ktestcqi (__A, __B);
  return (unsigned char) __builtin_ia32_ktestzqi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestz_mask8_u8 (__mmask8 __A, __mmask8 __B)
{
  return (unsigned char) __builtin_ia32_ktestzqi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestc_mask8_u8 (__mmask8 __A, __mmask8 __B)
{
  return (unsigned char) __builtin_ia32_ktestcqi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktest_mask16_u8 (__mmask16 __A, __mmask16 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_ktestchi (__A, __B);
  return (unsigned char) __builtin_ia32_ktestzhi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestz_mask16_u8 (__mmask16 __A, __mmask16 __B)
{
  return (unsigned char) __builtin_ia32_ktestzhi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_ktestc_mask16_u8 (__mmask16 __A, __mmask16 __B)
{
  return (unsigned char) __builtin_ia32_ktestchi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortest_mask8_u8 (__mmask8 __A, __mmask8 __B, unsigned char *__CF)
{
  *__CF = (unsigned char) __builtin_ia32_kortestcqi (__A, __B);
  return (unsigned char) __builtin_ia32_kortestzqi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestz_mask8_u8 (__mmask8 __A, __mmask8 __B)
{
  return (unsigned char) __builtin_ia32_kortestzqi (__A, __B);
}

extern __inline unsigned char
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kortestc_mask8_u8 (__mmask8 __A, __mmask8 __B)
{
  return (unsigned char) __builtin_ia32_kortestcqi (__A, __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kadd_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_kaddqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kadd_mask16 (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kaddhi ((__mmask16) __A, (__mmask16) __B);
}

extern __inline unsigned int
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtmask8_u32 (__mmask8 __A)
{
  return (unsigned int) __builtin_ia32_kmovb ((__mmask8 ) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_cvtu32_mask8 (unsigned int __A)
{
  return (__mmask8) __builtin_ia32_kmovb ((__mmask8) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_load_mask8 (__mmask8 *__A)
{
  return (__mmask8) __builtin_ia32_kmovb (*(__mmask8 *) __A);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_store_mask8 (__mmask8 *__A, __mmask8 __B)
{
  *(__mmask8 *) __A = __builtin_ia32_kmovb (__B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_knot_mask8 (__mmask8 __A)
{
  return (__mmask8) __builtin_ia32_knotqi ((__mmask8) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kor_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_korqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxnor_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_kxnorqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kxor_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_kxorqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kand_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_kandqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kandn_mask8 (__mmask8 __A, __mmask8 __B)
{
  return (__mmask8) __builtin_ia32_kandnqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_f64x2 (__m128d __A)
{
  return (__m512d)
  __builtin_ia32_broadcastf64x2_512_mask ((__v2df) __A,
       _mm512_undefined_pd (),
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_f64x2 (__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d) __builtin_ia32_broadcastf64x2_512_mask ((__v2df)
          __A,
          (__v8df)
          __O, __M);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_f64x2 (__mmask8 __M, __m128d __A)
{
  return (__m512d) __builtin_ia32_broadcastf64x2_512_mask ((__v2df)
          __A,
          (__v8df)
          _mm512_setzero_ps (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_i64x2 (__m128i __A)
{
  return (__m512i)
  __builtin_ia32_broadcasti64x2_512_mask ((__v2di) __A,
       _mm512_undefined_epi32 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_i64x2 (__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti64x2_512_mask ((__v2di)
          __A,
          (__v8di)
          __O, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_i64x2 (__mmask8 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti64x2_512_mask ((__v2di)
          __A,
          (__v8di)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_f32x2 (__m128 __A)
{
  return (__m512)
  __builtin_ia32_broadcastf32x2_512_mask ((__v4sf) __A,
       (__v16sf)_mm512_undefined_ps (),
       (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_f32x2 (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x2_512_mask ((__v4sf) __A,
         (__v16sf)
         __O, __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_f32x2 (__mmask16 __M, __m128 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x2_512_mask ((__v4sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_i32x2 (__m128i __A)
{
  return (__m512i)
  __builtin_ia32_broadcasti32x2_512_mask ((__v4si) __A,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_i32x2 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x2_512_mask ((__v4si)
          __A,
          (__v16si)
          __O, __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_i32x2 (__mmask16 __M, __m128i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x2_512_mask ((__v4si)
          __A,
          (__v16si)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_f32x8 (__m256 __A)
{
  return (__m512)
  __builtin_ia32_broadcastf32x8_512_mask ((__v8sf) __A,
       _mm512_undefined_ps (),
       (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_f32x8 (__m512 __O, __mmask16 __M, __m256 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x8_512_mask ((__v8sf) __A,
         (__v16sf)__O,
         __M);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_f32x8 (__mmask16 __M, __m256 __A)
{
  return (__m512) __builtin_ia32_broadcastf32x8_512_mask ((__v8sf) __A,
         (__v16sf)
         _mm512_setzero_ps (),
         __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_broadcast_i32x8 (__m256i __A)
{
  return (__m512i)
  __builtin_ia32_broadcasti32x8_512_mask ((__v8si) __A,
       (__v16si)
       _mm512_undefined_epi32 (),
       (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_broadcast_i32x8 (__m512i __O, __mmask16 __M, __m256i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x8_512_mask ((__v8si)
          __A,
          (__v16si)__O,
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_broadcast_i32x8 (__mmask16 __M, __m256i __A)
{
  return (__m512i) __builtin_ia32_broadcasti32x8_512_mask ((__v8si)
          __A,
          (__v16si)
          _mm512_setzero_si512 (),
          __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mullo_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_mullo_epi64 (__m512i __W, __mmask8 __U, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_pmullq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di) __W,
        (__mmask8) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_mullo_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmullq512_mask ((__v8di) __A,
        (__v8di) __B,
        (__v8di)
        _mm512_setzero_si512 (),
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_xor_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_xorpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_xor_pd (__m512d __W, __mmask8 __U, __m512d __A,
      __m512d __B)
{
  return (__m512d) __builtin_ia32_xorpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_xor_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_xorpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_xor_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_xorps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_xor_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_xorps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_xor_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_xorps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_or_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_orpd512_mask ((__v8df) __A,
      (__v8df) __B,
      (__v8df)
      _mm512_setzero_pd (),
      (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_or_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_orpd512_mask ((__v8df) __A,
      (__v8df) __B,
      (__v8df) __W,
      (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_or_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_orpd512_mask ((__v8df) __A,
      (__v8df) __B,
      (__v8df)
      _mm512_setzero_pd (),
      (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_or_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_orps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf)
            _mm512_setzero_ps (),
            (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_or_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_orps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf) __W,
            (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_or_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_orps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf)
            _mm512_setzero_ps (),
            (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_and_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_andpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_and_pd (__m512d __W, __mmask8 __U, __m512d __A,
      __m512d __B)
{
  return (__m512d) __builtin_ia32_andpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df) __W,
       (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_and_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_andpd512_mask ((__v8df) __A,
       (__v8df) __B,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_and_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_andps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_and_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_andps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf) __W,
      (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_and_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_andps512_mask ((__v16sf) __A,
      (__v16sf) __B,
      (__v16sf)
      _mm512_setzero_ps (),
      (__mmask16) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_andnot_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_andnpd512_mask ((__v8df) __A,
        (__v8df) __B,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_andnot_pd (__m512d __W, __mmask8 __U, __m512d __A,
         __m512d __B)
{
  return (__m512d) __builtin_ia32_andnpd512_mask ((__v8df) __A,
        (__v8df) __B,
        (__v8df) __W,
        (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_andnot_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_andnpd512_mask ((__v8df) __A,
        (__v8df) __B,
        (__v8df)
        _mm512_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_andnot_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_andnps512_mask ((__v16sf) __A,
       (__v16sf) __B,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_andnot_ps (__m512 __W, __mmask16 __U, __m512 __A,
         __m512 __B)
{
  return (__m512) __builtin_ia32_andnps512_mask ((__v16sf) __A,
       (__v16sf) __B,
       (__v16sf) __W,
       (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_andnot_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_andnps512_mask ((__v16sf) __A,
       (__v16sf) __B,
       (__v16sf)
       _mm512_setzero_ps (),
       (__mmask16) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movepi32_mask (__m512i __A)
{
  return (__mmask16) __builtin_ia32_cvtd2mask512 ((__v16si) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movepi64_mask (__m512i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask512 ((__v8di) __A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movm_epi32 (__mmask16 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2d512 (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_movm_epi64 (__mmask8 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2q512 (__A);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttpd_epi64 (__m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttpd_epi64 (__mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttpd_epu64 (__m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) -1,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di) __W,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttpd_epu64 (__mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttps_epi64 (__m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttps_epi64 (__m512i __W, __mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttps_epi64 (__mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvttps_epu64 (__m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) -1,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvttps_epu64 (__m512i __W, __mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di) __W,
            (__mmask8) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvttps_epu64 (__mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U,
            0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_epi64 (__m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_epi64 (__mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_epu64 (__m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_epu64 (__mmask8 __U, __m512d __A)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_epi64 (__m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_epi64 (__m512i __W, __mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_epi64 (__mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtps_epu64 (__m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtps_epu64 (__m512i __W, __mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtps_epu64 (__mmask8 __U, __m256 __A)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi64_ps (__m512i __A)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) -1,
         0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_ps (__m256 __W, __mmask8 __U, __m512i __A)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf) __W,
         (__mmask8) __U,
         0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi64_ps (__mmask8 __U, __m512i __A)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U,
         0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu64_ps (__m512i __A)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu64_ps (__m256 __W, __mmask8 __U, __m512i __A)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu64_ps (__mmask8 __U, __m512i __A)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepi64_pd (__m512i __A)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) -1,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepi64_pd (__m512d __W, __mmask8 __U, __m512i __A)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df) __W,
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepi64_pd (__mmask8 __U, __m512i __A)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U,
          0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtepu64_pd (__m512i __A)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) -1,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtepu64_pd (__m512d __W, __mmask8 __U, __m512i __A)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df) __W,
           (__mmask8) __U,
           0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtepu64_pd (__mmask8 __U, __m512i __A)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U,
           0x04);
}


extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftli_mask8 (__mmask8 __A, unsigned int __B)
{
  return (__mmask8) __builtin_ia32_kshiftliqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_kshiftri_mask8 (__mmask8 __A, unsigned int __B)
{
  return (__mmask8) __builtin_ia32_kshiftriqi ((__mmask8) __A, (__mmask8) __B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_range_pd (__m512d __A, __m512d __B, int __C)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) -1,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_range_pd (__m512d __W, __mmask8 __U,
        __m512d __A, __m512d __B, int __C)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df) __W,
         (__mmask8) __U,
         0x04);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_range_pd (__mmask8 __U, __m512d __A, __m512d __B, int __C)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U,
         0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_range_ps (__m512 __A, __m512 __B, int __C)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) -1,
        0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_range_ps (__m512 __W, __mmask16 __U,
        __m512 __A, __m512 __B, int __C)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf) __W,
        (__mmask16) __U,
        0x04);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_range_ps (__mmask16 __U, __m512 __A, __m512 __B, int __C)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_reduce_sd (__m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_reducesd_mask ((__v2df) __A,
       (__v2df) __B, __C,
       (__v2df) _mm_setzero_pd (),
       (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_reduce_sd (__m128d __W, __mmask8 __U, __m128d __A,
      __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_reducesd_mask ((__v2df) __A,
       (__v2df) __B, __C,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_reduce_sd (__mmask8 __U, __m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_reducesd_mask ((__v2df) __A,
       (__v2df) __B, __C,
       (__v2df) _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_reduce_ss (__m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_reducess_mask ((__v4sf) __A,
      (__v4sf) __B, __C,
      (__v4sf) _mm_setzero_ps (),
      (__mmask8) -1);
}


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_reduce_ss (__m128 __W, __mmask8 __U, __m128 __A,
      __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_reducess_mask ((__v4sf) __A,
      (__v4sf) __B, __C,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_reduce_ss (__mmask8 __U, __m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_reducess_mask ((__v4sf) __A,
      (__v4sf) __B, __C,
      (__v4sf) _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_sd (__m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) -1,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df) __W,
         (__mmask8) __U,
         0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_sd (__mmask8 __U, __m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U,
         0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_ss (__m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) -1,
        0x04);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf) __W,
        (__mmask8) __U,
        0x04);
}


extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_ss (__mmask8 __U, __m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) __U,
        0x04);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_round_sd (__m128d __A, __m128d __B, int __C, const int __R)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) -1, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_round_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B,
    int __C, const int __R)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df) __W,
         (__mmask8) __U, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_round_sd (__mmask8 __U, __m128d __A, __m128d __B, int __C,
     const int __R)
{
  return (__m128d) __builtin_ia32_rangesd128_mask_round ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_round_ss (__m128 __A, __m128 __B, int __C, const int __R)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) -1, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_round_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B,
    int __C, const int __R)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf) __W,
        (__mmask8) __U, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_round_ss (__mmask8 __U, __m128 __A, __m128 __B, int __C,
     const int __R)
{
  return (__m128) __builtin_ia32_rangess128_mask_round ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) __U, __R);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fpclass_ss_mask (__m128 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassss_mask ((__v4sf) __A, __imm,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fpclass_sd_mask (__m128d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasssd_mask ((__v2df) __A, __imm,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fpclass_ss_mask (__mmask8 __U, __m128 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassss_mask ((__v4sf) __A, __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fpclass_sd_mask (__mmask8 __U, __m128d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasssd_mask ((__v2df) __A, __imm, __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundpd_epi64 (__m512d __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di) __W,
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundpd_epi64 (__mmask8 __U, __m512d __A,
     const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundpd_epu64 (__m512d __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) -1,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di) __W,
            (__mmask8) __U,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundpd_epu64 (__mmask8 __U, __m512d __A,
     const int __R)
{
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundps_epi64 (__m256 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundps_epi64 (__m512i __W, __mmask8 __U, __m256 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di) __W,
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundps_epi64 (__mmask8 __U, __m256 __A,
     const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtt_roundps_epu64 (__m256 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) -1,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtt_roundps_epu64 (__m512i __W, __mmask8 __U, __m256 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di) __W,
            (__mmask8) __U,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtt_roundps_epu64 (__mmask8 __U, __m256 __A,
     const int __R)
{
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
            (__v8di)
            _mm512_setzero_si512 (),
            (__mmask8) __U,
            __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_epi64 (__m512d __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) -1,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di) __W,
          (__mmask8) __U,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_epi64 (__mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_epu64 (__m512d __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di) __W,
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_epu64 (__mmask8 __U, __m512d __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_epi64 (__m256 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) -1,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_epi64 (__m512i __W, __mmask8 __U, __m256 __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di) __W,
          (__mmask8) __U,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_epi64 (__mmask8 __U, __m256 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
          (__v8di)
          _mm512_setzero_si512 (),
          (__mmask8) __U,
          __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundps_epu64 (__m256 __A, const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) -1,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundps_epu64 (__m512i __W, __mmask8 __U, __m256 __A,
          const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di) __W,
           (__mmask8) __U,
           __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundps_epu64 (__mmask8 __U, __m256 __A,
    const int __R)
{
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
           (__v8di)
           _mm512_setzero_si512 (),
           (__mmask8) __U,
           __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepi64_ps (__m512i __A, const int __R)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) -1,
         __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepi64_ps (__m256 __W, __mmask8 __U, __m512i __A,
          const int __R)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf) __W,
         (__mmask8) __U,
         __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepi64_ps (__mmask8 __U, __m512i __A,
    const int __R)
{
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U,
         __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepu64_ps (__m512i __A, const int __R)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1,
          __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepu64_ps (__m256 __W, __mmask8 __U, __m512i __A,
          const int __R)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf) __W,
          (__mmask8) __U,
          __R);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepu64_ps (__mmask8 __U, __m512i __A,
    const int __R)
{
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U,
          __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepi64_pd (__m512i __A, const int __R)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) -1,
          __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepi64_pd (__m512d __W, __mmask8 __U, __m512i __A,
          const int __R)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df) __W,
          (__mmask8) __U,
          __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepi64_pd (__mmask8 __U, __m512i __A,
    const int __R)
{
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U,
          __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundepu64_pd (__m512i __A, const int __R)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) -1,
           __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundepu64_pd (__m512d __W, __mmask8 __U, __m512i __A,
          const int __R)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df) __W,
           (__mmask8) __U,
           __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundepu64_pd (__mmask8 __U, __m512i __A,
    const int __R)
{
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
           (__v8df)
           _mm512_setzero_pd (),
           (__mmask8) __U,
           __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_pd (__m512d __A, int __B)
{
  return (__m512d) __builtin_ia32_reducepd512_mask ((__v8df) __A, __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_pd (__m512d __W, __mmask8 __U, __m512d __A, int __B)
{
  return (__m512d) __builtin_ia32_reducepd512_mask ((__v8df) __A, __B,
          (__v8df) __W,
          (__mmask8) __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_reduce_pd (__mmask8 __U, __m512d __A, int __B)
{
  return (__m512d) __builtin_ia32_reducepd512_mask ((__v8df) __A, __B,
          (__v8df)
          _mm512_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_reduce_ps (__m512 __A, int __B)
{
  return (__m512) __builtin_ia32_reduceps512_mask ((__v16sf) __A, __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_reduce_ps (__m512 __W, __mmask16 __U, __m512 __A, int __B)
{
  return (__m512) __builtin_ia32_reduceps512_mask ((__v16sf) __A, __B,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_reduce_ps (__mmask16 __U, __m512 __A, int __B)
{
  return (__m512) __builtin_ia32_reduceps512_mask ((__v16sf) __A, __B,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extractf32x8_ps (__m512 __A, const int __imm)
{
  return (__m256) __builtin_ia32_extractf32x8_mask ((__v16sf) __A,
          __imm,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extractf32x8_ps (__m256 __W, __mmask8 __U, __m512 __A,
        const int __imm)
{
  return (__m256) __builtin_ia32_extractf32x8_mask ((__v16sf) __A,
          __imm,
          (__v8sf) __W,
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extractf32x8_ps (__mmask8 __U, __m512 __A,
         const int __imm)
{
  return (__m256) __builtin_ia32_extractf32x8_mask ((__v16sf) __A,
          __imm,
          (__v8sf)
          _mm256_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extractf64x2_pd (__m512d __A, const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df) __A,
        __imm,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extractf64x2_pd (__m128d __W, __mmask8 __U, __m512d __A,
        const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df) __A,
        __imm,
        (__v2df) __W,
        (__mmask8)
        __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extractf64x2_pd (__mmask8 __U, __m512d __A,
         const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df) __A,
        __imm,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extracti32x8_epi32 (__m512i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_extracti32x8_mask ((__v16si) __A,
           __imm,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extracti32x8_epi32 (__m256i __W, __mmask8 __U, __m512i __A,
    const int __imm)
{
  return (__m256i) __builtin_ia32_extracti32x8_mask ((__v16si) __A,
           __imm,
           (__v8si) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extracti32x8_epi32 (__mmask8 __U, __m512i __A,
     const int __imm)
{
  return (__m256i) __builtin_ia32_extracti32x8_mask ((__v16si) __A,
           __imm,
           (__v8si)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_extracti64x2_epi64 (__m512i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di) __A,
        __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_extracti64x2_epi64 (__m128i __W, __mmask8 __U, __m512i __A,
    const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di) __A,
        __imm,
        (__v2di) __W,
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_extracti64x2_epi64 (__mmask8 __U, __m512i __A,
     const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di) __A,
        __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_range_round_pd (__m512d __A, __m512d __B, int __C,
         const int __R)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) -1,
         __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_range_round_pd (__m512d __W, __mmask8 __U,
       __m512d __A, __m512d __B, int __C,
       const int __R)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df) __W,
         (__mmask8) __U,
         __R);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_range_round_pd (__mmask8 __U, __m512d __A, __m512d __B,
        int __C, const int __R)
{
  return (__m512d) __builtin_ia32_rangepd512_mask ((__v8df) __A,
         (__v8df) __B, __C,
         (__v8df)
         _mm512_setzero_pd (),
         (__mmask8) __U,
         __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_range_round_ps (__m512 __A, __m512 __B, int __C, const int __R)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) -1,
        __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_range_round_ps (__m512 __W, __mmask16 __U,
       __m512 __A, __m512 __B, int __C,
       const int __R)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf) __W,
        (__mmask16) __U,
        __R);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_range_round_ps (__mmask16 __U, __m512 __A, __m512 __B,
        int __C, const int __R)
{
  return (__m512) __builtin_ia32_rangeps512_mask ((__v16sf) __A,
        (__v16sf) __B, __C,
        (__v16sf)
        _mm512_setzero_ps (),
        (__mmask16) __U,
        __R);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_inserti32x8 (__m512i __A, __m256i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x8_mask ((__v16si) __A,
          (__v8si) __B,
          __imm,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_inserti32x8 (__m512i __W, __mmask16 __U, __m512i __A,
    __m256i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x8_mask ((__v16si) __A,
          (__v8si) __B,
          __imm,
          (__v16si) __W,
          (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_inserti32x8 (__mmask16 __U, __m512i __A, __m256i __B,
     const int __imm)
{
  return (__m512i) __builtin_ia32_inserti32x8_mask ((__v16si) __A,
          (__v8si) __B,
          __imm,
          (__v16si)
          _mm512_setzero_si512 (),
          (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_insertf32x8 (__m512 __A, __m256 __B, const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x8_mask ((__v16sf) __A,
         (__v8sf) __B,
         __imm,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) -1);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_insertf32x8 (__m512 __W, __mmask16 __U, __m512 __A,
    __m256 __B, const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x8_mask ((__v16sf) __A,
         (__v8sf) __B,
         __imm,
         (__v16sf) __W,
         (__mmask16) __U);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_insertf32x8 (__mmask16 __U, __m512 __A, __m256 __B,
     const int __imm)
{
  return (__m512) __builtin_ia32_insertf32x8_mask ((__v16sf) __A,
         (__v8sf) __B,
         __imm,
         (__v16sf)
         _mm512_setzero_ps (),
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_inserti64x2 (__m512i __A, __m128i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x2_512_mask ((__v8di) __A,
       (__v2di) __B,
       __imm,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_inserti64x2 (__m512i __W, __mmask8 __U, __m512i __A,
    __m128i __B, const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x2_512_mask ((__v8di) __A,
       (__v2di) __B,
       __imm,
       (__v8di) __W,
       (__mmask8)
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_inserti64x2 (__mmask8 __U, __m512i __A, __m128i __B,
     const int __imm)
{
  return (__m512i) __builtin_ia32_inserti64x2_512_mask ((__v8di) __A,
       (__v2di) __B,
       __imm,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8)
       __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_insertf64x2 (__m512d __A, __m128d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x2_512_mask ((__v8df) __A,
       (__v2df) __B,
       __imm,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8) -1);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_insertf64x2 (__m512d __W, __mmask8 __U, __m512d __A,
    __m128d __B, const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x2_512_mask ((__v8df) __A,
       (__v2df) __B,
       __imm,
       (__v8df) __W,
       (__mmask8)
       __U);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_insertf64x2 (__mmask8 __U, __m512d __A, __m128d __B,
     const int __imm)
{
  return (__m512d) __builtin_ia32_insertf64x2_512_mask ((__v8df) __A,
       (__v2df) __B,
       __imm,
       (__v8df)
       _mm512_setzero_pd (),
       (__mmask8)
       __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fpclass_pd_mask (__mmask8 __U, __m512d __A,
        const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd512_mask ((__v8df) __A,
            __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fpclass_pd_mask (__m512d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd512_mask ((__v8df) __A,
            __imm,
            (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fpclass_ps_mask (__mmask16 __U, __m512 __A,
        const int __imm)
{
  return (__mmask16) __builtin_ia32_fpclassps512_mask ((__v16sf) __A,
             __imm, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fpclass_ps_mask (__m512 __A, const int __imm)
{
  return (__mmask16) __builtin_ia32_fpclassps512_mask ((__v16sf) __A,
             __imm,
             (__mmask16) -1);
}
# 2702 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512dqintrin.h" 3 4
#pragma GCC pop_options
# 68 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlbwintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlbwintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vl,avx512bw")




extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdquqi256_mask ((__v32qi) __A,
          (__v32qi) __W,
          (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdquqi256_mask ((__v32qi) __A,
          (__v32qi)
          _mm256_setzero_si256 (),
          (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdquqi128_mask ((__v16qi) __A,
          (__v16qi) __W,
          (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdquqi128_mask ((__v16qi) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_epi8 (void *__P, __mmask32 __U, __m256i __A)
{
  __builtin_ia32_storedquqi256_mask ((char *) __P,
         (__v32qi) __A,
         (__mmask32) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_epi8 (void *__P, __mmask16 __U, __m128i __A)
{
  __builtin_ia32_storedquqi128_mask ((char *) __P,
         (__v16qi) __A,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_epi16 (__m256i __W, __mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const short *) __P,
           (__v16hi) __W,
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const short *) __P,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_epi16 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const short *) __P,
           (__v8hi) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_epi16 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const short *) __P,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mov_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdquhi256_mask ((__v16hi) __A,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mov_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_movdquhi256_mask ((__v16hi) __A,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mov_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdquhi128_mask ((__v8hi) __A,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mov_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_movdquhi128_mask ((__v8hi) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const char *) __P,
           (__v32qi) __W,
           (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const char *) __P,
           (__v32qi)
           _mm256_setzero_si256 (),
           (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const char *) __P,
           (__v16qi) __W,
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const char *) __P,
           (__v16qi)
           _mm_setzero_si128 (),
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi16_epi8 (__m256i __A)
{

  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
        (__v16qi)_mm_undefined_si128(),
        (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi16_storeu_epi8 (void * __P, __mmask16 __M,__m256i __A)
{
  __builtin_ia32_pmovwb256mem_mask ((__v16qi *) __P , (__v16hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi16_epi8 (__mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsepi16_epi8 (__m128i __A)
{

  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
         (__v16qi)_mm_undefined_si128(),
         (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask8 __M,__m128i __A)
{
  __builtin_ia32_pmovswb128mem_mask ((__v8qi *) __P , (__v8hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsepi16_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtsepi16_epi8 (__m256i __A)
{

  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
         (__v16qi)_mm_undefined_si128(),
         (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask16 __M,__m256i __A)
{
  __builtin_ia32_pmovswb256mem_mask ((__v16qi *) __P , (__v16hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtsepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
         (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtsepi16_epi8 (__mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
         (__v16qi)
         _mm_setzero_si128 (),
         __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtusepi16_epi8 (__m128i __A)
{

  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
          (__v16qi)_mm_undefined_si128(),
          (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask8 __M,__m128i __A)
{
  __builtin_ia32_pmovuswb128mem_mask ((__v8qi *) __P , (__v8hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtusepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtusepi16_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtusepi16_epi8 (__m256i __A)
{

  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
          (__v16qi)_mm_undefined_si128(),
          (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask16 __M,__m256i __A)
{
  __builtin_ia32_pmovuswb256mem_mask ((__v16qi *) __P , (__v16hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtusepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtusepi16_epi8 (__mmask16 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastb_epi8 (__m256i __O, __mmask32 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastb256_mask ((__v16qi) __A,
             (__v32qi) __O,
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastb_epi8 (__mmask32 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastb256_mask ((__v16qi) __A,
             (__v32qi)
             _mm256_setzero_si256 (),
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_set1_epi8 (__m256i __O, __mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_pbroadcastb256_gpr_mask (__A,
          (__v32qi) __O,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_set1_epi8 (__mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_pbroadcastb256_gpr_mask (__A,
          (__v32qi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcastb_epi8 (__m128i __O, __mmask16 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastb128_mask ((__v16qi) __A,
             (__v16qi) __O,
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcastb_epi8 (__mmask16 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastb128_mask ((__v16qi) __A,
             (__v16qi)
             _mm_setzero_si128 (),
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_set1_epi8 (__m128i __O, __mmask16 __M, char __A)
{
  return (__m128i) __builtin_ia32_pbroadcastb128_gpr_mask (__A,
          (__v16qi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_set1_epi8 (__mmask16 __M, char __A)
{
  return (__m128i) __builtin_ia32_pbroadcastb128_gpr_mask (__A,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcastw_epi16 (__m256i __O, __mmask16 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastw256_mask ((__v8hi) __A,
             (__v16hi) __O,
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcastw_epi16 (__mmask16 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_pbroadcastw256_mask ((__v8hi) __A,
             (__v16hi)
             _mm256_setzero_si256 (),
             __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_set1_epi16 (__m256i __O, __mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_pbroadcastw256_gpr_mask (__A,
          (__v16hi) __O,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_set1_epi16 (__mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_pbroadcastw256_gpr_mask (__A,
          (__v16hi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcastw_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastw128_mask ((__v8hi) __A,
             (__v8hi) __O,
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcastw_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pbroadcastw128_mask ((__v8hi) __A,
             (__v8hi)
             _mm_setzero_si128 (),
             __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_set1_epi16 (__m128i __O, __mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_pbroadcastw128_gpr_mask (__A,
          (__v8hi) __O,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_set1_epi16 (__mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_pbroadcastw128_gpr_mask (__A,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutexvar_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
           (__v16hi) __A,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_epi16 (__mmask16 __M, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
           (__v16hi) __A,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
          __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
           (__v16hi) __A,
           (__v16hi) __W,
           (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutexvar_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
           (__v8hi) __A,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutexvar_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
           (__v8hi) __A,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutexvar_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
           (__v8hi) __A,
           (__v8hi) __W,
           (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_epi16 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_mask ((__v16hi) __I
                 ,
       (__v16hi) __A,
       (__v16hi) __B,
       (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_epi16 (__m256i __A, __mmask16 __U,
    __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_mask ((__v16hi) __I
                 ,
       (__v16hi) __A,
       (__v16hi) __B,
       (__mmask16)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_epi16 (__m256i __A, __m256i __I,
     __mmask16 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2varhi256_mask ((__v16hi) __A,
       (__v16hi) __I
                 ,
       (__v16hi) __B,
       (__mmask16)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_epi16 (__mmask16 __U, __m256i __A,
     __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_maskz ((__v16hi) __I
                  ,
        (__v16hi) __A,
        (__v16hi) __B,
        (__mmask16)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_epi16 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_mask ((__v8hi) __I
                 ,
       (__v8hi) __A,
       (__v8hi) __B,
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_epi16 (__m128i __A, __mmask8 __U, __m128i __I,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_mask ((__v8hi) __I
                 ,
       (__v8hi) __A,
       (__v8hi) __B,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_epi16 (__m128i __A, __m128i __I, __mmask8 __U,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2varhi128_mask ((__v8hi) __A,
       (__v8hi) __I
                 ,
       (__v8hi) __B,
       (__mmask8)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_epi16 (__mmask8 __U, __m128i __A, __m128i __I,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_maskz ((__v8hi) __I
                  ,
        (__v8hi) __A,
        (__v8hi) __B,
        (__mmask8)
        __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_maddubs_epi16 (__m256i __W, __mmask16 __U, __m256i __X,
      __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmaddubsw256_mask ((__v32qi) __X,
           (__v32qi) __Y,
           (__v16hi) __W,
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_maddubs_epi16 (__mmask16 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmaddubsw256_mask ((__v32qi) __X,
           (__v32qi) __Y,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_maddubs_epi16 (__m128i __W, __mmask8 __U, __m128i __X,
   __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaddubsw128_mask ((__v16qi) __X,
           (__v16qi) __Y,
           (__v8hi) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_maddubs_epi16 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmaddubsw128_mask ((__v16qi) __X,
           (__v16qi) __Y,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_madd_epi16 (__m256i __W, __mmask8 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaddwd256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v8si) __W,
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_madd_epi16 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaddwd256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v8si)
         _mm256_setzero_si256 (),
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_madd_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaddwd128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v4si) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_madd_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaddwd128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v4si)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movepi8_mask (__m128i __A)
{
  return (__mmask16) __builtin_ia32_cvtb2mask128 ((__v16qi) __A);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movepi8_mask (__m256i __A)
{
  return (__mmask32) __builtin_ia32_cvtb2mask256 ((__v32qi) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movepi16_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtw2mask128 ((__v8hi) __A);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movepi16_mask (__m256i __A)
{
  return (__mmask16) __builtin_ia32_cvtw2mask256 ((__v16hi) __A);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movm_epi8 (__mmask16 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2b128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movm_epi8 (__mmask32 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2b256 (__A);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movm_epi16 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2w128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movm_epi16 (__mmask16 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2w256 (__A);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_test_epi8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ptestmb128 ((__v16qi) __A,
      (__v16qi) __B,
      (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_test_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ptestmb128 ((__v16qi) __A,
      (__v16qi) __B, __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_test_epi8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ptestmb256 ((__v32qi) __A,
      (__v32qi) __B,
      (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_test_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ptestmb256 ((__v32qi) __A,
      (__v32qi) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_test_epi16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmw128 ((__v8hi) __A,
            (__v8hi) __B,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_test_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestmw128 ((__v8hi) __A,
            (__v8hi) __B, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_test_epi16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ptestmw256 ((__v16hi) __A,
      (__v16hi) __B,
      (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_test_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ptestmw256 ((__v16hi) __A,
      (__v16hi) __B, __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epu16 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminuw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epu16 (__m256i __W, __mmask16 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminuw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epu16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminuw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epu16 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminuw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epi16 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxub256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epu8 (__m256i __W, __mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxub256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epu8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxub128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epu8 (__m128i __W, __mmask16 __M, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxub128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epi8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminub256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epu8 (__m256i __W, __mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pminub256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epu8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminub128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epu8 (__m128i __W, __mmask16 __M, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_pminub128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_min_epi8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_min_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pminsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epi16 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_max_epu16 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxuw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_max_epu16 (__m256i __W, __mmask16 __M, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pmaxuw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_max_epu16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxuw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_max_epu16 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pmaxuw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_min_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_min_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pminsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __M);
}


extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_alignr_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
    __m256i __B, const int __N)
{
  return (__m256i) __builtin_ia32_palignr256_mask ((__v4di) __A,
         (__v4di) __B,
         __N * 8,
         (__v4di) __W,
         (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_alignr_epi8 (__mmask32 __U, __m256i __A, __m256i __B,
     const int __N)
{
  return (__m256i) __builtin_ia32_palignr256_mask ((__v4di) __A,
         (__v4di) __B,
         __N * 8,
         (__v4di)
         _mm256_setzero_si256 (),
         (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_alignr_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
        __m128i __B, const int __N)
{
  return (__m128i) __builtin_ia32_palignr128_mask ((__v2di) __A,
         (__v2di) __B,
         __N * 8,
         (__v2di) __W,
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_alignr_epi8 (__mmask16 __U, __m128i __A, __m128i __B,
         const int __N)
{
  return (__m128i) __builtin_ia32_palignr128_mask ((__v2di) __A,
         (__v2di) __B,
         __N * 8,
         (__v2di)
         _mm_setzero_si128 (),
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dbsad_epu8 (__m256i __A, __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_dbpsadbw256_mask ((__v32qi) __A,
          (__v32qi) __B,
          __imm,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dbsad_epu8 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_dbpsadbw256_mask ((__v32qi) __A,
          (__v32qi) __B,
          __imm,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dbsad_epu8 (__mmask16 __U, __m256i __A, __m256i __B,
    const int __imm)
{
  return (__m256i) __builtin_ia32_dbpsadbw256_mask ((__v32qi) __A,
          (__v32qi) __B,
          __imm,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_dbsad_epu8 (__m128i __A, __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_dbpsadbw128_mask ((__v16qi) __A,
          (__v16qi) __B,
          __imm,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dbsad_epu8 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B, const int __imm)
{
  return (__m128i) __builtin_ia32_dbpsadbw128_mask ((__v16qi) __A,
          (__v16qi) __B,
          __imm,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dbsad_epu8 (__mmask8 __U, __m128i __A, __m128i __B,
        const int __imm)
{
  return (__m128i) __builtin_ia32_dbpsadbw128_mask ((__v16qi) __A,
          (__v16qi) __B,
          __imm,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_epi16 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_blendmw_128_mask ((__v8hi) __A,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_blend_epi8 (__mmask16 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_blendmb_128_mask ((__v16qi) __A,
          (__v16qi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_epi16 (__mmask16 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_blendmw_256_mask ((__v16hi) __A,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_blend_epi8 (__mmask32 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_blendmb_256_mask ((__v32qi) __A,
          (__v32qi) __W,
          (__mmask32) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epi16_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, __P,
       (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epi16_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, __P,
       (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epi16_mask (__mmask16 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, __P,
        (__mmask16) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epi16_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, __P,
        (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epi8_mask (__mmask16 __U, __m128i __X, __m128i __Y,
   const int __P)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, __P,
        (__mmask16) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epi8_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, __P,
        (__mmask16) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epi8_mask (__mmask32 __U, __m256i __X, __m256i __Y,
      const int __P)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, __P,
        (__mmask32) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epi8_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, __P,
        (__mmask32) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epu16_mask (__mmask8 __U, __m128i __X, __m128i __Y,
    const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, __P,
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epu16_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, __P,
        (__mmask8) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epu16_mask (__mmask16 __U, __m256i __X, __m256i __Y,
       const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, __P,
         (__mmask16) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epu16_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, __P,
         (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmp_epu8_mask (__mmask16 __U, __m128i __X, __m128i __Y,
   const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, __P,
         (__mmask16) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmp_epu8_mask (__m128i __X, __m128i __Y, const int __P)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, __P,
         (__mmask16) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmp_epu8_mask (__mmask32 __U, __m256i __X, __m256i __Y,
      const int __P)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, __P,
         (__mmask32) __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmp_epu8_mask (__m256i __X, __m256i __Y, const int __P)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, __P,
         (__mmask32) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srli_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psrlwi256_mask ((__v16hi) __A, __imm,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srli_epi16 (__mmask16 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psrlwi256_mask ((__v16hi) __A, __imm,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srli_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psrlwi128_mask ((__v8hi) __A, __imm,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srli_epi16 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psrlwi128_mask ((__v8hi) __A, __imm,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shufflehi_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
        const int __imm)
{
  return (__m256i) __builtin_ia32_pshufhw256_mask ((__v16hi) __A,
         __imm,
         (__v16hi) __W,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shufflehi_epi16 (__mmask16 __U, __m256i __A,
         const int __imm)
{
  return (__m256i) __builtin_ia32_pshufhw256_mask ((__v16hi) __A,
         __imm,
         (__v16hi)
         _mm256_setzero_si256 (),
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shufflehi_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
     const int __imm)
{
  return (__m128i) __builtin_ia32_pshufhw128_mask ((__v8hi) __A, __imm,
         (__v8hi) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shufflehi_epi16 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_pshufhw128_mask ((__v8hi) __A, __imm,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shufflelo_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
        const int __imm)
{
  return (__m256i) __builtin_ia32_pshuflw256_mask ((__v16hi) __A,
         __imm,
         (__v16hi) __W,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shufflelo_epi16 (__mmask16 __U, __m256i __A,
         const int __imm)
{
  return (__m256i) __builtin_ia32_pshuflw256_mask ((__v16hi) __A,
         __imm,
         (__v16hi)
         _mm256_setzero_si256 (),
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shufflelo_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
     const int __imm)
{
  return (__m128i) __builtin_ia32_pshuflw128_mask ((__v8hi) __A, __imm,
         (__v8hi) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shufflelo_epi16 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_pshuflw128_mask ((__v8hi) __A, __imm,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srai_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   const int __imm)
{
  return (__m256i) __builtin_ia32_psrawi256_mask ((__v16hi) __A, __imm,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srai_epi16 (__mmask16 __U, __m256i __A, const int __imm)
{
  return (__m256i) __builtin_ia32_psrawi256_mask ((__v16hi) __A, __imm,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srai_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       const int __imm)
{
  return (__m128i) __builtin_ia32_psrawi128_mask ((__v8hi) __A, __imm,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srai_epi16 (__mmask8 __U, __m128i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_psrawi128_mask ((__v8hi) __A, __imm,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_slli_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   int __B)
{
  return (__m256i) __builtin_ia32_psllwi256_mask ((__v16hi) __A, __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_slli_epi16 (__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i) __builtin_ia32_psllwi256_mask ((__v16hi) __A, __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_slli_epi16 (__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_psllwi128_mask ((__v8hi) __A, __B,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i) __builtin_ia32_psllwi128_mask ((__v8hi) __A, __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}
# 2039 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vlbwintrin.h" 3 4
extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epi8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 4,
        (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epi8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 1,
        (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epi8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 5,
        (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epi8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 2,
        (__mmask32) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epi16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 4,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epi16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 1,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epi16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 5,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epi16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 2,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epu8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 4,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epu8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 1,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epu8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 5,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epu8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 2,
         (__mmask16) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epu16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 4,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epu16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 1,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epu16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 5,
        (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epu16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 2,
        (__mmask8) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epi8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 4,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 1,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epi8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 5,
        (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epi8_mask (__m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 2,
        (__mmask16) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpneq_epi16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 4,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmplt_epi16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 1,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpge_epi16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 5,
       (__mmask8) -1);
}

extern __inline __mmask8
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmple_epi16_mask (__m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 2,
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mulhrs_epi16 (__m256i __W, __mmask16 __U, __m256i __X,
     __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmulhrsw256_mask ((__v16hi) __X,
          (__v16hi) __Y,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mulhrs_epi16 (__mmask16 __U, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_pmulhrsw256_mask ((__v16hi) __X,
          (__v16hi) __Y,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mulhi_epu16 (__m256i __W, __mmask16 __U, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulhuw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi) __W,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mulhi_epu16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulhuw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi)
         _mm256_setzero_si256 (),
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mulhi_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulhw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mulhi_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmulhw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mulhi_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulhw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mulhi_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulhw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mulhi_epu16 (__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulhuw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mulhi_epu16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmulhuw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mulhrs_epi16 (__m128i __W, __mmask8 __U, __m128i __X,
         __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmulhrsw128_mask ((__v8hi) __X,
          (__v8hi) __Y,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mulhrs_epi16 (__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_pmulhrsw128_mask ((__v8hi) __X,
          (__v8hi) __Y,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mullo_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_pmullw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mullo_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmullw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mullo_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_pmullw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mullo_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmullw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi8_epi16 (__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbw256_mask ((__v16qi) __A,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi8_epi16 (__mmask16 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovsxbw256_mask ((__v16qi) __A,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi8_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbw128_mask ((__v16qi) __A,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi8_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsxbw128_mask ((__v16qi) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu8_epi16 (__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbw256_mask ((__v16qi) __A,
          (__v16hi) __W,
          (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu8_epi16 (__mmask16 __U, __m128i __A)
{
  return (__m256i) __builtin_ia32_pmovzxbw256_mask ((__v16qi) __A,
          (__v16hi)
          _mm256_setzero_si256 (),
          (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu8_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbw128_mask ((__v16qi) __A,
          (__v8hi) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu8_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovzxbw128_mask ((__v16qi) __A,
          (__v8hi)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_avg_epu8 (__m256i __W, __mmask32 __U, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_pavgb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi) __W,
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_avg_epu8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pavgb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi)
       _mm256_setzero_si256 (),
       (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_avg_epu8 (__m128i __W, __mmask16 __U, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_pavgb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi) __W,
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_avg_epu8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pavgb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi)
       _mm_setzero_si128 (),
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_avg_epu16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_pavgw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_avg_epu16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pavgw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_avg_epu16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_pavgw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_avg_epu16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pavgw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_paddb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi) __W,
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi)
       _mm256_setzero_si256 (),
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_add_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_paddw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_add_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_adds_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_paddsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_adds_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_adds_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_paddsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_adds_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_adds_epu8 (__m256i __W, __mmask32 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_paddusb256_mask ((__v32qi) __A,
         (__v32qi) __B,
         (__v32qi) __W,
         (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_adds_epu8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddusb256_mask ((__v32qi) __A,
         (__v32qi) __B,
         (__v32qi)
         _mm256_setzero_si256 (),
         (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_adds_epu16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_paddusw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi) __W,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_adds_epu16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_paddusw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi)
         _mm256_setzero_si256 (),
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_psubb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi) __W,
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__v32qi)
       _mm256_setzero_si256 (),
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sub_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_psubw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sub_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubw256_mask ((__v16hi) __A,
       (__v16hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_subs_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_psubsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_subs_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubsb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_subs_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_psubsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_subs_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubsw256_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_subs_epu8 (__m256i __W, __mmask32 __U, __m256i __A,
         __m256i __B)
{
  return (__m256i) __builtin_ia32_psubusb256_mask ((__v32qi) __A,
         (__v32qi) __B,
         (__v32qi) __W,
         (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_subs_epu8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubusb256_mask ((__v32qi) __A,
         (__v32qi) __B,
         (__v32qi)
         _mm256_setzero_si256 (),
         (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_subs_epu16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_psubusw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi) __W,
         (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_subs_epu16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psubusw256_mask ((__v16hi) __A,
         (__v16hi) __B,
         (__v16hi)
         _mm256_setzero_si256 (),
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_paddb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi) __W,
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi)
       _mm_setzero_si128 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_add_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_paddw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_add_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhbw256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__v32qi) __W,
           (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhbw256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__v32qi)
           _mm256_setzero_si256 (),
           (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
   __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhbw128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__v16qi) __W,
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhbw128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__v16qi)
           _mm_setzero_si128 (),
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpackhi_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhwd256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__v16hi) __W,
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpackhi_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpckhwd256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpackhi_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhwd128_mask ((__v8hi) __A,
           (__v8hi) __B,
           (__v8hi) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpackhi_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpckhwd128_mask ((__v8hi) __A,
           (__v8hi) __B,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklbw256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__v32qi) __W,
           (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklbw256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__v32qi)
           _mm256_setzero_si256 (),
           (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
   __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklbw128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__v16qi) __W,
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklbw128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__v16qi)
           _mm_setzero_si128 (),
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_unpacklo_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
       __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklwd256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__v16hi) __W,
           (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_unpacklo_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_punpcklwd256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__v16hi)
           _mm256_setzero_si256 (),
           (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_unpacklo_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
    __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklwd128_mask ((__v8hi) __A,
           (__v8hi) __B,
           (__v8hi) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_unpacklo_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_punpcklwd128_mask ((__v8hi) __A,
           (__v8hi) __B,
           (__v8hi)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqb128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epu8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __A,
          (__v16qi) __B, 0,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epu8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __A,
          (__v16qi) __B, 0,
          __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqb128_mask ((__v16qi) __A,
           (__v16qi) __B,
           __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epu8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __A,
          (__v32qi) __B, 0,
          (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_pcmpeqb256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epu8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __A,
          (__v32qi) __B, 0,
          __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_pcmpeqb256_mask ((__v32qi) __A,
           (__v32qi) __B,
           __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epu16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __A,
         (__v8hi) __B, 0,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpeq_epi16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqw128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epu16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __A,
         (__v8hi) __B, 0, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpeq_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpeqw128_mask ((__v8hi) __A,
          (__v8hi) __B, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epu16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __A,
          (__v16hi) __B, 0,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpeq_epi16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqw256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epu16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __A,
          (__v16hi) __B, 0,
          __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpeq_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_pcmpeqw256_mask ((__v16hi) __A,
           (__v16hi) __B,
           __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epu8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __A,
          (__v16qi) __B, 6,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtb128_mask ((__v16qi) __A,
           (__v16qi) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epu8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __A,
          (__v16qi) __B, 6,
          __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtb128_mask ((__v16qi) __A,
           (__v16qi) __B,
           __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epu8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __A,
          (__v32qi) __B, 6,
          (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_pcmpgtb256_mask ((__v32qi) __A,
           (__v32qi) __B,
           (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epu8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __A,
          (__v32qi) __B, 6,
          __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_pcmpgtb256_mask ((__v32qi) __A,
           (__v32qi) __B,
           __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epu16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __A,
         (__v8hi) __B, 6,
         (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cmpgt_epi16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtw128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epu16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __A,
         (__v8hi) __B, 6, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpgt_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_pcmpgtw128_mask ((__v8hi) __A,
          (__v8hi) __B, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epu16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __A,
          (__v16hi) __B, 6,
          (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpgt_epi16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtw256_mask ((__v16hi) __A,
           (__v16hi) __B,
           (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epu16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __A,
          (__v16hi) __B, 6,
          __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpgt_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_pcmpgtw256_mask ((__v16hi) __A,
           (__v16hi) __B,
           __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_testn_epi8_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmb128 ((__v16qi) __A,
       (__v16qi) __B,
       (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_testn_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmb128 ((__v16qi) __A,
       (__v16qi) __B, __U);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testn_epi8_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ptestnmb256 ((__v32qi) __A,
       (__v32qi) __B,
       (__mmask32) -1);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_testn_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_ptestnmb256 ((__v32qi) __A,
       (__v32qi) __B, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_testn_epi16_mask (__m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmw128 ((__v8hi) __A,
      (__v8hi) __B,
      (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_testn_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__mmask8) __builtin_ia32_ptestnmw128 ((__v8hi) __A,
      (__v8hi) __B, __U);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_testn_epi16_mask (__m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmw256 ((__v16hi) __A,
       (__v16hi) __B,
       (__mmask16) -1);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_testn_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__mmask16) __builtin_ia32_ptestnmw256 ((__v16hi) __A,
       (__v16hi) __B, __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shuffle_epi8 (__m256i __W, __mmask32 __U, __m256i __A,
     __m256i __B)
{
  return (__m256i) __builtin_ia32_pshufb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi) __W,
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shuffle_epi8 (__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pshufb256_mask ((__v32qi) __A,
        (__v32qi) __B,
        (__v32qi)
        _mm256_setzero_si256 (),
        (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shuffle_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_pshufb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shuffle_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pshufb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_packs_epi16 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_packsswb256_mask ((__v16hi) __A,
          (__v16hi) __B,
          (__v32qi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_packs_epi16 (__m256i __W, __mmask32 __M, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_packsswb256_mask ((__v16hi) __A,
          (__v16hi) __B,
          (__v32qi) __W,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_packs_epi16 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_packsswb128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_packs_epi16 (__m128i __W, __mmask16 __M, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_packsswb128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__v16qi) __W,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_packus_epi16 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_packuswb256_mask ((__v16hi) __A,
          (__v16hi) __B,
          (__v32qi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_packus_epi16 (__m256i __W, __mmask32 __M, __m256i __A,
     __m256i __B)
{
  return (__m256i) __builtin_ia32_packuswb256_mask ((__v16hi) __A,
          (__v16hi) __B,
          (__v32qi) __W,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_packus_epi16 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_packuswb128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__v16qi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_packus_epi16 (__m128i __W, __mmask16 __M, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_packuswb128_mask ((__v8hi) __A,
          (__v8hi) __B,
          (__v16qi) __W,
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_abs_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsb256_mask ((__v32qi) __A,
       (__v32qi) __W,
       (__mmask32) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_abs_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsb256_mask ((__v32qi) __A,
       (__v32qi)
       _mm256_setzero_si256 (),
       (__mmask32) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_abs_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsb128_mask ((__v16qi) __A,
       (__v16qi) __W,
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_abs_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsb128_mask ((__v16qi) __A,
       (__v16qi)
       _mm_setzero_si128 (),
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_abs_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsw256_mask ((__v16hi) __A,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_abs_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_pabsw256_mask ((__v16hi) __A,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_abs_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsw128_mask ((__v8hi) __A,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_abs_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_pabsw128_mask ((__v8hi) __A,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epu8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 4,
         (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epu8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 1,
         (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epu8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 5,
         (__mmask32) -1);
}

extern __inline __mmask32
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epu8_mask (__m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 2,
         (__mmask32) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpneq_epu16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 4,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmplt_epu16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 1,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmpge_epu16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 5,
         (__mmask16) -1);
}

extern __inline __mmask16
  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cmple_epu16_mask (__m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 2,
         (__mmask16) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_storeu_epi16 (void *__P, __mmask16 __U, __m256i __A)
{
  __builtin_ia32_storedquhi256_mask ((short *) __P,
         (__v16hi) __A,
         (__mmask16) __U);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_storeu_epi16 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedquhi128_mask ((short *) __P,
         (__v8hi) __A,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_adds_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_paddsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_subs_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psubsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_subs_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_subs_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_psubsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_subs_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_subs_epu8 (__m128i __W, __mmask16 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psubusb128_mask ((__v16qi) __A,
         (__v16qi) __B,
         (__v16qi) __W,
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_subs_epu8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubusb128_mask ((__v16qi) __A,
         (__v16qi) __B,
         (__v16qi)
         _mm_setzero_si128 (),
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_subs_epu16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_psubusw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_subs_epu16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubusw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srl_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psrlw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srl_epi16 (__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psrlw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srl_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srl_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sra_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psraw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sra_epi16 (__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psraw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sra_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psraw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sra_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psraw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_adds_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddsw128_mask ((__v8hi) __A,
        (__v8hi) __B,
        (__v8hi)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_adds_epu8 (__m128i __W, __mmask16 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_paddusb128_mask ((__v16qi) __A,
         (__v16qi) __B,
         (__v16qi) __W,
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_adds_epu8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddusb128_mask ((__v16qi) __A,
         (__v16qi) __B,
         (__v16qi)
         _mm_setzero_si128 (),
         (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_adds_epu16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_paddusw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi) __W,
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_adds_epu16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddusw128_mask ((__v8hi) __A,
         (__v8hi) __B,
         (__v8hi)
         _mm_setzero_si128 (),
         (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
     __m128i __B)
{
  return (__m128i) __builtin_ia32_psubb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi) __W,
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__v16qi)
       _mm_setzero_si128 (),
       (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sub_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psubw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sub_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psubw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_adds_epi8 (__m128i __W, __mmask16 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_paddsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi) __W,
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_adds_epi8 (__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_paddsb128_mask ((__v16qi) __A,
        (__v16qi) __B,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi16_epi8 (__m128i __A)
{

  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
        (__v16qi)_mm_undefined_si128(),
        (__mmask8) -1);
}

extern __inline void
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi16_storeu_epi8 (void * __P, __mmask8 __M,__m128i __A)
{
  __builtin_ia32_pmovwb128mem_mask ((__v8qi *) __P , (__v8hi) __A, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
        (__v16qi) __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi16_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srav_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psrav16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srav_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_psrav16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srav_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psrav16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srav_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrav8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srav_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_psrav8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srav_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrav8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_srlv_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psrlv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_srlv_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_psrlv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_srlv_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psrlv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_srlv_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_srlv_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_srlv_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psrlv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_sllv_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psllv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sllv_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
   __m256i __B)
{
  return (__m256i) __builtin_ia32_psllv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sllv_epi16 (__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_psllv16hi_mask ((__v16hi) __A,
        (__v16hi) __B,
        (__v16hi)
        _mm256_setzero_si256 (),
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sllv_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psllv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sllv_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_psllv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sllv_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psllv8hi_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_sll_epi16 (__m128i __W, __mmask8 __U, __m128i __A,
      __m128i __B)
{
  return (__m128i) __builtin_ia32_psllw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_sll_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_psllw128_mask ((__v8hi) __A,
       (__v8hi) __B,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_sll_epi16 (__m256i __W, __mmask16 __U, __m256i __A,
         __m128i __B)
{
  return (__m256i) __builtin_ia32_psllw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_sll_epi16 (__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i) __builtin_ia32_psllw256_mask ((__v16hi) __A,
       (__v8hi) __B,
       (__v16hi)
       _mm256_setzero_si256 (),
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_packus_epi32 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_packusdw256_mask ((__v8si) __A,
          (__v8si) __B,
          (__v16hi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_packus_epi32 (__m256i __W, __mmask16 __M, __m256i __A,
     __m256i __B)
{
  return (__m256i) __builtin_ia32_packusdw256_mask ((__v8si) __A,
          (__v8si) __B,
          (__v16hi) __W,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_packus_epi32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_packusdw128_mask ((__v4si) __A,
          (__v4si) __B,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_packus_epi32 (__m128i __W, __mmask8 __M, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_packusdw128_mask ((__v4si) __A,
          (__v4si) __B,
          (__v8hi) __W, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_packs_epi32 (__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_packssdw256_mask ((__v8si) __A,
          (__v8si) __B,
          (__v16hi)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_packs_epi32 (__m256i __W, __mmask16 __M, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_packssdw256_mask ((__v8si) __A,
          (__v8si) __B,
          (__v16hi) __W,
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_packs_epi32 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_packssdw128_mask ((__v4si) __A,
          (__v4si) __B,
          (__v8hi)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_packs_epi32 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_packssdw128_mask ((__v4si) __A,
          (__v4si) __B,
          (__v8hi) __W, __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epu8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 4,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epu8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 1,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epu8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 5,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epu8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpb128_mask ((__v16qi) __X,
         (__v16qi) __Y, 2,
         (__mmask16) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epu16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 4,
        (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epu16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 1,
        (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epu16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 5,
        (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epu16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_ucmpw128_mask ((__v8hi) __X,
        (__v8hi) __Y, 2,
        (__mmask8) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epi8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 4,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epi8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 1,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epi8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 5,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epi8_mask (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__mmask16) __builtin_ia32_cmpb128_mask ((__v16qi) __X,
        (__v16qi) __Y, 2,
        (__mmask16) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpneq_epi16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 4,
       (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmplt_epi16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 1,
       (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmpge_epi16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 5,
       (__mmask8) __M);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cmple_epi16_mask (__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__mmask8) __builtin_ia32_cmpw128_mask ((__v8hi) __X,
       (__v8hi) __Y, 2,
       (__mmask8) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epu8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 4,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epu8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 1,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epu8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 5,
         (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epu8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi) __X,
         (__v32qi) __Y, 2,
         (__mmask32) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epu16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 4,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epu16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 1,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epu16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 5,
         (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epu16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_ucmpw256_mask ((__v16hi) __X,
         (__v16hi) __Y, 2,
         (__mmask16) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epi8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 4,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epi8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 1,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epi8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 5,
        (__mmask32) __M);
}

extern __inline __mmask32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epi8_mask (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi) __X,
        (__v32qi) __Y, 2,
        (__mmask32) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpneq_epi16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 4,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmplt_epi16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 1,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmpge_epi16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 5,
        (__mmask16) __M);
}

extern __inline __mmask16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cmple_epi16_mask (__mmask16 __M, __m256i __X, __m256i __Y)
{
  return (__mmask16) __builtin_ia32_cmpw256_mask ((__v16hi) __X,
        (__v16hi) __Y, 2,
        (__mmask16) __M);
}



#pragma GCC pop_options
# 70 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vldqintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vldqintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vl,avx512dq")



extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttpd_epi64 (__m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttpd_epi64 (__mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttpd_epi64 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttpd_epi64 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttpd_epu64 (__m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
            (__v4di) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttpd_epu64 (__mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttpd_epu64 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
            (__v2di) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttpd_epu64 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtpd_epi64 (__m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtpd_epi64 (__mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_epi64 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtpd_epi64 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtpd_epu64 (__m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtpd_epu64 (__mmask8 __U, __m256d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtpd_epu64 (__m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtpd_epu64 (__mmask8 __U, __m128d __A)
{
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttps_epi64 (__m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttps_epi64 (__m256i __W, __mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttps_epi64 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttps_epi64 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvttps_epu64 (__m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvttps_epu64 (__m256i __W, __mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
            (__v4di) __W,
            (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
            (__v4di)
            _mm256_setzero_si256 (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvttps_epu64 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvttps_epu64 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
            (__v2di) __W,
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
            (__v2di)
            _mm_setzero_si128 (),
            (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_f64x2 (__m128d __A)
{
  return (__m256d) __builtin_ia32_broadcastf64x2_256_mask ((__v2df)
          __A,
                 (__v4df)_mm256_undefined_pd(),
          (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_f64x2 (__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d) __builtin_ia32_broadcastf64x2_256_mask ((__v2df)
          __A,
          (__v4df)
          __O, __M);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_f64x2 (__mmask8 __M, __m128d __A)
{
  return (__m256d) __builtin_ia32_broadcastf64x2_256_mask ((__v2df)
          __A,
          (__v4df)
          _mm256_setzero_ps (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_i64x2 (__m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti64x2_256_mask ((__v2di)
          __A,
                 (__v4di)_mm256_undefined_si256(),
          (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_i64x2 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti64x2_256_mask ((__v2di)
          __A,
          (__v4di)
          __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_i64x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti64x2_256_mask ((__v2di)
          __A,
          (__v4di)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_f32x2 (__m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x2_256_mask ((__v4sf) __A,
                (__v8sf)_mm256_undefined_ps(),
         (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_f32x2 (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x2_256_mask ((__v4sf) __A,
         (__v8sf) __O,
         __M);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_f32x2 (__mmask8 __M, __m128 __A)
{
  return (__m256) __builtin_ia32_broadcastf32x2_256_mask ((__v4sf) __A,
         (__v8sf)
         _mm256_setzero_ps (),
         __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_broadcast_i32x2 (__m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x2_256_mask ((__v4si)
          __A,
                (__v8si)_mm256_undefined_si256(),
          (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_broadcast_i32x2 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x2_256_mask ((__v4si)
          __A,
          (__v8si)
          __O, __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i) __builtin_ia32_broadcasti32x2_256_mask ((__v4si)
          __A,
          (__v8si)
          _mm256_setzero_si256 (),
          __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_broadcast_i32x2 (__m128i __A)
{
  return (__m128i) __builtin_ia32_broadcasti32x2_128_mask ((__v4si)
          __A,
                (__v4si)_mm_undefined_si128(),
          (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_broadcast_i32x2 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_broadcasti32x2_128_mask ((__v4si)
          __A,
          (__v4si)
          __O, __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_broadcasti32x2_128_mask ((__v4si)
          __A,
          (__v4si)
          _mm_setzero_si128 (),
          __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mullo_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) ((__v4du) __A * (__v4du) __B);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_mullo_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_pmullq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di) __W,
        (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_mullo_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_pmullq256_mask ((__v4di) __A,
        (__v4di) __B,
        (__v4di)
        _mm256_setzero_si256 (),
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mullo_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) ((__v2du) __A * (__v2du) __B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_mullo_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_pmullq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di) __W,
        (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_mullo_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_pmullq128_mask ((__v2di) __A,
        (__v2di) __B,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_andnot_pd (__m256d __W, __mmask8 __U, __m256d __A,
         __m256d __B)
{
  return (__m256d) __builtin_ia32_andnpd256_mask ((__v4df) __A,
        (__v4df) __B,
        (__v4df) __W,
        (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_andnot_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_andnpd256_mask ((__v4df) __A,
        (__v4df) __B,
        (__v4df)
        _mm256_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_andnot_pd (__m128d __W, __mmask8 __U, __m128d __A,
      __m128d __B)
{
  return (__m128d) __builtin_ia32_andnpd128_mask ((__v2df) __A,
        (__v2df) __B,
        (__v2df) __W,
        (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_andnot_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_andnpd128_mask ((__v2df) __A,
        (__v2df) __B,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_andnot_ps (__m256 __W, __mmask8 __U, __m256 __A,
         __m256 __B)
{
  return (__m256) __builtin_ia32_andnps256_mask ((__v8sf) __A,
       (__v8sf) __B,
       (__v8sf) __W,
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_andnot_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_andnps256_mask ((__v8sf) __A,
       (__v8sf) __B,
       (__v8sf)
       _mm256_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_andnot_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_andnps128_mask ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf) __W,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_andnot_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_andnps128_mask ((__v4sf) __A,
       (__v4sf) __B,
       (__v4sf)
       _mm_setzero_ps (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_epi64 (__m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_epi64 (__m256i __W, __mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
          (__v4di) __W,
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
          (__v4di)
          _mm256_setzero_si256 (),
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_epi64 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_epi64 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
          (__v2di) __W,
          (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
          (__v2di)
          _mm_setzero_si128 (),
          (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_epu64 (__m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtps_epu64 (__m256i __W, __mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
           (__v4di) __W,
           (__mmask8) __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A)
{
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
           (__v4di)
           _mm256_setzero_si256 (),
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_epu64 (__m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtps_epu64 (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
           (__v2di) __W,
           (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
           (__v2di)
           _mm_setzero_si128 (),
           (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi64_ps (__m256i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m256i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi64_ps (__mmask8 __U, __m256i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi64_ps (__m128i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi64_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu64_ps (__m256i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m256i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu64_ps (__mmask8 __U, __m256i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu64_ps (__m128i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
          (__v4sf) __W,
          (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu64_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
          (__v4sf)
          _mm_setzero_ps (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepi64_pd (__m256i __A)
{
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepi64_pd (__m256d __W, __mmask8 __U, __m256i __A)
{
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepi64_pd (__mmask8 __U, __m256i __A)
{
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepi64_pd (__m128i __A)
{
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepi64_pd (__m128d __W, __mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepi64_pd (__mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtepu64_pd (__m256i __A)
{
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtepu64_pd (__m256d __W, __mmask8 __U, __m256i __A)
{
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
           (__v4df) __W,
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtepu64_pd (__mmask8 __U, __m256i __A)
{
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
           (__v4df)
           _mm256_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_and_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_andpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_and_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_andpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_and_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_andpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_and_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_andpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_and_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_andps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_and_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_andps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_and_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_andps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_and_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_andps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtepu64_pd (__m128i __A)
{
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtepu64_pd (__m128d __W, __mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
           (__v2df) __W,
           (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtepu64_pd (__mmask8 __U, __m128i __A)
{
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
           (__v2df)
           _mm_setzero_pd (),
           (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_xor_pd (__m256d __W, __mmask8 __U, __m256d __A,
      __m256d __B)
{
  return (__m256d) __builtin_ia32_xorpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df) __W,
       (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_xor_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_xorpd256_mask ((__v4df) __A,
       (__v4df) __B,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_xor_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_xorpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df) __W,
       (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_xor_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_xorpd128_mask ((__v2df) __A,
       (__v2df) __B,
       (__v2df)
       _mm_setzero_pd (),
       (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_xor_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_xorps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf) __W,
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_xor_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_xorps256_mask ((__v8sf) __A,
      (__v8sf) __B,
      (__v8sf)
      _mm256_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_xor_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_xorps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf) __W,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_xor_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_xorps128_mask ((__v4sf) __A,
      (__v4sf) __B,
      (__v4sf)
      _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_or_pd (__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_orpd256_mask ((__v4df) __A,
      (__v4df) __B,
      (__v4df) __W,
      (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_or_pd (__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d) __builtin_ia32_orpd256_mask ((__v4df) __A,
      (__v4df) __B,
      (__v4df)
      _mm256_setzero_pd (),
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_or_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_orpd128_mask ((__v2df) __A,
      (__v2df) __B,
      (__v2df) __W,
      (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_or_pd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_orpd128_mask ((__v2df) __A,
      (__v2df) __B,
      (__v2df)
      _mm_setzero_pd (),
      (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_or_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_orps256_mask ((__v8sf) __A,
            (__v8sf) __B,
            (__v8sf) __W,
            (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_or_ps (__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256) __builtin_ia32_orps256_mask ((__v8sf) __A,
            (__v8sf) __B,
            (__v8sf)
            _mm256_setzero_ps (),
            (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_or_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_orps128_mask ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf) __W,
            (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_or_ps (__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_orps128_mask ((__v4sf) __A,
            (__v4sf) __B,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movm_epi32 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2d128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movm_epi32 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2d256 (__A);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movm_epi64 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2q128 (__A);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movm_epi64 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2q256 (__A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movepi32_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask128 ((__v4si) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movepi32_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask256 ((__v8si) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_movepi64_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask128 ((__v2di) __A);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_movepi64_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask256 ((__v4di) __A);
}


extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extractf64x2_pd (__m256d __A, const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_256_mask ((__v4df) __A,
        __imm,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_extractf64x2_pd (__m128d __W, __mmask8 __U, __m256d __A,
        const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_256_mask ((__v4df) __A,
        __imm,
        (__v2df) __W,
        (__mmask8)
        __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_extractf64x2_pd (__mmask8 __U, __m256d __A,
         const int __imm)
{
  return (__m128d) __builtin_ia32_extractf64x2_256_mask ((__v4df) __A,
        __imm,
        (__v2df)
        _mm_setzero_pd (),
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_extracti64x2_epi64 (__m256i __A, const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di) __A,
        __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_extracti64x2_epi64 (__m128i __W, __mmask8 __U, __m256i __A,
    const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di) __A,
        __imm,
        (__v2di) __W,
        (__mmask8)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_extracti64x2_epi64 (__mmask8 __U, __m256i __A,
     const int __imm)
{
  return (__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di) __A,
        __imm,
        (__v2di)
        _mm_setzero_si128 (),
        (__mmask8)
        __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_reduce_pd (__m256d __A, int __B)
{
  return (__m256d) __builtin_ia32_reducepd256_mask ((__v4df) __A, __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_reduce_pd (__m256d __W, __mmask8 __U, __m256d __A, int __B)
{
  return (__m256d) __builtin_ia32_reducepd256_mask ((__v4df) __A, __B,
          (__v4df) __W,
          (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_reduce_pd (__mmask8 __U, __m256d __A, int __B)
{
  return (__m256d) __builtin_ia32_reducepd256_mask ((__v4df) __A, __B,
          (__v4df)
          _mm256_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_reduce_pd (__m128d __A, int __B)
{
  return (__m128d) __builtin_ia32_reducepd128_mask ((__v2df) __A, __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_reduce_pd (__m128d __W, __mmask8 __U, __m128d __A, int __B)
{
  return (__m128d) __builtin_ia32_reducepd128_mask ((__v2df) __A, __B,
          (__v2df) __W,
          (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_reduce_pd (__mmask8 __U, __m128d __A, int __B)
{
  return (__m128d) __builtin_ia32_reducepd128_mask ((__v2df) __A, __B,
          (__v2df)
          _mm_setzero_pd (),
          (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_reduce_ps (__m256 __A, int __B)
{
  return (__m256) __builtin_ia32_reduceps256_mask ((__v8sf) __A, __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_reduce_ps (__m256 __W, __mmask8 __U, __m256 __A, int __B)
{
  return (__m256) __builtin_ia32_reduceps256_mask ((__v8sf) __A, __B,
         (__v8sf) __W,
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_reduce_ps (__mmask8 __U, __m256 __A, int __B)
{
  return (__m256) __builtin_ia32_reduceps256_mask ((__v8sf) __A, __B,
         (__v8sf)
         _mm256_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_reduce_ps (__m128 __A, int __B)
{
  return (__m128) __builtin_ia32_reduceps128_mask ((__v4sf) __A, __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_reduce_ps (__m128 __W, __mmask8 __U, __m128 __A, int __B)
{
  return (__m128) __builtin_ia32_reduceps128_mask ((__v4sf) __A, __B,
         (__v4sf) __W,
         (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_reduce_ps (__mmask8 __U, __m128 __A, int __B)
{
  return (__m128) __builtin_ia32_reduceps128_mask ((__v4sf) __A, __B,
         (__v4sf)
         _mm_setzero_ps (),
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_range_pd (__m256d __A, __m256d __B, int __C)
{
  return (__m256d) __builtin_ia32_rangepd256_mask ((__v4df) __A,
         (__v4df) __B, __C,
         (__v4df)
         _mm256_setzero_pd (),
         (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_range_pd (__m256d __W, __mmask8 __U,
        __m256d __A, __m256d __B, int __C)
{
  return (__m256d) __builtin_ia32_rangepd256_mask ((__v4df) __A,
         (__v4df) __B, __C,
         (__v4df) __W,
         (__mmask8) __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_range_pd (__mmask8 __U, __m256d __A, __m256d __B, int __C)
{
  return (__m256d) __builtin_ia32_rangepd256_mask ((__v4df) __A,
         (__v4df) __B, __C,
         (__v4df)
         _mm256_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_pd (__m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangepd128_mask ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) -1);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_pd (__m128d __W, __mmask8 __U,
     __m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangepd128_mask ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df) __W,
         (__mmask8) __U);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_pd (__mmask8 __U, __m128d __A, __m128d __B, int __C)
{
  return (__m128d) __builtin_ia32_rangepd128_mask ((__v2df) __A,
         (__v2df) __B, __C,
         (__v2df)
         _mm_setzero_pd (),
         (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_range_ps (__m256 __A, __m256 __B, int __C)
{
  return (__m256) __builtin_ia32_rangeps256_mask ((__v8sf) __A,
        (__v8sf) __B, __C,
        (__v8sf)
        _mm256_setzero_ps (),
        (__mmask8) -1);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_range_ps (__m256 __W, __mmask8 __U, __m256 __A, __m256 __B,
        int __C)
{
  return (__m256) __builtin_ia32_rangeps256_mask ((__v8sf) __A,
        (__v8sf) __B, __C,
        (__v8sf) __W,
        (__mmask8) __U);
}

extern __inline __m256
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_range_ps (__mmask8 __U, __m256 __A, __m256 __B, int __C)
{
  return (__m256) __builtin_ia32_rangeps256_mask ((__v8sf) __A,
        (__v8sf) __B, __C,
        (__v8sf)
        _mm256_setzero_ps (),
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_range_ps (__m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangeps128_mask ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) -1);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_range_ps (__m128 __W, __mmask8 __U,
     __m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangeps128_mask ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf) __W,
        (__mmask8) __U);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_range_ps (__mmask8 __U, __m128 __A, __m128 __B, int __C)
{
  return (__m128) __builtin_ia32_rangeps128_mask ((__v4sf) __A,
        (__v4sf) __B, __C,
        (__v4sf)
        _mm_setzero_ps (),
        (__mmask8) __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fpclass_pd_mask (__mmask8 __U, __m256d __A,
        const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd256_mask ((__v4df) __A,
            __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fpclass_pd_mask (__m256d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd256_mask ((__v4df) __A,
            __imm,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_fpclass_ps_mask (__mmask8 __U, __m256 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassps256_mask ((__v8sf) __A,
            __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fpclass_ps_mask (__m256 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassps256_mask ((__v8sf) __A,
            __imm,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fpclass_pd_mask (__mmask8 __U, __m128d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd128_mask ((__v2df) __A,
            __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fpclass_pd_mask (__m128d __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclasspd128_mask ((__v2df) __A,
            __imm,
            (__mmask8) -1);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_fpclass_ps_mask (__mmask8 __U, __m128 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassps128_mask ((__v4sf) __A,
            __imm, __U);
}

extern __inline __mmask8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_fpclass_ps_mask (__m128 __A, const int __imm)
{
  return (__mmask8) __builtin_ia32_fpclassps128_mask ((__v4sf) __A,
            __imm,
            (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_inserti64x2 (__m256i __A, __m128i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_inserti64x2_256_mask ((__v4di) __A,
       (__v2di) __B,
       __imm,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_inserti64x2 (__m256i __W, __mmask8 __U, __m256i __A,
    __m128i __B, const int __imm)
{
  return (__m256i) __builtin_ia32_inserti64x2_256_mask ((__v4di) __A,
       (__v2di) __B,
       __imm,
       (__v4di) __W,
       (__mmask8)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_inserti64x2 (__mmask8 __U, __m256i __A, __m128i __B,
     const int __imm)
{
  return (__m256i) __builtin_ia32_inserti64x2_256_mask ((__v4di) __A,
       (__v2di) __B,
       __imm,
       (__v4di)
       _mm256_setzero_si256 (),
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_insertf64x2 (__m256d __A, __m128d __B, const int __imm)
{
  return (__m256d) __builtin_ia32_insertf64x2_256_mask ((__v4df) __A,
       (__v2df) __B,
       __imm,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8) -1);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_insertf64x2 (__m256d __W, __mmask8 __U, __m256d __A,
    __m128d __B, const int __imm)
{
  return (__m256d) __builtin_ia32_insertf64x2_256_mask ((__v4df) __A,
       (__v2df) __B,
       __imm,
       (__v4df) __W,
       (__mmask8)
       __U);
}

extern __inline __m256d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_insertf64x2 (__mmask8 __U, __m256d __A, __m128d __B,
     const int __imm)
{
  return (__m256d) __builtin_ia32_insertf64x2_256_mask ((__v4df) __A,
       (__v2df) __B,
       __imm,
       (__v4df)
       _mm256_setzero_pd (),
       (__mmask8)
       __U);
}
# 2013 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vldqintrin.h" 3 4
#pragma GCC pop_options
# 72 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512ifmaintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512ifmaintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512ifma")



extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_madd52lo_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_mask ((__v8di) __X,
             (__v8di) __Y,
             (__v8di) __Z,
             (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_madd52hi_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_mask ((__v8di) __X,
             (__v8di) __Y,
             (__v8di) __Z,
             (__mmask8) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_madd52lo_epu64 (__m512i __W, __mmask8 __M, __m512i __X,
       __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_mask ((__v8di) __W,
             (__v8di) __X,
             (__v8di) __Y,
             (__mmask8) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_madd52hi_epu64 (__m512i __W, __mmask8 __M, __m512i __X,
       __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_mask ((__v8di) __W,
             (__v8di) __X,
             (__v8di) __Y,
             (__mmask8) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_madd52lo_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_maskz ((__v8di) __X,
       (__v8di) __Y,
       (__v8di) __Z,
       (__mmask8) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_madd52hi_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_maskz ((__v8di) __X,
       (__v8di) __Y,
       (__v8di) __Z,
       (__mmask8) __M);
}



#pragma GCC pop_options
# 74 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512ifmavlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512ifmavlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512ifma,avx512vl")



extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_madd52lo_epu64 (__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_mask ((__v2di) __X,
             (__v2di) __Y,
             (__v2di) __Z,
             (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_madd52hi_epu64 (__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_mask ((__v2di) __X,
             (__v2di) __Y,
             (__v2di) __Z,
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_madd52lo_epu64 (__m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_mask ((__v4di) __X,
             (__v4di) __Y,
             (__v4di) __Z,
             (__mmask8) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_madd52hi_epu64 (__m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_mask ((__v4di) __X,
             (__v4di) __Y,
             (__v4di) __Z,
             (__mmask8) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_madd52lo_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_mask ((__v2di) __W,
             (__v2di) __X,
             (__v2di) __Y,
             (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_madd52hi_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_mask ((__v2di) __W,
             (__v2di) __X,
             (__v2di) __Y,
             (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_madd52lo_epu64 (__m256i __W, __mmask8 __M, __m256i __X,
       __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_mask ((__v4di) __W,
             (__v4di) __X,
             (__v4di) __Y,
             (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_madd52hi_epu64 (__m256i __W, __mmask8 __M, __m256i __X,
       __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_mask ((__v4di) __W,
             (__v4di) __X,
             (__v4di) __Y,
             (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_madd52lo_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_maskz ((__v2di) __X,
       (__v2di) __Y,
       (__v2di) __Z,
       (__mmask8) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_madd52hi_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_maskz ((__v2di) __X,
       (__v2di) __Y,
       (__v2di) __Z,
       (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_madd52lo_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_maskz ((__v4di) __X,
       (__v4di) __Y,
       (__v4di) __Z,
       (__mmask8) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_madd52hi_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_maskz ((__v4di) __X,
       (__v4di) __Y,
       (__v4di) __Z,
       (__mmask8) __M);
}



#pragma GCC pop_options
# 76 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmiintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmiintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vbmi")



extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_multishift_epi64_epi8 (__m512i __W, __mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
         (__v64qi) __Y,
         (__v64qi) __W,
         (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_multishift_epi64_epi8 (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
         (__v64qi) __Y,
         (__v64qi)
         _mm512_setzero_si512 (),
         (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_multishift_epi64_epi8 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
         (__v64qi) __Y,
         (__v64qi)
         _mm512_undefined_epi32 (),
         (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutexvar_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
           (__v64qi) __A,
           (__v64qi)
           _mm512_undefined_epi32 (),
           (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutexvar_epi8 (__mmask64 __M, __m512i __A,
    __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
           (__v64qi) __A,
           (__v64qi)
           _mm512_setzero_si512(),
           (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutexvar_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
           (__v64qi) __A,
           (__v64qi) __W,
           (__mmask64) __M);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_permutex2var_epi8 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_mask ((__v64qi) __I
                 ,
       (__v64qi) __A,
       (__v64qi) __B,
       (__mmask64) -1);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_permutex2var_epi8 (__m512i __A, __mmask64 __U,
    __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_mask ((__v64qi) __I
                 ,
       (__v64qi) __A,
       (__v64qi) __B,
       (__mmask64)
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask2_permutex2var_epi8 (__m512i __A, __m512i __I,
     __mmask64 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varqi512_mask ((__v64qi) __A,
       (__v64qi) __I
                 ,
       (__v64qi) __B,
       (__mmask64)
       __U);
}

extern __inline __m512i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_permutex2var_epi8 (__mmask64 __U, __m512i __A,
     __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_maskz ((__v64qi) __I
                  ,
        (__v64qi) __A,
        (__v64qi) __B,
        (__mmask64)
        __U);
}



#pragma GCC pop_options
# 78 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmivlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmivlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vbmi,avx512vl")



extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_multishift_epi64_epi8 (__m256i __W, __mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
         (__v32qi) __Y,
         (__v32qi) __W,
         (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_multishift_epi64_epi8 (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
         (__v32qi) __Y,
         (__v32qi)
         _mm256_setzero_si256 (),
         (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_multishift_epi64_epi8 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
         (__v32qi) __Y,
         (__v32qi)
         _mm256_undefined_si256 (),
         (__mmask32) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_multishift_epi64_epi8 (__m128i __W, __mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
         (__v16qi) __Y,
         (__v16qi) __W,
         (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_multishift_epi64_epi8 (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
         (__v16qi) __Y,
         (__v16qi)
         _mm_setzero_si128 (),
         (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_multishift_epi64_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
         (__v16qi) __Y,
         (__v16qi)
         _mm_undefined_si128 (),
         (__mmask16) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutexvar_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
           (__v32qi) __A,
           (__v32qi)
           _mm256_undefined_si256 (),
           (__mmask32) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutexvar_epi8 (__mmask32 __M, __m256i __A,
    __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
           (__v32qi) __A,
           (__v32qi)
           _mm256_setzero_si256 (),
           (__mmask32) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutexvar_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
          __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
           (__v32qi) __A,
           (__v32qi) __W,
           (__mmask32) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutexvar_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
           (__v16qi) __A,
           (__v16qi)
           _mm_undefined_si128 (),
           (__mmask16) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutexvar_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
           (__v16qi) __A,
           (__v16qi)
           _mm_setzero_si128 (),
           (__mmask16) __M);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutexvar_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
       __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
           (__v16qi) __A,
           (__v16qi) __W,
           (__mmask16) __M);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_permutex2var_epi8 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_mask ((__v32qi) __I
                 ,
       (__v32qi) __A,
       (__v32qi) __B,
       (__mmask32) -1);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_permutex2var_epi8 (__m256i __A, __mmask32 __U,
    __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_mask ((__v32qi) __I
                 ,
       (__v32qi) __A,
       (__v32qi) __B,
       (__mmask32)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask2_permutex2var_epi8 (__m256i __A, __m256i __I,
     __mmask32 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2varqi256_mask ((__v32qi) __A,
       (__v32qi) __I
                 ,
       (__v32qi) __B,
       (__mmask32)
       __U);
}

extern __inline __m256i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_permutex2var_epi8 (__mmask32 __U, __m256i __A,
     __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_maskz ((__v32qi) __I
                  ,
        (__v32qi) __A,
        (__v32qi) __B,
        (__mmask32)
        __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_permutex2var_epi8 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_mask ((__v16qi) __I
                 ,
       (__v16qi) __A,
       (__v16qi) __B,
       (__mmask16) -1);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_permutex2var_epi8 (__m128i __A, __mmask16 __U, __m128i __I,
        __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_mask ((__v16qi) __I
                 ,
       (__v16qi) __A,
       (__v16qi) __B,
       (__mmask16)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask2_permutex2var_epi8 (__m128i __A, __m128i __I, __mmask16 __U,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2varqi128_mask ((__v16qi) __A,
       (__v16qi) __I
                 ,
       (__v16qi) __B,
       (__mmask16)
       __U);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_permutex2var_epi8 (__mmask16 __U, __m128i __A, __m128i __I,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_maskz ((__v16qi) __I
                  ,
        (__v16qi) __A,
        (__v16qi) __B,
        (__mmask16)
        __U);
}



#pragma GCC pop_options
# 80 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx5124fmapsintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx5124fmapsintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx5124fmaps")



extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_4fmadd_ps (__m512 __A, __m512 __B, __m512 __C,
    __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fmaddps ((__v16sf) __B,
        (__v16sf) __C,
        (__v16sf) __D,
        (__v16sf) __E,
        (__v16sf) __A,
        (const __v4sf *) __F);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_4fmadd_ps (__m512 __A, __mmask16 __U, __m512 __B,
         __m512 __C, __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fmaddps_mask ((__v16sf) __B,
      (__v16sf) __C,
      (__v16sf) __D,
      (__v16sf) __E,
      (__v16sf) __A,
      (const __v4sf *) __F,
      (__v16sf) __A,
      (__mmask16) __U);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_4fmadd_ps (__mmask16 __U,
   __m512 __A, __m512 __B, __m512 __C,
   __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fmaddps_mask ((__v16sf) __B,
      (__v16sf) __C,
      (__v16sf) __D,
      (__v16sf) __E,
      (__v16sf) __A,
      (const __v4sf *) __F,
      (__v16sf) _mm512_setzero_ps (),
      (__mmask16) __U);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_4fmadd_ss (__m128 __A, __m128 __B, __m128 __C,
        __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fmaddss ((__v4sf) __B,
        (__v4sf) __C,
        (__v4sf) __D,
        (__v4sf) __E,
        (__v4sf) __A,
        (const __v4sf *) __F);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_4fmadd_ss (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C,
      __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fmaddss_mask ((__v4sf) __B,
      (__v4sf) __C,
      (__v4sf) __D,
      (__v4sf) __E,
      (__v4sf) __A,
      (const __v4sf *) __F,
      (__v4sf) __A,
      (__mmask8) __U);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_4fmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C,
       __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fmaddss_mask ((__v4sf) __B,
      (__v4sf) __C,
      (__v4sf) __D,
      (__v4sf) __E,
      (__v4sf) __A,
      (const __v4sf *) __F,
      (__v4sf) _mm_setzero_ps (),
      (__mmask8) __U);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_4fnmadd_ps (__m512 __A, __m512 __B, __m512 __C,
     __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fnmaddps ((__v16sf) __B,
         (__v16sf) __C,
         (__v16sf) __D,
         (__v16sf) __E,
         (__v16sf) __A,
         (const __v4sf *) __F);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_4fnmadd_ps (__m512 __A, __mmask16 __U, __m512 __B,
   __m512 __C, __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fnmaddps_mask ((__v16sf) __B,
       (__v16sf) __C,
       (__v16sf) __D,
       (__v16sf) __E,
       (__v16sf) __A,
       (const __v4sf *) __F,
       (__v16sf) __A,
       (__mmask16) __U);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_4fnmadd_ps (__mmask16 __U,
    __m512 __A, __m512 __B, __m512 __C,
    __m512 __D, __m512 __E, __m128 *__F)
{
  return (__m512) __builtin_ia32_4fnmaddps_mask ((__v16sf) __B,
       (__v16sf) __C,
       (__v16sf) __D,
       (__v16sf) __E,
       (__v16sf) __A,
       (const __v4sf *) __F,
       (__v16sf) _mm512_setzero_ps (),
       (__mmask16) __U);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_4fnmadd_ss (__m128 __A, __m128 __B, __m128 __C,
  __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fnmaddss ((__v4sf) __B,
         (__v4sf) __C,
         (__v4sf) __D,
         (__v4sf) __E,
         (__v4sf) __A,
         (const __v4sf *) __F);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_4fnmadd_ss (__m128 __A, __mmask8 __U, __m128 __B, __m128 __C,
       __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fnmaddss_mask ((__v4sf) __B,
       (__v4sf) __C,
       (__v4sf) __D,
       (__v4sf) __E,
       (__v4sf) __A,
       (const __v4sf *) __F,
       (__v4sf) __A,
       (__mmask8) __U);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_4fnmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C,
        __m128 __D, __m128 __E, __m128 *__F)
{
  return (__m128) __builtin_ia32_4fnmaddss_mask ((__v4sf) __B,
       (__v4sf) __C,
       (__v4sf) __D,
       (__v4sf) __E,
       (__v4sf) __A,
       (const __v4sf *) __F,
       (__v4sf) _mm_setzero_ps (),
       (__mmask8) __U);
}



#pragma GCC pop_options
# 82 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx5124vnniwintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx5124vnniwintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx5124vnniw")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_4dpwssd_epi32 (__m512i __A, __m512i __B, __m512i __C,
        __m512i __D, __m512i __E, __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssd ((__v16si) __B,
          (__v16si) __C,
          (__v16si) __D,
          (__v16si) __E,
          (__v16si) __A,
          (const __v4si *) __F);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_4dpwssd_epi32 (__m512i __A, __mmask16 __U, __m512i __B,
      __m512i __C, __m512i __D, __m512i __E,
      __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssd_mask ((__v16si) __B,
        (__v16si) __C,
        (__v16si) __D,
        (__v16si) __E,
        (__v16si) __A,
        (const __v4si *) __F,
        (__v16si) __A,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_4dpwssd_epi32 (__mmask16 __U, __m512i __A, __m512i __B,
       __m512i __C, __m512i __D, __m512i __E,
       __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssd_mask ((__v16si) __B,
        (__v16si) __C,
        (__v16si) __D,
        (__v16si) __E,
        (__v16si) __A,
        (const __v4si *) __F,
        (__v16si) _mm512_setzero_ps (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_4dpwssds_epi32 (__m512i __A, __m512i __B, __m512i __C,
         __m512i __D, __m512i __E, __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssds ((__v16si) __B,
           (__v16si) __C,
           (__v16si) __D,
           (__v16si) __E,
           (__v16si) __A,
           (const __v4si *) __F);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_4dpwssds_epi32 (__m512i __A, __mmask16 __U, __m512i __B,
       __m512i __C, __m512i __D, __m512i __E,
       __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssds_mask ((__v16si) __B,
         (__v16si) __C,
         (__v16si) __D,
         (__v16si) __E,
         (__v16si) __A,
         (const __v4si *) __F,
         (__v16si) __A,
         (__mmask16) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_4dpwssds_epi32 (__mmask16 __U, __m512i __A, __m512i __B,
        __m512i __C, __m512i __D, __m512i __E,
        __m128i *__F)
{
  return (__m512i) __builtin_ia32_vp4dpwssds_mask ((__v16si) __B,
         (__v16si) __C,
         (__v16si) __D,
         (__v16si) __E,
         (__v16si) __A,
         (const __v4si *) __F,
         (__v16si) _mm512_setzero_ps (),
         (__mmask16) __U);
}



#pragma GCC pop_options
# 84 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vpopcntdqintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vpopcntdqintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vpopcntdq")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_popcnt_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountd_v16si ((__v16si) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_popcnt_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountd_v16si_mask ((__v16si) __A,
        (__v16si) __W,
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_popcnt_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountd_v16si_mask ((__v16si) __A,
        (__v16si)
        _mm512_setzero_si512 (),
        (__mmask16) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_popcnt_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountq_v8di ((__v8di) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_popcnt_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountq_v8di_mask ((__v8di) __A,
       (__v8di) __W,
       (__mmask8) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_popcnt_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountq_v8di_mask ((__v8di) __A,
       (__v8di)
       _mm512_setzero_si512 (),
       (__mmask8) __U);
}



#pragma GCC pop_options
# 86 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2intrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2intrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vbmi2")




extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdi_epi16 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshrd_v32hi ((__v32hi)__A, (__v32hi) __B,
         __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdi_epi32 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshrd_v16si ((__v16si)__A, (__v16si) __B,
         __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdi_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshrd_v16si_mask ((__v16si)__C,
   (__v16si) __D, __E, (__v16si) __A, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdi_epi32 (__mmask16 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshrd_v16si_mask ((__v16si)__B,
 (__v16si) __C, __D, (__v16si) _mm512_setzero_si512 (), (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdi_epi64 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshrd_v8di ((__v8di)__A, (__v8di) __B, __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdi_epi64 (__m512i __A, __mmask8 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshrd_v8di_mask ((__v8di)__C, (__v8di) __D,
     __E, (__v8di) __A, (__mmask8)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdi_epi64 (__mmask8 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshrd_v8di_mask ((__v8di)__B, (__v8di) __C,
   __D, (__v8di) _mm512_setzero_si512 (), (__mmask8)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldi_epi16 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshld_v32hi ((__v32hi)__A, (__v32hi) __B,
         __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldi_epi32 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshld_v16si ((__v16si)__A, (__v16si) __B,
         __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldi_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshld_v16si_mask ((__v16si)__C,
   (__v16si) __D, __E, (__v16si) __A, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldi_epi32 (__mmask16 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshld_v16si_mask ((__v16si)__B,
 (__v16si) __C, __D, (__v16si) _mm512_setzero_si512 (), (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldi_epi64 (__m512i __A, __m512i __B, int __C)
{
  return (__m512i) __builtin_ia32_vpshld_v8di ((__v8di)__A, (__v8di) __B, __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldi_epi64 (__m512i __A, __mmask8 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshld_v8di_mask ((__v8di)__C, (__v8di) __D,
     __E, (__v8di) __A, (__mmask8)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldi_epi64 (__mmask8 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshld_v8di_mask ((__v8di)__B, (__v8di) __C,
   __D, (__v8di) _mm512_setzero_si512 (), (__mmask8)__A);
}
# 218 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2intrin.h" 3 4
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdv_epi16 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshrdv_v32hi ((__v32hi)__A, (__v32hi) __B,
        (__v32hi) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdv_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshrdv_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdv_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdv_epi32 (__mmask16 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shrdv_epi64 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshrdv_v8di ((__v8di)__A, (__v8di) __B,
        (__v8di) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdv_epi64 (__m512i __A, __mmask8 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v8di_mask ((__v8di)__A, (__v8di) __C,
      (__v8di) __D, (__mmask8)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdv_epi64 (__mmask8 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v8di_maskz ((__v8di)__B, (__v8di) __C,
       (__v8di) __D, (__mmask8)__A);
}
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldv_epi16 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshldv_v32hi ((__v32hi)__A, (__v32hi) __B,
        (__v32hi) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldv_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshldv_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldv_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldv_epi32 (__mmask16 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_shldv_epi64 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpshldv_v8di ((__v8di)__A, (__v8di) __B,
        (__v8di) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldv_epi64 (__m512i __A, __mmask8 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v8di_mask ((__v8di)__A, (__v8di) __C,
      (__v8di) __D, (__mmask8)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldv_epi64 (__mmask8 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v8di_maskz ((__v8di)__B, (__v8di) __C,
      (__v8di) __D, (__mmask8)__A);
}




#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("avx512vbmi2,avx512bw")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_epi8 (__m512i __A, __mmask64 __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi)__C,
      (__v64qi)__A, (__mmask64)__B);
}


extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_epi8 (__mmask64 __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi)__B,
   (__v64qi)_mm512_setzero_si512 (), (__mmask64)__A);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_epi8 (void * __A, __mmask64 __B, __m512i __C)
{
  __builtin_ia32_compressstoreuqi512_mask ((__v64qi *) __A, (__v64qi) __C,
       (__mmask64) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compress_epi16 (__m512i __A, __mmask32 __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi)__C,
      (__v32hi)__A, (__mmask32)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_compress_epi16 (__mmask32 __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi)__B,
   (__v32hi)_mm512_setzero_si512 (), (__mmask32)__A);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_compressstoreu_epi16 (void * __A, __mmask32 __B, __m512i __C)
{
  __builtin_ia32_compressstoreuhi512_mask ((__v32hi *) __A, (__v32hi) __C,
       (__mmask32) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_epi8 (__m512i __A, __mmask64 __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __C,
          (__v64qi) __A,
          (__mmask64) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_epi8 (__mmask64 __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_expandqi512_maskz ((__v64qi) __B,
   (__v64qi) _mm512_setzero_si512 (), (__mmask64) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_epi8 (__m512i __A, __mmask64 __B, const void * __C)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((const __v64qi *) __C,
     (__v64qi) __A, (__mmask64) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_epi8 (__mmask64 __A, const void * __B)
{
  return (__m512i) __builtin_ia32_expandloadqi512_maskz ((const __v64qi *) __B,
   (__v64qi) _mm512_setzero_si512 (), (__mmask64) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expand_epi16 (__m512i __A, __mmask32 __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __C,
          (__v32hi) __A,
          (__mmask32) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expand_epi16 (__mmask32 __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_expandhi512_maskz ((__v32hi) __B,
   (__v32hi) _mm512_setzero_si512 (), (__mmask32) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_expandloadu_epi16 (__m512i __A, __mmask32 __B, const void * __C)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((const __v32hi *) __C,
     (__v32hi) __A, (__mmask32) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_expandloadu_epi16 (__mmask32 __A, const void * __B)
{
  return (__m512i) __builtin_ia32_expandloadhi512_maskz ((const __v32hi *) __B,
   (__v32hi) _mm512_setzero_si512 (), (__mmask32) __A);
}


extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdi_epi16 (__m512i __A, __mmask32 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshrd_v32hi_mask ((__v32hi)__C,
   (__v32hi) __D, __E, (__v32hi) __A, (__mmask32)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdi_epi16 (__mmask32 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshrd_v32hi_mask ((__v32hi)__B,
 (__v32hi) __C, __D, (__v32hi) _mm512_setzero_si512 (), (__mmask32)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldi_epi16 (__m512i __A, __mmask32 __B, __m512i __C, __m512i __D,
        int __E)
{
  return (__m512i)__builtin_ia32_vpshld_v32hi_mask ((__v32hi)__C,
   (__v32hi) __D, __E, (__v32hi) __A, (__mmask32)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldi_epi16 (__mmask32 __A, __m512i __B, __m512i __C, int __D)
{
  return (__m512i)__builtin_ia32_vpshld_v32hi_mask ((__v32hi)__B,
 (__v32hi) __C, __D, (__v32hi) _mm512_setzero_si512 (), (__mmask32)__A);
}
# 519 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2intrin.h" 3 4
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shrdv_epi16 (__m512i __A, __mmask32 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v32hi_mask ((__v32hi)__A,
    (__v32hi) __C, (__v32hi) __D, (__mmask32)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shrdv_epi16 (__mmask32 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshrdv_v32hi_maskz ((__v32hi)__B,
    (__v32hi) __C, (__v32hi) __D, (__mmask32)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_shldv_epi16 (__m512i __A, __mmask32 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v32hi_mask ((__v32hi)__A,
    (__v32hi) __C, (__v32hi) __D, (__mmask32)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_shldv_epi16 (__mmask32 __A, __m512i __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpshldv_v32hi_maskz ((__v32hi)__B,
    (__v32hi) __C, (__v32hi) __D, (__mmask32)__A);
}




#pragma GCC pop_options
# 88 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2vlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2vlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vbmi2,avx512vl")



extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_epi8 (__m128i __A, __mmask16 __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi)__C,
      (__v16qi)__A, (__mmask16)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_epi8 (__mmask16 __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __B,
   (__v16qi) _mm_setzero_si128 (), (__mmask16) __A);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_epi16 (void * __A, __mmask16 __B, __m256i __C)
{
  __builtin_ia32_compressstoreuhi256_mask ((__v16hi *) __A, (__v16hi) __C,
       (__mmask16) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compress_epi16 (__m128i __A, __mmask8 __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi)__C, (__v8hi)__A,
        (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_compress_epi16 (__mmask8 __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __B,
    (__v8hi) _mm_setzero_si128 (), (__mmask8) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_epi16 (__m256i __A, __mmask16 __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi)__C,
      (__v16hi)__A, (__mmask16)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_epi16 (__mmask16 __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __B,
   (__v16hi) _mm256_setzero_si256 (), (__mmask16) __A);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_epi8 (void * __A, __mmask16 __B, __m128i __C)
{
  __builtin_ia32_compressstoreuqi128_mask ((__v16qi *) __A, (__v16qi) __C,
       (__mmask16) __B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_compressstoreu_epi16 (void * __A, __mmask8 __B, __m128i __C)
{
  __builtin_ia32_compressstoreuhi128_mask ((__v8hi *) __A, (__v8hi) __C,
       (__mmask8) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_epi8 (__m128i __A, __mmask16 __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __C,
          (__v16qi) __A,
          (__mmask16) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_epi8 (__mmask16 __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_expandqi128_maskz ((__v16qi) __B,
   (__v16qi) _mm_setzero_si128 (), (__mmask16) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_epi8 (__m128i __A, __mmask16 __B, const void * __C)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((const __v16qi *) __C,
     (__v16qi) __A, (__mmask16) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_epi8 (__mmask16 __A, const void * __B)
{
  return (__m128i) __builtin_ia32_expandloadqi128_maskz ((const __v16qi *) __B,
   (__v16qi) _mm_setzero_si128 (), (__mmask16) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expand_epi16 (__m128i __A, __mmask8 __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __C,
          (__v8hi) __A,
          (__mmask8) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expand_epi16 (__mmask8 __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_expandhi128_maskz ((__v8hi) __B,
    (__v8hi) _mm_setzero_si128 (), (__mmask8) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_expandloadu_epi16 (__m128i __A, __mmask8 __B, const void * __C)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((const __v8hi *) __C,
      (__v8hi) __A, (__mmask8) __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_expandloadu_epi16 (__mmask8 __A, const void * __B)
{
  return (__m128i) __builtin_ia32_expandloadhi128_maskz ((const __v8hi *) __B,
    (__v8hi) _mm_setzero_si128 (), (__mmask8) __A);
}
extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_epi16 (__m256i __A, __mmask16 __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __C,
          (__v16hi) __A,
          (__mmask16) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_epi16 (__mmask16 __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_expandhi256_maskz ((__v16hi) __B,
   (__v16hi) _mm256_setzero_si256 (), (__mmask16) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_epi16 (__m256i __A, __mmask16 __B, const void * __C)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((const __v16hi *) __C,
     (__v16hi) __A, (__mmask16) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_epi16 (__mmask16 __A, const void * __B)
{
  return (__m256i) __builtin_ia32_expandloadhi256_maskz ((const __v16hi *) __B,
   (__v16hi) _mm256_setzero_si256 (), (__mmask16) __A);
}


extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdi_epi16 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshrd_v16hi ((__v16hi)__A, (__v16hi) __B,
         __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdi_epi16 (__m256i __A, __mmask16 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshrd_v16hi_mask ((__v16hi)__C,
   (__v16hi) __D, __E, (__v16hi) __A, (__mmask16)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdi_epi16 (__mmask16 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshrd_v16hi_mask ((__v16hi)__B,
 (__v16hi) __C, __D, (__v16hi) _mm256_setzero_si256 (), (__mmask16)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdi_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshrd_v8si_mask ((__v8si)__C, (__v8si) __D,
     __E, (__v8si) __A, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdi_epi32 (__mmask8 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshrd_v8si_mask ((__v8si)__B, (__v8si) __C,
   __D, (__v8si) _mm256_setzero_si256 (), (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdi_epi32 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshrd_v8si ((__v8si)__A, (__v8si) __B, __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdi_epi64 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshrd_v4di_mask ((__v4di)__C, (__v4di) __D,
     __E, (__v4di) __A, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdi_epi64 (__mmask8 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshrd_v4di_mask ((__v4di)__B, (__v4di) __C,
   __D, (__v4di) _mm256_setzero_si256 (), (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdi_epi64 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshrd_v4di ((__v4di)__A, (__v4di) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdi_epi16 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshrd_v8hi_mask ((__v8hi)__C, (__v8hi) __D,
     __E, (__v8hi) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdi_epi16 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshrd_v8hi_mask ((__v8hi)__B, (__v8hi) __C,
   __D, (__v8hi) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdi_epi16 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshrd_v8hi ((__v8hi)__A, (__v8hi) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdi_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshrd_v4si_mask ((__v4si)__C, (__v4si) __D,
     __E, (__v4si) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdi_epi32 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshrd_v4si_mask ((__v4si)__B, (__v4si) __C,
   __D, (__v4si) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdi_epi32 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshrd_v4si ((__v4si)__A, (__v4si) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdi_epi64 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshrd_v2di_mask ((__v2di)__C, (__v2di) __D,
     __E, (__v2di) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdi_epi64 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshrd_v2di_mask ((__v2di)__B, (__v2di) __C,
   __D, (__v2di) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdi_epi64 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshrd_v2di ((__v2di)__A, (__v2di) __B, __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldi_epi16 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshld_v16hi ((__v16hi)__A, (__v16hi) __B,
         __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldi_epi16 (__m256i __A, __mmask16 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshld_v16hi_mask ((__v16hi)__C,
   (__v16hi) __D, __E, (__v16hi) __A, (__mmask16)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldi_epi16 (__mmask16 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshld_v16hi_mask ((__v16hi)__B,
 (__v16hi) __C, __D, (__v16hi) _mm256_setzero_si256 (), (__mmask16)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldi_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshld_v8si_mask ((__v8si)__C, (__v8si) __D,
     __E, (__v8si) __A, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldi_epi32 (__mmask8 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshld_v8si_mask ((__v8si)__B, (__v8si) __C,
   __D, (__v8si) _mm256_setzero_si256 (), (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldi_epi32 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshld_v8si ((__v8si)__A, (__v8si) __B, __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldi_epi64 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D,
        int __E)
{
  return (__m256i)__builtin_ia32_vpshld_v4di_mask ((__v4di)__C, (__v4di) __D,
     __E, (__v4di) __A, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldi_epi64 (__mmask8 __A, __m256i __B, __m256i __C, int __D)
{
  return (__m256i)__builtin_ia32_vpshld_v4di_mask ((__v4di)__B, (__v4di) __C,
   __D, (__v4di) _mm256_setzero_si256 (), (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldi_epi64 (__m256i __A, __m256i __B, int __C)
{
  return (__m256i) __builtin_ia32_vpshld_v4di ((__v4di)__A, (__v4di) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldi_epi16 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshld_v8hi_mask ((__v8hi)__C, (__v8hi) __D,
     __E, (__v8hi) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldi_epi16 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshld_v8hi_mask ((__v8hi)__B, (__v8hi) __C,
   __D, (__v8hi) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldi_epi16 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshld_v8hi ((__v8hi)__A, (__v8hi) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldi_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshld_v4si_mask ((__v4si)__C, (__v4si) __D,
     __E, (__v4si) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldi_epi32 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshld_v4si_mask ((__v4si)__B, (__v4si) __C,
   __D, (__v4si) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldi_epi32 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshld_v4si ((__v4si)__A, (__v4si) __B, __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldi_epi64 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D,
        int __E)
{
  return (__m128i)__builtin_ia32_vpshld_v2di_mask ((__v2di)__C, (__v2di) __D,
     __E, (__v2di) __A, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldi_epi64 (__mmask8 __A, __m128i __B, __m128i __C, int __D)
{
  return (__m128i)__builtin_ia32_vpshld_v2di_mask ((__v2di)__B, (__v2di) __C,
   __D, (__v2di) _mm_setzero_si128 (), (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldi_epi64 (__m128i __A, __m128i __B, int __C)
{
  return (__m128i) __builtin_ia32_vpshld_v2di ((__v2di)__A, (__v2di) __B, __C);
}
# 672 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vbmi2vlintrin.h" 3 4
extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdv_epi16 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshrdv_v16hi ((__v16hi)__A, (__v16hi) __B,
        (__v16hi) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdv_epi16 (__m256i __A, __mmask16 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v16hi_mask ((__v16hi)__A,
    (__v16hi) __C, (__v16hi) __D, (__mmask16)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdv_epi16 (__mmask16 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v16hi_maskz ((__v16hi)__B,
    (__v16hi) __C, (__v16hi) __D, (__mmask16)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdv_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshrdv_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdv_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v8si_mask ((__v8si)__A, (__v8si) __C,
      (__v8si) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdv_epi32 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v8si_maskz ((__v8si)__B, (__v8si) __C,
       (__v8si) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shrdv_epi64 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshrdv_v4di ((__v4di)__A, (__v4di) __B,
        (__v4di) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shrdv_epi64 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v4di_mask ((__v4di)__A, (__v4di) __C,
      (__v4di) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shrdv_epi64 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshrdv_v4di_maskz ((__v4di)__B, (__v4di) __C,
       (__v4di) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdv_epi16 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshrdv_v8hi ((__v8hi)__A, (__v8hi) __B,
        (__v8hi) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdv_epi16 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v8hi_mask ((__v8hi)__A, (__v8hi) __C,
      (__v8hi) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdv_epi16 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v8hi_maskz ((__v8hi)__B, (__v8hi) __C,
       (__v8hi) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdv_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshrdv_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdv_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v4si_mask ((__v4si)__A, (__v4si) __C,
      (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdv_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v4si_maskz ((__v4si)__B, (__v4si) __C,
       (__v4si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shrdv_epi64 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshrdv_v2di ((__v2di)__A, (__v2di) __B,
        (__v2di) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shrdv_epi64 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v2di_mask ((__v2di)__A, (__v2di) __C,
      (__v2di) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shrdv_epi64 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshrdv_v2di_maskz ((__v2di)__B, (__v2di) __C,
       (__v2di) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldv_epi16 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshldv_v16hi ((__v16hi)__A, (__v16hi) __B,
        (__v16hi) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldv_epi16 (__m256i __A, __mmask16 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v16hi_mask ((__v16hi)__A,
    (__v16hi) __C, (__v16hi) __D, (__mmask16)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldv_epi16 (__mmask16 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v16hi_maskz ((__v16hi)__B,
    (__v16hi) __C, (__v16hi) __D, (__mmask16)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldv_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshldv_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldv_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v8si_mask ((__v8si)__A, (__v8si) __C,
      (__v8si) __D, (__mmask8)__B) ;
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldv_epi32 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v8si_maskz ((__v8si)__B, (__v8si) __C,
      (__v8si) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_shldv_epi64 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpshldv_v4di ((__v4di)__A, (__v4di) __B,
        (__v4di) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_shldv_epi64 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v4di_mask ((__v4di)__A, (__v4di) __C,
      (__v4di) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_shldv_epi64 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpshldv_v4di_maskz ((__v4di)__B, (__v4di) __C,
       (__v4di) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldv_epi16 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshldv_v8hi ((__v8hi)__A, (__v8hi) __B,
        (__v8hi) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldv_epi16 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v8hi_mask ((__v8hi)__A, (__v8hi) __C,
      (__v8hi) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldv_epi16 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v8hi_maskz ((__v8hi)__B, (__v8hi) __C,
       (__v8hi) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldv_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshldv_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldv_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v4si_mask ((__v4si)__A, (__v4si) __C,
      (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldv_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v4si_maskz ((__v4si)__B, (__v4si) __C,
       (__v4si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shldv_epi64 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpshldv_v2di ((__v2di)__A, (__v2di) __B,
        (__v2di) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_shldv_epi64 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v2di_mask ((__v2di)__A, (__v2di) __C,
      (__v2di) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_shldv_epi64 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpshldv_v2di_maskz ((__v2di)__B, (__v2di) __C,
      (__v2di) __D, (__mmask8)__A);
}






#pragma GCC pop_options




#pragma GCC push_options
#pragma GCC target("avx512vbmi2,avx512vl,avx512bw")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compress_epi8 (__m256i __A, __mmask32 __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi)__C,
      (__v32qi)__A, (__mmask32)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_compress_epi8 (__mmask32 __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __B,
   (__v32qi) _mm256_setzero_si256 (), (__mmask32) __A);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_compressstoreu_epi8 (void * __A, __mmask32 __B, __m256i __C)
{
  __builtin_ia32_compressstoreuqi256_mask ((__v32qi *) __A, (__v32qi) __C,
       (__mmask32) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expand_epi8 (__m256i __A, __mmask32 __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __C,
          (__v32qi) __A,
          (__mmask32) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expand_epi8 (__mmask32 __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_expandqi256_maskz ((__v32qi) __B,
   (__v32qi) _mm256_setzero_si256 (), (__mmask32) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_expandloadu_epi8 (__m256i __A, __mmask32 __B, const void * __C)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((const __v32qi *) __C,
     (__v32qi) __A, (__mmask32) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_expandloadu_epi8 (__mmask32 __A, const void * __B)
{
  return (__m256i) __builtin_ia32_expandloadqi256_maskz ((const __v32qi *) __B,
   (__v32qi) _mm256_setzero_si256 (), (__mmask32) __A);
}



#pragma GCC pop_options
# 90 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vnniintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vnniintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vnni")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dpbusd_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpdpbusd_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dpbusd_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpbusd_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dpbusd_epi32 (__mmask16 __A, __m512i __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpbusd_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dpbusds_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpdpbusds_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dpbusds_epi32 (__m512i __A, __mmask16 __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpbusds_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dpbusds_epi32 (__mmask16 __A, __m512i __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpbusds_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dpwssd_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpdpwssd_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dpwssd_epi32 (__m512i __A, __mmask16 __B, __m512i __C, __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpwssd_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dpwssd_epi32 (__mmask16 __A, __m512i __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpwssd_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dpwssds_epi32 (__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vpdpwssds_v16si ((__v16si)__A, (__v16si) __B,
        (__v16si) __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dpwssds_epi32 (__m512i __A, __mmask16 __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpwssds_v16si_mask ((__v16si)__A,
    (__v16si) __C, (__v16si) __D, (__mmask16)__B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dpwssds_epi32 (__mmask16 __A, __m512i __B, __m512i __C,
       __m512i __D)
{
  return (__m512i)__builtin_ia32_vpdpwssds_v16si_maskz ((__v16si)__B,
    (__v16si) __C, (__v16si) __D, (__mmask16)__A);
}



#pragma GCC pop_options
# 92 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vnnivlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vnnivlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vnni,avx512vl")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dpbusd_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpdpbusd_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dpbusd_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpbusd_v8si_mask ((__v8si)__A, (__v8si) __C,
      (__v8si) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dpbusd_epi32 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpbusd_v8si_maskz ((__v8si)__B,
    (__v8si) __C, (__v8si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dpbusd_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpdpbusd_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dpbusd_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpbusd_v4si_mask ((__v4si)__A, (__v4si) __C,
      (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dpbusd_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpbusd_v4si_maskz ((__v4si)__B,
    (__v4si) __C, (__v4si) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dpbusds_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpdpbusds_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dpbusds_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpbusds_v8si_mask ((__v8si)__A,
    (__v8si) __C, (__v8si) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dpbusds_epi32 (__mmask8 __A, __m256i __B, __m256i __C,
        __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpbusds_v8si_maskz ((__v8si)__B,
    (__v8si) __C, (__v8si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dpbusds_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpdpbusds_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dpbusds_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpbusds_v4si_mask ((__v4si)__A,
    (__v4si) __C, (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dpbusds_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpbusds_v4si_maskz ((__v4si)__B,
    (__v4si) __C, (__v4si) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dpwssd_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpdpwssd_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dpwssd_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpwssd_v8si_mask ((__v8si)__A, (__v8si) __C,
      (__v8si) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dpwssd_epi32 (__mmask8 __A, __m256i __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpwssd_v8si_maskz ((__v8si)__B,
    (__v8si) __C, (__v8si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dpwssd_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpdpwssd_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dpwssd_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpwssd_v4si_mask ((__v4si)__A, (__v4si) __C,
      (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dpwssd_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpwssd_v4si_maskz ((__v4si)__B,
    (__v4si) __C, (__v4si) __D, (__mmask8)__A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dpwssds_epi32 (__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vpdpwssds_v8si ((__v8si)__A, (__v8si) __B,
        (__v8si) __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dpwssds_epi32 (__m256i __A, __mmask8 __B, __m256i __C, __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpwssds_v8si_mask ((__v8si)__A,
    (__v8si) __C, (__v8si) __D, (__mmask8)__B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dpwssds_epi32 (__mmask8 __A, __m256i __B, __m256i __C,
       __m256i __D)
{
  return (__m256i)__builtin_ia32_vpdpwssds_v8si_maskz ((__v8si)__B,
    (__v8si) __C, (__v8si) __D, (__mmask8)__A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dpwssds_epi32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vpdpwssds_v4si ((__v4si)__A, (__v4si) __B,
        (__v4si) __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dpwssds_epi32 (__m128i __A, __mmask8 __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpwssds_v4si_mask ((__v4si)__A,
    (__v4si) __C, (__v4si) __D, (__mmask8)__B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dpwssds_epi32 (__mmask8 __A, __m128i __B, __m128i __C, __m128i __D)
{
  return (__m128i)__builtin_ia32_vpdpwssds_v4si_maskz ((__v4si)__B,
    (__v4si) __C, (__v4si) __D, (__mmask8)__A);
}


#pragma GCC pop_options
# 94 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vpopcntdqvlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vpopcntdqvlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vpopcntdq,avx512vl")



extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountd_v4si ((__v4si) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_popcnt_epi32 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountd_v4si_mask ((__v4si) __A,
        (__v4si) __W,
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_popcnt_epi32 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountd_v4si_mask ((__v4si) __A,
        (__v4si)
        _mm_setzero_si128 (),
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_popcnt_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountd_v8si ((__v8si) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_popcnt_epi32 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountd_v8si_mask ((__v8si) __A,
        (__v8si) __W,
        (__mmask16) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_popcnt_epi32 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountd_v8si_mask ((__v8si) __A,
      (__v8si)
      _mm256_setzero_si256 (),
      (__mmask16) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountq_v2di ((__v2di) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_popcnt_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountq_v2di_mask ((__v2di) __A,
       (__v2di) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_popcnt_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountq_v2di_mask ((__v2di) __A,
       (__v2di)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_popcnt_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountq_v4di ((__v4di) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_popcnt_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountq_v4di_mask ((__v4di) __A,
       (__v4di) __W,
       (__mmask8) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_popcnt_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountq_v4di_mask ((__v4di) __A,
      (__v4di)
      _mm256_setzero_si256 (),
      (__mmask8) __U);
}



#pragma GCC pop_options
# 96 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bitalgintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bitalgintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512bitalg")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_popcnt_epi8 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountb_v64qi ((__v64qi) __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_popcnt_epi16 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountw_v32hi ((__v32hi) __A);
}



#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("avx512bitalg,avx512bw")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_popcnt_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountb_v64qi_mask ((__v64qi) __A,
        (__v64qi) __W,
        (__mmask64) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_popcnt_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountb_v64qi_mask ((__v64qi) __A,
      (__v64qi)
      _mm512_setzero_si512 (),
      (__mmask64) __U);
}
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_popcnt_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountw_v32hi_mask ((__v32hi) __A,
       (__v32hi) __W,
       (__mmask32) __U);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_popcnt_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcountw_v32hi_mask ((__v32hi) __A,
      (__v32hi)
      _mm512_setzero_si512 (),
      (__mmask32) __U);
}

extern __inline __mmask64
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_bitshuffle_epi64_mask (__m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_vpshufbitqmb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__mmask64) -1);
}

extern __inline __mmask64
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_bitshuffle_epi64_mask (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_vpshufbitqmb512_mask ((__v64qi) __A,
       (__v64qi) __B,
       (__mmask64) __M);
}



#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("avx512bitalg,avx512vl,avx512bw")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_popcnt_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountb_v32qi_mask ((__v32qi) __A,
        (__v32qi) __W,
        (__mmask32) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_popcnt_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountb_v32qi_mask ((__v32qi) __A,
      (__v32qi)
       _mm256_setzero_si256 (),
      (__mmask32) __U);
}

extern __inline __mmask32
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_bitshuffle_epi64_mask (__m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_vpshufbitqmb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__mmask32) -1);
}

extern __inline __mmask32
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_bitshuffle_epi64_mask (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_vpshufbitqmb256_mask ((__v32qi) __A,
       (__v32qi) __B,
       (__mmask32) __M);
}



#pragma GCC pop_options




#pragma GCC push_options
#pragma GCC target("avx512bitalg,avx512vl")



extern __inline __mmask16
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_bitshuffle_epi64_mask (__m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_vpshufbitqmb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__mmask16) -1);
}

extern __inline __mmask16
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_bitshuffle_epi64_mask (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_vpshufbitqmb128_mask ((__v16qi) __A,
       (__v16qi) __B,
       (__mmask16) __M);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_popcnt_epi8 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountb_v32qi ((__v32qi) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_popcnt_epi16 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountw_v16hi ((__v16hi) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountb_v16qi ((__v16qi) __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_popcnt_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountw_v8hi ((__v8hi) __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_popcnt_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountw_v16hi_mask ((__v16hi) __A,
       (__v16hi) __W,
       (__mmask16) __U);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_popcnt_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcountw_v16hi_mask ((__v16hi) __A,
      (__v16hi)
      _mm256_setzero_si256 (),
      (__mmask16) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_popcnt_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountb_v16qi_mask ((__v16qi) __A,
        (__v16qi) __W,
        (__mmask16) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_popcnt_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountb_v16qi_mask ((__v16qi) __A,
        (__v16qi)
        _mm_setzero_si128 (),
        (__mmask16) __U);
}
extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_popcnt_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountw_v8hi_mask ((__v8hi) __A,
       (__v8hi) __W,
       (__mmask8) __U);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_popcnt_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcountw_v8hi_mask ((__v8hi) __A,
       (__v8hi)
       _mm_setzero_si128 (),
       (__mmask8) __U);
}


#pragma GCC pop_options
# 98 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vp2intersectintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vp2intersectintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vp2intersect")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_2intersect_epi32 (__m512i __A, __m512i __B, __mmask16 *__U,
    __mmask16 *__M)
{
  __builtin_ia32_2intersectd512 (__U, __M, (__v16si) __A, (__v16si) __B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_2intersect_epi64 (__m512i __A, __m512i __B, __mmask8 *__U,
    __mmask8 *__M)
{
  __builtin_ia32_2intersectq512 (__U, __M, (__v8di) __A, (__v8di) __B);
}



#pragma GCC pop_options
# 100 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vp2intersectvlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512vp2intersectvlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512vp2intersect,avx512vl")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_2intersect_epi32 (__m128i __A, __m128i __B, __mmask8 *__U, __mmask8 *__M)
{
  __builtin_ia32_2intersectd128 (__U, __M, (__v4si) __A, (__v4si) __B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_2intersect_epi32 (__m256i __A, __m256i __B, __mmask8 *__U,
    __mmask8 *__M)
{
  __builtin_ia32_2intersectd256 (__U, __M, (__v8si) __A, (__v8si) __B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_2intersect_epi64 (__m128i __A, __m128i __B, __mmask8 *__U, __mmask8 *__M)
{
  __builtin_ia32_2intersectq128 (__U, __M, (__v2di) __A, (__v2di) __B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_2intersect_epi64 (__m256i __A, __m256i __B, __mmask8 *__U,
    __mmask8 *__M)
{
  __builtin_ia32_2intersectq256 (__U, __M, (__v4di) __A, (__v4di) __B);
}



#pragma GCC pop_options
# 102 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/shaintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/shaintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("sha")



extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha1msg1_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_sha1msg1 ((__v4si) __A, (__v4si) __B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha1msg2_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_sha1msg2 ((__v4si) __A, (__v4si) __B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha1nexte_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_sha1nexte ((__v4si) __A, (__v4si) __B);
}


extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha1rnds4_epu32 (__m128i __A, __m128i __B, const int __I)
{
  return (__m128i) __builtin_ia32_sha1rnds4 ((__v4si) __A, (__v4si) __B, __I);
}






extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha256msg1_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_sha256msg1 ((__v4si) __A, (__v4si) __B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha256msg2_epu32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_sha256msg2 ((__v4si) __A, (__v4si) __B);
}

extern __inline __m128i
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_sha256rnds2_epu32 (__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_sha256rnds2 ((__v4si) __A, (__v4si) __B,
            (__v4si) __C);
}



#pragma GCC pop_options
# 104 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/lzcntintrin.h" 1 3 4
# 38 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/lzcntintrin.h" 3 4
extern __inline unsigned short __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__lzcnt16 (unsigned short __X)
{
  return __builtin_ia32_lzcnt_u16 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__lzcnt32 (unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_lzcnt_u32 (unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32 (__X);
}


extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__lzcnt64 (unsigned long long __X)
{
  return __builtin_ia32_lzcnt_u64 (__X);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_lzcnt_u64 (unsigned long long __X)
{
  return __builtin_ia32_lzcnt_u64 (__X);
}
# 106 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/bmiintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/bmiintrin.h" 3 4
extern __inline unsigned short __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__tzcnt_u16 (unsigned short __X)
{
  return __builtin_ia32_tzcnt_u16 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__andn_u32 (unsigned int __X, unsigned int __Y)
{
  return ~__X & __Y;
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__bextr_u32 (unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bextr_u32 (__X, __Y);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_bextr_u32 (unsigned int __X, unsigned int __Y, unsigned __Z)
{
  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsi_u32 (unsigned int __X)
{
  return __X & -__X;
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsi_u32 (unsigned int __X)
{
  return __blsi_u32 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsmsk_u32 (unsigned int __X)
{
  return __X ^ (__X - 1);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsmsk_u32 (unsigned int __X)
{
  return __blsmsk_u32 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsr_u32 (unsigned int __X)
{
  return __X & (__X - 1);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsr_u32 (unsigned int __X)
{
  return __blsr_u32 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__tzcnt_u32 (unsigned int __X)
{
  return __builtin_ia32_tzcnt_u32 (__X);
}

extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_tzcnt_u32 (unsigned int __X)
{
  return __builtin_ia32_tzcnt_u32 (__X);
}



extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__andn_u64 (unsigned long long __X, unsigned long long __Y)
{
  return ~__X & __Y;
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__bextr_u64 (unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bextr_u64 (__X, __Y);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_bextr_u64 (unsigned long long __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsi_u64 (unsigned long long __X)
{
  return __X & -__X;
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsi_u64 (unsigned long long __X)
{
  return __blsi_u64 (__X);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsmsk_u64 (unsigned long long __X)
{
  return __X ^ (__X - 1);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsmsk_u64 (unsigned long long __X)
{
  return __blsmsk_u64 (__X);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__blsr_u64 (unsigned long long __X)
{
  return __X & (__X - 1);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_blsr_u64 (unsigned long long __X)
{
  return __blsr_u64 (__X);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__tzcnt_u64 (unsigned long long __X)
{
  return __builtin_ia32_tzcnt_u64 (__X);
}

extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_tzcnt_u64 (unsigned long long __X)
{
  return __builtin_ia32_tzcnt_u64 (__X);
}
# 108 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/bmi2intrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/bmi2intrin.h" 3 4
extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_bzhi_u32 (unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bzhi_si (__X, __Y);
}

extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_pdep_u32 (unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pdep_si (__X, __Y);
}

extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_pext_u32 (unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pext_si (__X, __Y);
}



extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_bzhi_u64 (unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bzhi_di (__X, __Y);
}

extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_pdep_u64 (unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pdep_di (__X, __Y);
}

extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_pext_u64 (unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pext_di (__X, __Y);
}

extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mulx_u64 (unsigned long long __X, unsigned long long __Y,
    unsigned long long *__P)
{
  unsigned __int128 __res = (unsigned __int128) __X * __Y;
  *__P = (unsigned long long) (__res >> 64);
  return (unsigned long long) __res;
}
# 110 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/fmaintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/fmaintrin.h" 3 4
extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd ((__v2df)__A, (__v2df)__B,
                                           (__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmadd_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256 ((__v4df)__A, (__v4df)__B,
                                              (__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps ((__v4sf)__A, (__v4sf)__B,
                                          (__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmadd_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256 ((__v8sf)__A, (__v8sf)__B,
                                             (__v8sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_sd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsd3 ((__v2df)__A, (__v2df)__B,
                                             (__v2df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmadd_ss (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddss3 ((__v4sf)__A, (__v4sf)__B,
                                            (__v4sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubpd ((__v2df)__A, (__v2df)__B,
                                           (__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmsub_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmsubpd256 ((__v4df)__A, (__v4df)__B,
                                              (__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubps ((__v4sf)__A, (__v4sf)__B,
                                          (__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmsub_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmsubps256 ((__v8sf)__A, (__v8sf)__B,
                                             (__v8sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_sd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubsd3 ((__v2df)__A, (__v2df)__B,
                                            (__v2df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsub_ss (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubss3 ((__v4sf)__A, (__v4sf)__B,
                                           (__v4sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddpd ((__v2df)__A, (__v2df)__B,
         (__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fnmadd_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmaddpd256 ((__v4df)__A, (__v4df)__B,
            (__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddps ((__v4sf)__A, (__v4sf)__B,
        (__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fnmadd_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmaddps256 ((__v8sf)__A, (__v8sf)__B,
           (__v8sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_sd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddsd3 ((__v2df)__A, (__v2df)__B,
          (__v2df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmadd_ss (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddss3 ((__v4sf)__A, (__v4sf)__B,
         (__v4sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubpd ((__v2df)__A, (__v2df)__B,
         (__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fnmsub_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmsubpd256 ((__v4df)__A, (__v4df)__B,
            (__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubps ((__v4sf)__A, (__v4sf)__B,
        (__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fnmsub_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmsubps256 ((__v8sf)__A, (__v8sf)__B,
           (__v8sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_sd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubsd3 ((__v2df)__A, (__v2df)__B,
          (__v2df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fnmsub_ss (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubss3 ((__v4sf)__A, (__v4sf)__B,
         (__v4sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmaddsub_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd ((__v2df)__A, (__v2df)__B,
                                              (__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmaddsub_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256 ((__v4df)__A,
                                                 (__v4df)__B,
                                                 (__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmaddsub_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps ((__v4sf)__A, (__v4sf)__B,
                                             (__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmaddsub_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256 ((__v8sf)__A,
                                                (__v8sf)__B,
                                                (__v8sf)__C);
}

extern __inline __m128d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsubadd_pd (__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd ((__v2df)__A, (__v2df)__B,
                                              -(__v2df)__C);
}

extern __inline __m256d
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmsubadd_pd (__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256 ((__v4df)__A,
                                                 (__v4df)__B,
                                                 -(__v4df)__C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_fmsubadd_ps (__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps ((__v4sf)__A, (__v4sf)__B,
                                             -(__v4sf)__C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_fmsubadd_ps (__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256 ((__v8sf)__A,
                                                (__v8sf)__B,
                                                -(__v8sf)__C);
}
# 112 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/f16cintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/f16cintrin.h" 3 4
extern __inline float __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_cvtsh_ss (unsigned short __S)
{
  __v8hi __H = __extension__ (__v8hi){ (short) __S, 0, 0, 0, 0, 0, 0, 0 };
  __v4sf __A = __builtin_ia32_vcvtph2ps (__H);
  return __builtin_ia32_vec_ext_v4sf (__A, 0);
}

extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtph_ps (__m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps ((__v8hi) __A);
}

extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtph_ps (__m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256 ((__v8hi) __A);
}


extern __inline unsigned short __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_cvtss_sh (float __F, const int __I)
{
  __v4sf __A = __extension__ (__v4sf){ __F, 0, 0, 0 };
  __v8hi __H = __builtin_ia32_vcvtps2ph (__A, __I);
  return (unsigned short) __builtin_ia32_vec_ext_v8hi (__H, 0);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtps_ph (__m128 __A, const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph ((__v4sf) __A, __I);
}

extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtps_ph (__m256 __A, const int __I)
{
  return (__m128i) __builtin_ia32_vcvtps2ph256 ((__v8sf) __A, __I);
}
# 114 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/rtmintrin.h" 1 3 4
# 48 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/rtmintrin.h" 3 4
extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xbegin (void)
{
  return __builtin_ia32_xbegin ();
}





extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xend (void)
{
  __builtin_ia32_xend ();
}




extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xabort (const unsigned int __imm)
{
  __builtin_ia32_xabort (__imm);
}
# 116 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xtestintrin.h" 1 3 4
# 39 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/xtestintrin.h" 3 4
extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_xtest (void)
{
  return __builtin_ia32_xtest ();
}
# 118 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/cetintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/cetintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target ("shstk")




extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_get_ssp (void)
{
  return __builtin_ia32_rdsspq ();
}
# 53 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/cetintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_inc_ssp (unsigned int __B)
{

  __builtin_ia32_incsspq ((unsigned long long) __B);



}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_saveprevssp (void)
{
  __builtin_ia32_saveprevssp ();
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rstorssp (void *__B)
{
  __builtin_ia32_rstorssp (__B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wrssd (unsigned int __B, void *__C)
{
  __builtin_ia32_wrssd (__B, __C);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wrssq (unsigned long long __B, void *__C)
{
  __builtin_ia32_wrssq (__B, __C);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wrussd (unsigned int __B, void *__C)
{
  __builtin_ia32_wrussd (__B, __C);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wrussq (unsigned long long __B, void *__C)
{
  __builtin_ia32_wrussq (__B, __C);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_setssbsy (void)
{
  __builtin_ia32_setssbsy ();
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_clrssbsy (void *__B)
{
  __builtin_ia32_clrssbsy (__B);
}



#pragma GCC pop_options
# 120 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("gfni,sse2")



extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_gf2p8mul_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi((__v16qi) __A,
         (__v16qi) __B);
}


extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_gf2p8affineinv_epi64_epi8 (__m128i __A, __m128i __B, const int __C)
{
  return (__m128i) __builtin_ia32_vgf2p8affineinvqb_v16qi ((__v16qi) __A,
          (__v16qi) __B,
           __C);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_gf2p8affine_epi64_epi8 (__m128i __A, __m128i __B, const int __C)
{
  return (__m128i) __builtin_ia32_vgf2p8affineqb_v16qi ((__v16qi) __A,
       (__v16qi) __B, __C);
}
# 73 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("gfni,avx")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_gf2p8mul_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi ((__v32qi) __A,
          (__v32qi) __B);
}


extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_gf2p8affineinv_epi64_epi8 (__m256i __A, __m256i __B, const int __C)
{
  return (__m256i) __builtin_ia32_vgf2p8affineinvqb_v32qi ((__v32qi) __A,
          (__v32qi) __B,
           __C);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_gf2p8affine_epi64_epi8 (__m256i __A, __m256i __B, const int __C)
{
  return (__m256i) __builtin_ia32_vgf2p8affineqb_v32qi ((__v32qi) __A,
       (__v32qi) __B, __C);
}
# 119 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("gfni,avx512vl")



extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_gf2p8mul_epi8 (__m128i __A, __mmask16 __B, __m128i __C, __m128i __D)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi_mask ((__v16qi) __C,
        (__v16qi) __D,
        (__v16qi)__A, __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_gf2p8mul_epi8 (__mmask16 __A, __m128i __B, __m128i __C)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi_mask ((__v16qi) __B,
   (__v16qi) __C, (__v16qi) _mm_setzero_si128 (), __A);
}


extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_gf2p8affineinv_epi64_epi8 (__m128i __A, __mmask16 __B, __m128i __C,
        __m128i __D, const int __E)
{
  return (__m128i) __builtin_ia32_vgf2p8affineinvqb_v16qi_mask ((__v16qi) __C,
        (__v16qi) __D,
         __E,
        (__v16qi)__A,
         __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_gf2p8affineinv_epi64_epi8 (__mmask16 __A, __m128i __B, __m128i __C,
         const int __D)
{
  return (__m128i) __builtin_ia32_vgf2p8affineinvqb_v16qi_mask ((__v16qi) __B,
      (__v16qi) __C, __D,
      (__v16qi) _mm_setzero_si128 (),
       __A);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_gf2p8affine_epi64_epi8 (__m128i __A, __mmask16 __B, __m128i __C,
     __m128i __D, const int __E)
{
  return (__m128i) __builtin_ia32_vgf2p8affineqb_v16qi_mask ((__v16qi) __C,
     (__v16qi) __D, __E, (__v16qi)__A, __B);
}

extern __inline __m128i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_gf2p8affine_epi64_epi8 (__mmask16 __A, __m128i __B, __m128i __C,
      const int __D)
{
  return (__m128i) __builtin_ia32_vgf2p8affineqb_v16qi_mask ((__v16qi) __B,
       (__v16qi) __C, __D, (__v16qi) _mm_setzero_si128 (), __A);
}
# 207 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("gfni,avx512vl,avx512bw")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_gf2p8mul_epi8 (__m256i __A, __mmask32 __B, __m256i __C,
      __m256i __D)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi_mask ((__v32qi) __C,
        (__v32qi) __D,
        (__v32qi)__A, __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_gf2p8mul_epi8 (__mmask32 __A, __m256i __B, __m256i __C)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi_mask ((__v32qi) __B,
   (__v32qi) __C, (__v32qi) _mm256_setzero_si256 (), __A);
}


extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_gf2p8affineinv_epi64_epi8 (__m256i __A, __mmask32 __B,
           __m256i __C, __m256i __D, const int __E)
{
  return (__m256i) __builtin_ia32_vgf2p8affineinvqb_v32qi_mask ((__v32qi) __C,
        (__v32qi) __D,
          __E,
        (__v32qi)__A,
         __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_gf2p8affineinv_epi64_epi8 (__mmask32 __A, __m256i __B,
     __m256i __C, const int __D)
{
  return (__m256i) __builtin_ia32_vgf2p8affineinvqb_v32qi_mask ((__v32qi) __B,
          (__v32qi) __C, __D,
          (__v32qi) _mm256_setzero_si256 (), __A);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_gf2p8affine_epi64_epi8 (__m256i __A, __mmask32 __B, __m256i __C,
        __m256i __D, const int __E)
{
  return (__m256i) __builtin_ia32_vgf2p8affineqb_v32qi_mask ((__v32qi) __C,
            (__v32qi) __D,
             __E,
            (__v32qi)__A,
             __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_gf2p8affine_epi64_epi8 (__mmask32 __A, __m256i __B,
         __m256i __C, const int __D)
{
  return (__m256i) __builtin_ia32_vgf2p8affineqb_v32qi_mask ((__v32qi) __B,
  (__v32qi) __C, __D, (__v32qi)_mm256_setzero_si256 (), __A);
}
# 297 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("gfni,avx512f,avx512bw")



extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_gf2p8mul_epi8 (__m512i __A, __mmask64 __B, __m512i __C,
      __m512i __D)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi_mask ((__v64qi) __C,
     (__v64qi) __D, (__v64qi)__A, __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_gf2p8mul_epi8 (__mmask64 __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi_mask ((__v64qi) __B,
   (__v64qi) __C, (__v64qi) _mm512_setzero_si512 (), __A);
}
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_gf2p8mul_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi ((__v64qi) __A,
          (__v64qi) __B);
}


extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_gf2p8affineinv_epi64_epi8 (__m512i __A, __mmask64 __B, __m512i __C,
           __m512i __D, const int __E)
{
  return (__m512i) __builtin_ia32_vgf2p8affineinvqb_v64qi_mask ((__v64qi) __C,
        (__v64qi) __D,
         __E,
        (__v64qi)__A,
         __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_gf2p8affineinv_epi64_epi8 (__mmask64 __A, __m512i __B,
     __m512i __C, const int __D)
{
  return (__m512i) __builtin_ia32_vgf2p8affineinvqb_v64qi_mask ((__v64qi) __B,
    (__v64qi) __C, __D,
    (__v64qi) _mm512_setzero_si512 (), __A);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_gf2p8affineinv_epi64_epi8 (__m512i __A, __m512i __B, const int __C)
{
  return (__m512i) __builtin_ia32_vgf2p8affineinvqb_v64qi ((__v64qi) __A,
          (__v64qi) __B, __C);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_gf2p8affine_epi64_epi8 (__m512i __A, __mmask64 __B, __m512i __C,
        __m512i __D, const int __E)
{
  return (__m512i) __builtin_ia32_vgf2p8affineqb_v64qi_mask ((__v64qi) __C,
     (__v64qi) __D, __E, (__v64qi)__A, __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_gf2p8affine_epi64_epi8 (__mmask64 __A, __m512i __B, __m512i __C,
         const int __D)
{
  return (__m512i) __builtin_ia32_vgf2p8affineqb_v64qi_mask ((__v64qi) __B,
    (__v64qi) __C, __D, (__v64qi) _mm512_setzero_si512 (), __A);
}
extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_gf2p8affine_epi64_epi8 (__m512i __A, __m512i __B, const int __C)
{
  return (__m512i) __builtin_ia32_vgf2p8affineqb_v64qi ((__v64qi) __A,
       (__v64qi) __B, __C);
}
# 411 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/gfniintrin.h" 3 4
#pragma GCC pop_options
# 122 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vaesintrin.h" 1 3 4
# 28 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vaesintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("vaes,avx")



extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_aesdec_epi128 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vaesdec_v32qi ((__v32qi) __A, (__v32qi) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_aesdeclast_epi128 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vaesdeclast_v32qi ((__v32qi) __A,
        (__v32qi) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_aesenc_epi128 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vaesenc_v32qi ((__v32qi) __A, (__v32qi) __B);
}

extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_aesenclast_epi128 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vaesenclast_v32qi ((__v32qi) __A,
        (__v32qi) __B);
}



#pragma GCC pop_options




#pragma GCC push_options
#pragma GCC target("vaes,avx512f")




extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_aesdec_epi128 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vaesdec_v64qi ((__v64qi) __A, (__v64qi) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_aesdeclast_epi128 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vaesdeclast_v64qi ((__v64qi) __A,
          (__v64qi) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_aesenc_epi128 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vaesenc_v64qi ((__v64qi) __A, (__v64qi) __B);
}

extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_aesenclast_epi128 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vaesenclast_v64qi ((__v64qi) __A,
          (__v64qi) __B);
}



#pragma GCC pop_options
# 124 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vpclmulqdqintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vpclmulqdqintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("vpclmulqdq,avx512f")




extern __inline __m512i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_clmulepi64_epi128 (__m512i __A, __m512i __B, const int __C)
{
  return (__m512i) __builtin_ia32_vpclmulqdq_v8di ((__v8di)__A,
        (__v8di) __B, __C);
}
# 53 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vpclmulqdqintrin.h" 3 4
#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target("vpclmulqdq,avx")




extern __inline __m256i
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_clmulepi64_epi128 (__m256i __A, __m256i __B, const int __C)
{
  return (__m256i) __builtin_ia32_vpclmulqdq_v4di ((__v4di)__A,
         (__v4di) __B, __C);
}
# 78 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/vpclmulqdqintrin.h" 3 4
#pragma GCC pop_options
# 126 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/movdirintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/movdirintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target ("movdiri")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_directstoreu_u32 (void * __P, unsigned int __A)
{
  __builtin_ia32_directstoreu_u32 ((unsigned int *)__P, __A);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_directstoreu_u64 (void * __P, unsigned long long __A)
{
  __builtin_ia32_directstoreu_u64 ((unsigned long long *)__P, __A);
}




#pragma GCC pop_options



#pragma GCC push_options
#pragma GCC target ("movdir64b")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_movdir64b (void * __P, const void * __Q)
{
  __builtin_ia32_movdir64b (__P, __Q);
}



#pragma GCC pop_options
# 128 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/sgxintrin.h" 1 3 4
# 110 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/sgxintrin.h" 3 4
extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_encls_u32 (const unsigned int __L, size_t __D[])
{
  enum __encls_type
  {
    __SGX_ECREATE = 0x00,
    __SGX_EADD = 0x01,
    __SGX_EINIT = 0x02,
    __SGX_EREMOVE = 0x03,
    __SGX_EDBGRD = 0x04,
    __SGX_EDBGWR = 0x05,
    __SGX_EEXTEND = 0x06,
    __SGX_ELDB = 0x07,
    __SGX_ELDU = 0x08,
    __SGX_EBLOCK = 0x09,
    __SGX_EPA = 0x0A,
    __SGX_EWB = 0x0B,
    __SGX_ETRACK = 0x0C,
    __SGX_EAUG = 0x0D,
    __SGX_EMODPR = 0x0E,
    __SGX_EMODT = 0x0F,
    __SGX_ERDINFO = 0x10,
    __SGX_ETRACKC = 0x11,
    __SGX_ELDBC = 0x12,
    __SGX_ELDUC = 0x13
  };
  enum __encls_type __T = (enum __encls_type)__L;
  unsigned int __R = 0;
  if (!__builtin_constant_p (__T))
    __asm__ __volatile__("encls\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
  else switch (__T)
    {
    case __SGX_ECREATE:
    case __SGX_EADD:
    case __SGX_EDBGWR:
    case __SGX_EEXTEND:
    case __SGX_EPA:
    case __SGX_EMODPR:
    case __SGX_EMODT:
    case __SGX_EAUG:
    case __SGX_ERDINFO:
      __asm__ __volatile__ ("encls\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]), "c" (__D[1]) : "cc");
      break;
    case __SGX_EINIT:
    case __SGX_ELDB:
    case __SGX_ELDU:
    case __SGX_EWB:
    case __SGX_ELDBC:
    case __SGX_ELDUC:
      __asm__ __volatile__("encls\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
      break;
    case __SGX_EREMOVE:
    case __SGX_EBLOCK:
    case __SGX_ETRACK:
    case __SGX_ETRACKC:
      __asm__ __volatile__("encls\n\t" : "=a" (__R) : "a" (__L), "c" (__D[1]) : "cc");
      break;
    case __SGX_EDBGRD:
      __asm__ __volatile__("encls\n\t" : "=a" (__R), "=b" (__D[0]) : "a" (__L), "c" (__D[1]));
      break;
    default:
      __asm__ __volatile__("encls\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
    }
  return __R;
}

extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_enclu_u32 (const unsigned int __L, size_t __D[])
{
  enum __enclu_type
  {
    __SGX_EREPORT = 0x00,
    __SGX_EGETKEY = 0x01,
    __SGX_EENTER = 0x02,
    __SGX_ERESUME = 0x03,
    __SGX_EEXIT = 0x04,
    __SGX_EACCEPT = 0x05,
    __SGX_EMODPE = 0x06,
    __SGX_EACCEPTCOPY = 0x07
  };
  enum __enclu_type __T = (enum __enclu_type) __L;
  unsigned int __R = 0;
  if (!__builtin_constant_p (__T))
    __asm__ __volatile__("enclu\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
  else switch (__T)
    {
    case __SGX_EREPORT:
    case __SGX_EACCEPTCOPY:
      __asm__ __volatile__("enclu\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
      break;
    case __SGX_EGETKEY:
    case __SGX_ERESUME:
    case __SGX_EACCEPT:
    case __SGX_EMODPE:
      __asm__ __volatile__("enclu\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]), "c" (__D[1]) : "cc");
      break;
    case __SGX_EENTER:
      __asm__ __volatile__("enclu\n\t" : "=a" (__R), "=c" (__D[1]) : "a" (__L), "b" (__D[0]), "c" (__D[1]) : "cc");
      break;
    case __SGX_EEXIT:
      __asm__ __volatile__("enclu\n\t" : "=a" (__R), "=c" (__D[1]) : "a" (__L), "b" (__D[0]) : "cc");
      break;
    default:
      __asm__ __volatile__("enclu\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
    }
  return __R;
}

extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_enclv_u32 (const unsigned int __L, size_t __D[])
{
  enum __enclv_type
  {
    __SGX_EDECVIRTCHILD = 0x00,
    __SGX_EINCVIRTCHILD = 0x01,
    __SGX_ESETCONTEXT = 0x02
  };
  unsigned int __R = 0;
  if (!__builtin_constant_p (__L))
    __asm__ __volatile__("enclv\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[0]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
  else switch (__L)
    {
    case __SGX_EDECVIRTCHILD:
    case __SGX_EINCVIRTCHILD:
      __asm__ __volatile__("enclv\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]), "c" (__D[1]) : "cc");
      break;
    case __SGX_ESETCONTEXT:
      __asm__ __volatile__("enclv\n\t" : "=a" (__R) : "a" (__L), "c" (__D[1]), "d" (__D[2]) : "cc");
      break;
    default:
      __asm__ __volatile__("enclv\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[0]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
    }
  return __R;
}
# 130 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pconfigintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pconfigintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("pconfig")
# 49 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pconfigintrin.h" 3 4
extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_pconfig_u32 (const unsigned int __L, size_t __D[])
{
  enum __pconfig_type
  {
    __PCONFIG_KEY_PROGRAM = 0x01,
  };

  unsigned int __R = 0;

  if (!__builtin_constant_p (__L))
    __asm__ __volatile__ ("pconfig\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
  else switch (__L)
    {
    case __PCONFIG_KEY_PROGRAM:
      __asm__ __volatile__ ("pconfig\n\t" : "=a" (__R) : "a" (__L), "b" (__D[0]) : "cc");
      break;
    default:
      __asm__ __volatile__ ("pconfig\n\t" : "=a" (__R), "=b" (__D[0]), "=c" (__D[1]), "=d" (__D[2]) : "a" (__L), "b" (__D[0]), "c" (__D[1]), "d" (__D[2]) : "cc");
    }
  return __R;
}



#pragma GCC pop_options
# 132 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/waitpkgintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/waitpkgintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("waitpkg")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_umonitor (void *__A)
{
  __builtin_ia32_umonitor (__A);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_umwait (unsigned int __A, unsigned long long __B)
{
  return __builtin_ia32_umwait (__A, __B);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_tpause (unsigned int __A, unsigned long long __B)
{
  return __builtin_ia32_tpause (__A, __B);
}



#pragma GCC pop_options
# 134 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/cldemoteintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/cldemoteintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("cldemote")


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_cldemote (void *__A)
{
  __builtin_ia32_cldemote (__A);
}


#pragma GCC pop_options
# 136 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bf16vlintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bf16vlintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512bf16,avx512vl")




typedef short __v16bh __attribute__ ((__vector_size__ (32)));
typedef short __v8bh __attribute__ ((__vector_size__ (16)));



typedef short __m256bh __attribute__ ((__vector_size__ (32), __may_alias__));
typedef short __m128bh __attribute__ ((__vector_size__ (16), __may_alias__));



extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtne2ps_pbh (__m256 __A, __m256 __B)
{
  return (__m256bh)__builtin_ia32_cvtne2ps2bf16_v16hi(__A, __B);
}

extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtne2ps_pbh (__m256bh __A, __mmask16 __B, __m256 __C, __m256 __D)
{
  return (__m256bh)__builtin_ia32_cvtne2ps2bf16_v16hi_mask(__C, __D, __A, __B);
}

extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtne2ps_pbh (__mmask16 __A, __m256 __B, __m256 __C)
{
  return (__m256bh)__builtin_ia32_cvtne2ps2bf16_v16hi_maskz(__B, __C, __A);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtne2ps_pbh (__m128 __A, __m128 __B)
{
  return (__m128bh)__builtin_ia32_cvtne2ps2bf16_v8hi(__A, __B);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtne2ps_pbh (__m128bh __A, __mmask8 __B, __m128 __C, __m128 __D)
{
  return (__m128bh)__builtin_ia32_cvtne2ps2bf16_v8hi_mask(__C, __D, __A, __B);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtne2ps_pbh (__mmask8 __A, __m128 __B, __m128 __C)
{
  return (__m128bh)__builtin_ia32_cvtne2ps2bf16_v8hi_maskz(__B, __C, __A);
}



extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_cvtneps_pbh (__m256 __A)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v8sf(__A);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_cvtneps_pbh (__m128bh __A, __mmask8 __B, __m256 __C)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v8sf_mask(__C, __A, __B);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_cvtneps_pbh (__mmask8 __A, __m256 __B)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v8sf_maskz(__B, __A);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtneps_pbh (__m128 __A)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v4sf(__A);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtneps_pbh (__m128bh __A, __mmask8 __B, __m128 __C)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v4sf_mask(__C, __A, __B);
}

extern __inline __m128bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtneps_pbh (__mmask8 __A, __m128 __B)
{
  return (__m128bh)__builtin_ia32_cvtneps2bf16_v4sf_maskz(__B, __A);
}



extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_dpbf16_ps (__m256 __A, __m256bh __B, __m256bh __C)
{
  return (__m256)__builtin_ia32_dpbf16ps_v8sf(__A, __B, __C);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_mask_dpbf16_ps (__m256 __A, __mmask8 __B, __m256bh __C, __m256bh __D)
{
  return (__m256)__builtin_ia32_dpbf16ps_v8sf_mask(__A, __C, __D, __B);
}

extern __inline __m256
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm256_maskz_dpbf16_ps (__mmask8 __A, __m256 __B, __m256bh __C, __m256bh __D)
{
  return (__m256)__builtin_ia32_dpbf16ps_v8sf_maskz(__B, __C, __D, __A);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_dpbf16_ps (__m128 __A, __m128bh __B, __m128bh __C)
{
  return (__m128)__builtin_ia32_dpbf16ps_v4sf(__A, __B, __C);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_dpbf16_ps (__m128 __A, __mmask8 __B, __m128bh __C, __m128bh __D)
{
  return (__m128)__builtin_ia32_dpbf16ps_v4sf_mask(__A, __C, __D, __B);
}

extern __inline __m128
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_dpbf16_ps (__mmask8 __A, __m128 __B, __m128bh __C, __m128bh __D)
{
  return (__m128)__builtin_ia32_dpbf16ps_v4sf_maskz(__B, __C, __D, __A);
}



#pragma GCC pop_options
# 138 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bf16intrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/avx512bf16intrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("avx512bf16")




typedef short __v32bh __attribute__ ((__vector_size__ (64)));



typedef short __m512bh __attribute__ ((__vector_size__ (64), __may_alias__));



extern __inline __m512bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtne2ps_pbh (__m512 __A, __m512 __B)
{
  return (__m512bh)__builtin_ia32_cvtne2ps2bf16_v32hi(__A, __B);
}

extern __inline __m512bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtne2ps_pbh (__m512bh __A, __mmask32 __B, __m512 __C, __m512 __D)
{
  return (__m512bh)__builtin_ia32_cvtne2ps2bf16_v32hi_mask(__C, __D, __A, __B);
}

extern __inline __m512bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtne2ps_pbh (__mmask32 __A, __m512 __B, __m512 __C)
{
  return (__m512bh)__builtin_ia32_cvtne2ps2bf16_v32hi_maskz(__B, __C, __A);
}



extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtneps_pbh (__m512 __A)
{
  return (__m256bh)__builtin_ia32_cvtneps2bf16_v16sf(__A);
}

extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtneps_pbh (__m256bh __A, __mmask16 __B, __m512 __C)
{
  return (__m256bh)__builtin_ia32_cvtneps2bf16_v16sf_mask(__C, __A, __B);
}

extern __inline __m256bh
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtneps_pbh (__mmask16 __A, __m512 __B)
{
  return (__m256bh)__builtin_ia32_cvtneps2bf16_v16sf_maskz(__B, __A);
}



extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_dpbf16_ps (__m512 __A, __m512bh __B, __m512bh __C)
{
  return (__m512)__builtin_ia32_dpbf16ps_v16sf(__A, __B, __C);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_dpbf16_ps (__m512 __A, __mmask16 __B, __m512bh __C, __m512bh __D)
{
  return (__m512)__builtin_ia32_dpbf16ps_v16sf_mask(__A, __C, __D, __B);
}

extern __inline __m512
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_dpbf16_ps (__mmask16 __A, __m512 __B, __m512bh __C, __m512bh __D)
{
  return (__m512)__builtin_ia32_dpbf16ps_v16sf_maskz(__B, __C, __D, __A);
}



#pragma GCC pop_options
# 140 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/enqcmdintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/enqcmdintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target ("enqcmd")



extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_enqcmd (void * __P, const void * __Q)
{
  return __builtin_ia32_enqcmd (__P, __Q);
}

extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_enqcmds (void * __P, const void * __Q)
{
  return __builtin_ia32_enqcmds (__P, __Q);
}



#pragma GCC pop_options
# 142 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/rdseedintrin.h" 1 3 4
# 38 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/rdseedintrin.h" 3 4
extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdseed16_step (unsigned short *__p)
{
  return __builtin_ia32_rdseed_hi_step (__p);
}

extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdseed32_step (unsigned int *__p)
{
  return __builtin_ia32_rdseed_si_step (__p);
}


extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdseed64_step (unsigned long long *__p)
{
  return __builtin_ia32_rdseed_di_step (__p);
}
# 144 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/prfchwintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/prfchwintrin.h" 3 4
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_m_prefetchw (void *__P)
{
  __builtin_prefetch (__P, 1, 3 );
}
# 146 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/adxintrin.h" 1 3 4
# 31 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/adxintrin.h" 3 4
extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_subborrow_u32 (unsigned char __CF, unsigned int __X,
  unsigned int __Y, unsigned int *__P)
{
  return __builtin_ia32_sbb_u32 (__CF, __X, __Y, __P);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_addcarry_u32 (unsigned char __CF, unsigned int __X,
        unsigned int __Y, unsigned int *__P)
{
  return __builtin_ia32_addcarryx_u32 (__CF, __X, __Y, __P);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_addcarryx_u32 (unsigned char __CF, unsigned int __X,
  unsigned int __Y, unsigned int *__P)
{
  return __builtin_ia32_addcarryx_u32 (__CF, __X, __Y, __P);
}


extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_subborrow_u64 (unsigned char __CF, unsigned long long __X,
  unsigned long long __Y, unsigned long long *__P)
{
  return __builtin_ia32_sbb_u64 (__CF, __X, __Y, __P);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_addcarry_u64 (unsigned char __CF, unsigned long long __X,
        unsigned long long __Y, unsigned long long *__P)
{
  return __builtin_ia32_addcarryx_u64 (__CF, __X, __Y, __P);
}

extern __inline unsigned char
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_addcarryx_u64 (unsigned char __CF, unsigned long long __X,
  unsigned long long __Y, unsigned long long *__P)
{
  return __builtin_ia32_addcarryx_u64 (__CF, __X, __Y, __P);
}
# 148 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/clwbintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/clwbintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("clwb")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_clwb (void *__A)
{
  __builtin_ia32_clwb (__A);
}



#pragma GCC pop_options
# 150 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/clflushoptintrin.h" 1 3 4
# 37 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/clflushoptintrin.h" 3 4
extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_clflushopt (void *__A)
{
  __builtin_ia32_clflushopt (__A);
}
# 152 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/wbnoinvdintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/wbnoinvdintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("wbnoinvd")



extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wbnoinvd (void)
{
  __builtin_ia32_wbnoinvd ();
}



#pragma GCC pop_options
# 154 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pkuintrin.h" 1 3 4
# 32 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/pkuintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("pku")



extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdpkru_u32 (void)
{
  return __builtin_ia32_rdpkru ();
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wrpkru (unsigned int __key)
{
  __builtin_ia32_wrpkru (__key);
}



#pragma GCC pop_options
# 156 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 2 3 4

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_wbinvd (void)
{
  __builtin_ia32_wbinvd ();
}






extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdrand16_step (unsigned short *__P)
{
  return __builtin_ia32_rdrand16_step (__P);
}

extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdrand32_step (unsigned int *__P)
{
  return __builtin_ia32_rdrand32_step (__P);
}






#pragma GCC push_options
#pragma GCC target("rdpid")


extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdpid_u32 (void)
{
  return __builtin_ia32_rdpid ();
}


#pragma GCC pop_options
# 210 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 3 4
extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_readfsbase_u32 (void)
{
  return __builtin_ia32_rdfsbase32 ();
}

extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_readfsbase_u64 (void)
{
  return __builtin_ia32_rdfsbase64 ();
}

extern __inline unsigned int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_readgsbase_u32 (void)
{
  return __builtin_ia32_rdgsbase32 ();
}

extern __inline unsigned long long
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_readgsbase_u64 (void)
{
  return __builtin_ia32_rdgsbase64 ();
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_writefsbase_u32 (unsigned int __B)
{
  __builtin_ia32_wrfsbase32 (__B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_writefsbase_u64 (unsigned long long __B)
{
  __builtin_ia32_wrfsbase64 (__B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_writegsbase_u32 (unsigned int __B)
{
  __builtin_ia32_wrgsbase32 (__B);
}

extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_writegsbase_u64 (unsigned long long __B)
{
  __builtin_ia32_wrgsbase64 (__B);
}
# 275 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 3 4
extern __inline int
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_rdrand64_step (unsigned long long *__P)
{
  return __builtin_ia32_rdrand64_step (__P);
}
# 289 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/immintrin.h" 3 4
#pragma GCC push_options
#pragma GCC target("ptwrite")




extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_ptwrite64 (unsigned long long __B)
{
  __builtin_ia32_ptwrite64 (__B);
}


extern __inline void
__attribute__((__gnu_inline__, __always_inline__, __artificial__))
_ptwrite32 (unsigned __B)
{
  __builtin_ia32_ptwrite32 (__B);
}


#pragma GCC pop_options
# 22 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2

# 1 "keccak4x/align.h" 1
# 24 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "keccak4x/KeccakP-1600-times4-SnP.h" 1
# 22 "keccak4x/KeccakP-1600-times4-SnP.h"
# 1 "keccak4x/SIMD256-config.h" 1
# 23 "keccak4x/KeccakP-1600-times4-SnP.h" 2







# 1 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/stddef.h" 1 3 4
# 1 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stddef.h" 1 3 4
# 24 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stddef.h" 3 4
  __attribute__ ((__dllimport__)) extern unsigned long __attribute__((__cdecl__)) __threadid(void);

  __attribute__ ((__dllimport__)) extern uintptr_t __attribute__((__cdecl__)) __threadhandle(void);
# 424 "C:/msys64/mingw64/x86_64-w64-mingw32/include/stddef.h" 3 4
typedef struct {
  long long __max_align_ll __attribute__((__aligned__(__alignof__(long long))));
  long double __max_align_ld __attribute__((__aligned__(__alignof__(long double))));
} max_align_t;
# 2 "C:/msys64/mingw64/lib/gcc/x86_64-w64-mingw32/10.3.0/include/stddef.h" 2 3 4
# 31 "keccak4x/KeccakP-1600-times4-SnP.h" 2



# 33 "keccak4x/KeccakP-1600-times4-SnP.h"
void KeccakP1600times4_InitializeAll(void *states);


void KeccakP1600times4_AddBytes(void *states, unsigned int instanceIndex, const unsigned char *data, unsigned int offset, unsigned int length);
void KeccakP1600times4_AddLanesAll(void *states, const unsigned char *data, unsigned int laneCount, unsigned int laneOffset);
void KeccakP1600times4_OverwriteBytes(void *states, unsigned int instanceIndex, const unsigned char *data, unsigned int offset, unsigned int length);
void KeccakP1600times4_OverwriteLanesAll(void *states, const unsigned char *data, unsigned int laneCount, unsigned int laneOffset);
void KeccakP1600times4_OverwriteWithZeroes(void *states, unsigned int instanceIndex, unsigned int byteCount);
void KeccakP1600times4_PermuteAll_12rounds(void *states);
void KeccakP1600times4_PermuteAll_24rounds(void *states);
void KeccakP1600times4_ExtractBytes(const void *states, unsigned int instanceIndex, unsigned char *data, unsigned int offset, unsigned int length);
void KeccakP1600times4_ExtractLanesAll(const void *states, unsigned char *data, unsigned int laneCount, unsigned int laneOffset);
void KeccakP1600times4_ExtractAndAddBytes(const void *states, unsigned int instanceIndex, const unsigned char *input, unsigned char *output, unsigned int offset, unsigned int length);
void KeccakP1600times4_ExtractAndAddLanesAll(const void *states, const unsigned char *input, unsigned char *output, unsigned int laneCount, unsigned int laneOffset);
size_t KeccakF1600times4_FastLoop_Absorb(void *states, unsigned int laneCount, unsigned int laneOffsetParallel, unsigned int laneOffsetSerial, const unsigned char *data, size_t dataByteLen);
size_t KeccakP1600times4_12rounds_FastLoop_Absorb(void *states, unsigned int laneCount, unsigned int laneOffsetParallel, unsigned int laneOffsetSerial, const unsigned char *data, size_t dataByteLen);
# 25 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2
# 1 "keccak4x/SIMD256-config.h" 1
# 26 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2

# 1 "keccak4x/brg_endian.h" 1
# 28 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2




typedef unsigned char UINT8;
typedef unsigned long long int UINT64;
typedef __m128i V128;
typedef __m256i V256;
# 49 "keccak4x/KeccakP-1600-times4-SIMD256.c"
static const UINT64 rho8[4] = {0x0605040302010007, 0x0E0D0C0B0A09080F, 0x1615141312111017, 0x1E1D1C1B1A19181F};
static const UINT64 rho56[4] = {0x0007060504030201, 0x080F0E0D0C0B0A09, 0x1017161514131211, 0x181F1E1D1C1B1A19};
# 83 "keccak4x/KeccakP-1600-times4-SIMD256.c"
void KeccakP1600times4_InitializeAll(void *states)
{
    memset(states, 0, 800);
}

void KeccakP1600times4_AddBytes(void *states, unsigned int instanceIndex, const unsigned char *data, unsigned int offset, unsigned int length)
{
    unsigned int sizeLeft = length;
    unsigned int lanePosition = offset/8;
    unsigned int offsetInLane = offset%8;
    const unsigned char *curData = data;
    UINT64 *statesAsLanes = (UINT64 *)states;

    if ((sizeLeft > 0) && (offsetInLane != 0)) {
        unsigned int bytesInLane = 8 - offsetInLane;
        UINT64 lane = 0;
        if (bytesInLane > sizeLeft)
            bytesInLane = sizeLeft;
        memcpy((unsigned char*)&lane + offsetInLane, curData, bytesInLane);
        statesAsLanes[((lanePosition)*4 + instanceIndex)] ^= lane;
        sizeLeft -= bytesInLane;
        lanePosition++;
        curData += bytesInLane;
    }

    while(sizeLeft >= 8) {
        UINT64 lane = *((const UINT64*)curData);
        statesAsLanes[((lanePosition)*4 + instanceIndex)] ^= lane;
        sizeLeft -= 8;
        lanePosition++;
        curData += 8;
    }

    if (sizeLeft > 0) {
        UINT64 lane = 0;
        memcpy(&lane, curData, sizeLeft);
        statesAsLanes[((lanePosition)*4 + instanceIndex)] ^= lane;
    }
}

void KeccakP1600times4_AddLanesAll(void *states, const unsigned char *data, unsigned int laneCount, unsigned int laneOffset)
{
    V256 *stateAsLanes = (V256 *)states;
    unsigned int i;
    const UINT64 *curData0 = (const UINT64 *)data;
    const UINT64 *curData1 = (const UINT64 *)(data+laneOffset*8);
    const UINT64 *curData2 = (const UINT64 *)(data+laneOffset*2*8);
    const UINT64 *curData3 = (const UINT64 *)(data+laneOffset*3*8);
    V256 lanes0, lanes1, lanes2, lanes3, lanesL01, lanesL23, lanesH01, lanesH23;
# 145 "keccak4x/KeccakP-1600-times4-SIMD256.c"
    if ( laneCount >= 16 ) {
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[0])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[0])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[0])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[0])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), stateAsLanes[0 +0] = _mm256_xor_si256(stateAsLanes[0 +0], lanes0), stateAsLanes[0 +1] = _mm256_xor_si256(stateAsLanes[0 +1], lanes1), stateAsLanes[0 +2] = _mm256_xor_si256(stateAsLanes[0 +2], lanes2), stateAsLanes[0 +3] = _mm256_xor_si256(stateAsLanes[0 +3], lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[4])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[4])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[4])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[4])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), stateAsLanes[4 +0] = _mm256_xor_si256(stateAsLanes[4 +0], lanes0), stateAsLanes[4 +1] = _mm256_xor_si256(stateAsLanes[4 +1], lanes1), stateAsLanes[4 +2] = _mm256_xor_si256(stateAsLanes[4 +2], lanes2), stateAsLanes[4 +3] = _mm256_xor_si256(stateAsLanes[4 +3], lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[8])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[8])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[8])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[8])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), stateAsLanes[8 +0] = _mm256_xor_si256(stateAsLanes[8 +0], lanes0), stateAsLanes[8 +1] = _mm256_xor_si256(stateAsLanes[8 +1], lanes1), stateAsLanes[8 +2] = _mm256_xor_si256(stateAsLanes[8 +2], lanes2), stateAsLanes[8 +3] = _mm256_xor_si256(stateAsLanes[8 +3], lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[12])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[12])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[12])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[12])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), stateAsLanes[12 +0] = _mm256_xor_si256(stateAsLanes[12 +0], lanes0), stateAsLanes[12 +1] = _mm256_xor_si256(stateAsLanes[12 +1], lanes1), stateAsLanes[12 +2] = _mm256_xor_si256(stateAsLanes[12 +2], lanes2), stateAsLanes[12 +3] = _mm256_xor_si256(stateAsLanes[12 +3], lanes3);
        if ( laneCount >= 20 ) {
            lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[16])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[16])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[16])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[16])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), stateAsLanes[16 +0] = _mm256_xor_si256(stateAsLanes[16 +0], lanes0), stateAsLanes[16 +1] = _mm256_xor_si256(stateAsLanes[16 +1], lanes1), stateAsLanes[16 +2] = _mm256_xor_si256(stateAsLanes[16 +2], lanes2), stateAsLanes[16 +3] = _mm256_xor_si256(stateAsLanes[16 +3], lanes3);
            for(i=20; i<laneCount; i++)
                stateAsLanes[i] = _mm256_xor_si256(stateAsLanes[i], _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
        }
        else {
            for(i=16; i<laneCount; i++)
                stateAsLanes[i] = _mm256_xor_si256(stateAsLanes[i], _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
        }
    }
    else {
        for(i=0; i<laneCount; i++)
            stateAsLanes[i] = _mm256_xor_si256(stateAsLanes[i], _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
    }


}

void KeccakP1600times4_OverwriteBytes(void *states, unsigned int instanceIndex, const unsigned char *data, unsigned int offset, unsigned int length)
{
    unsigned int sizeLeft = length;
    unsigned int lanePosition = offset/8;
    unsigned int offsetInLane = offset%8;
    const unsigned char *curData = data;
    UINT64 *statesAsLanes = (UINT64 *)states;

    if ((sizeLeft > 0) && (offsetInLane != 0)) {
        unsigned int bytesInLane = 8 - offsetInLane;
        if (bytesInLane > sizeLeft)
            bytesInLane = sizeLeft;
        memcpy( ((unsigned char *)&statesAsLanes[((lanePosition)*4 + instanceIndex)]) + offsetInLane, curData, bytesInLane);
        sizeLeft -= bytesInLane;
        lanePosition++;
        curData += bytesInLane;
    }

    while(sizeLeft >= 8) {
        UINT64 lane = *((const UINT64*)curData);
        statesAsLanes[((lanePosition)*4 + instanceIndex)] = lane;
        sizeLeft -= 8;
        lanePosition++;
        curData += 8;
    }

    if (sizeLeft > 0) {
        memcpy(&statesAsLanes[((lanePosition)*4 + instanceIndex)], curData, sizeLeft);
    }
}

void KeccakP1600times4_OverwriteLanesAll(void *states, const unsigned char *data, unsigned int laneCount, unsigned int laneOffset)
{
    V256 *stateAsLanes = (V256 *)states;
    unsigned int i;
    const UINT64 *curData0 = (const UINT64 *)data;
    const UINT64 *curData1 = (const UINT64 *)(data+laneOffset*8);
    const UINT64 *curData2 = (const UINT64 *)(data+laneOffset*2*8);
    const UINT64 *curData3 = (const UINT64 *)(data+laneOffset*3*8);
    V256 lanes0, lanes1, lanes2, lanes3, lanesL01, lanesL23, lanesH01, lanesH23;
# 221 "keccak4x/KeccakP-1600-times4-SIMD256.c"
    if ( laneCount >= 16 ) {
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[0])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[0])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[0])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[0])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), _mm256_store_si256((V256 *)&(stateAsLanes[0 +0]), lanes0), _mm256_store_si256((V256 *)&(stateAsLanes[0 +1]), lanes1), _mm256_store_si256((V256 *)&(stateAsLanes[0 +2]), lanes2), _mm256_store_si256((V256 *)&(stateAsLanes[0 +3]), lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[4])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[4])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[4])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[4])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), _mm256_store_si256((V256 *)&(stateAsLanes[4 +0]), lanes0), _mm256_store_si256((V256 *)&(stateAsLanes[4 +1]), lanes1), _mm256_store_si256((V256 *)&(stateAsLanes[4 +2]), lanes2), _mm256_store_si256((V256 *)&(stateAsLanes[4 +3]), lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[8])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[8])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[8])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[8])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), _mm256_store_si256((V256 *)&(stateAsLanes[8 +0]), lanes0), _mm256_store_si256((V256 *)&(stateAsLanes[8 +1]), lanes1), _mm256_store_si256((V256 *)&(stateAsLanes[8 +2]), lanes2), _mm256_store_si256((V256 *)&(stateAsLanes[8 +3]), lanes3);
        lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[12])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[12])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[12])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[12])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), _mm256_store_si256((V256 *)&(stateAsLanes[12 +0]), lanes0), _mm256_store_si256((V256 *)&(stateAsLanes[12 +1]), lanes1), _mm256_store_si256((V256 *)&(stateAsLanes[12 +2]), lanes2), _mm256_store_si256((V256 *)&(stateAsLanes[12 +3]), lanes3);
        if ( laneCount >= 20 ) {
            lanes0 = _mm256_loadu_si256((const V256 *)&(curData0[16])), lanes1 = _mm256_loadu_si256((const V256 *)&(curData1[16])), lanes2 = _mm256_loadu_si256((const V256 *)&(curData2[16])), lanes3 = _mm256_loadu_si256((const V256 *)&(curData3[16])), lanesL01 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x20), lanesH01 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x20), lanesL23 = (V256)_mm256_permute2f128_ps((__m256)(lanes0), (__m256)(lanes2), 0x31), lanesH23 = (V256)_mm256_permute2f128_ps((__m256)(lanes1), (__m256)(lanes3), 0x31), lanes0 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x00), lanes1 = (V256)_mm256_shuffle_pd((__m256d)(lanesL01), (__m256d)(lanesH01), 0x0F), lanes2 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x00), lanes3 = (V256)_mm256_shuffle_pd((__m256d)(lanesL23), (__m256d)(lanesH23), 0x0F), _mm256_store_si256((V256 *)&(stateAsLanes[16 +0]), lanes0), _mm256_store_si256((V256 *)&(stateAsLanes[16 +1]), lanes1), _mm256_store_si256((V256 *)&(stateAsLanes[16 +2]), lanes2), _mm256_store_si256((V256 *)&(stateAsLanes[16 +3]), lanes3);
            for(i=20; i<laneCount; i++)
                _mm256_store_si256((V256 *)&(stateAsLanes[i]), _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
        }
        else {
            for(i=16; i<laneCount; i++)
                _mm256_store_si256((V256 *)&(stateAsLanes[i]), _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
        }
    }
    else {
        for(i=0; i<laneCount; i++)
            _mm256_store_si256((V256 *)&(stateAsLanes[i]), _mm256_set_epi64x((UINT64)(curData3[i]), (UINT64)(curData2[i]), (UINT64)(curData1[i]), (UINT64)(curData0[i])));
    }


}

void KeccakP1600times4_OverwriteWithZeroes(void *states, unsigned int instanceIndex, unsigned int byteCount)
{
    unsigned int sizeLeft = byteCount;
    unsigned int lanePosition = 0;
    UINT64 *statesAsLanes = (UINT64 *)states;

    while(sizeLeft >= 8) {
        statesAsLanes[((lanePosition)*4 + instanceIndex)] = 0;
        sizeLeft -= 8;
        lanePosition++;
    }

    if (sizeLeft > 0) {
        memset(&statesAsLanes[((lanePosition)*4 + instanceIndex)], 0, sizeLeft);
    }
}

void KeccakP1600times4_ExtractBytes(const void *states, unsigned int instanceIndex, unsigned char *data, unsigned int offset, unsigned int length)
{
    unsigned int sizeLeft = length;
    unsigned int lanePosition = offset/8;
    unsigned int offsetInLane = offset%8;
    unsigned char *curData = data;
    const UINT64 *statesAsLanes = (const UINT64 *)states;

    if ((sizeLeft > 0) && (offsetInLane != 0)) {
        unsigned int bytesInLane = 8 - offsetInLane;
        if (bytesInLane > sizeLeft)
            bytesInLane = sizeLeft;
        memcpy( curData, ((unsigned char *)&statesAsLanes[((lanePosition)*4 + instanceIndex)]) + offsetInLane, bytesInLane);
        sizeLeft -= bytesInLane;
        lanePosition++;
        curData += bytesInLane;
    }

    while(sizeLeft >= 8) {
        *(UINT64*)curData = statesAsLanes[((lanePosition)*4 + instanceIndex)];
        sizeLeft -= 8;
        lanePosition++;
        curData += 8;
    }

    if (sizeLeft > 0) {
        memcpy( curData, &statesAsLanes[((lanePosition)*4 + instanceIndex)], sizeLeft);
    }
}

void KeccakP1600times4_ExtractLanesAll(const void *states, unsigned char *data, unsigned int laneCount, unsigned int laneOffset)
{
    UINT64 *curData0 = (UINT64 *)data;
    UINT64 *curData1 = (UINT64 *)(data+laneOffset*1*8);
    UINT64 *curData2 = (UINT64 *)(data+laneOffset*2*8);
    UINT64 *curData3 = (UINT64 *)(data+laneOffset*3*8);

    const V256 *stateAsLanes = (const V256 *)states;
    const UINT64 *stateAsLanes64 = (const UINT64*)states;
    V256 lanes0, lanes1, lanes2, lanes3, lanesL01, lanesL23, lanesH01, lanesH23;
    unsigned int i;
# 318 "keccak4x/KeccakP-1600-times4-SIMD256.c"
    if ( laneCount >= 16 ) {
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), _mm256_storeu_si256((V256 *)&(curData0[0]), lanes0), _mm256_storeu_si256((V256 *)&(curData1[0]), lanes1), _mm256_storeu_si256((V256 *)&(curData2[0]), lanes2), _mm256_storeu_si256((V256 *)&(curData3[0]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), _mm256_storeu_si256((V256 *)&(curData0[4]), lanes0), _mm256_storeu_si256((V256 *)&(curData1[4]), lanes1), _mm256_storeu_si256((V256 *)&(curData2[4]), lanes2), _mm256_storeu_si256((V256 *)&(curData3[4]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), _mm256_storeu_si256((V256 *)&(curData0[8]), lanes0), _mm256_storeu_si256((V256 *)&(curData1[8]), lanes1), _mm256_storeu_si256((V256 *)&(curData2[8]), lanes2), _mm256_storeu_si256((V256 *)&(curData3[8]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), _mm256_storeu_si256((V256 *)&(curData0[12]), lanes0), _mm256_storeu_si256((V256 *)&(curData1[12]), lanes1), _mm256_storeu_si256((V256 *)&(curData2[12]), lanes2), _mm256_storeu_si256((V256 *)&(curData3[12]), lanes3);
        if ( laneCount >= 20 ) {
            lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), _mm256_storeu_si256((V256 *)&(curData0[16]), lanes0), _mm256_storeu_si256((V256 *)&(curData1[16]), lanes1), _mm256_storeu_si256((V256 *)&(curData2[16]), lanes2), _mm256_storeu_si256((V256 *)&(curData3[16]), lanes3);
            for(i=20; i<laneCount; i++)
                curData0[i] = stateAsLanes64[4*(i)], curData1[i] = stateAsLanes64[4*(i)+1], curData2[i] = stateAsLanes64[4*(i)+2], curData3[i] = stateAsLanes64[4*(i)+3];
        }
        else {
            for(i=16; i<laneCount; i++)
                curData0[i] = stateAsLanes64[4*(i)], curData1[i] = stateAsLanes64[4*(i)+1], curData2[i] = stateAsLanes64[4*(i)+2], curData3[i] = stateAsLanes64[4*(i)+3];
        }
    }
    else {
        for(i=0; i<laneCount; i++)
            curData0[i] = stateAsLanes64[4*(i)], curData1[i] = stateAsLanes64[4*(i)+1], curData2[i] = stateAsLanes64[4*(i)+2], curData3[i] = stateAsLanes64[4*(i)+3];
    }


}

void KeccakP1600times4_ExtractAndAddBytes(const void *states, unsigned int instanceIndex, const unsigned char *input, unsigned char *output, unsigned int offset, unsigned int length)
{
    unsigned int sizeLeft = length;
    unsigned int lanePosition = offset/8;
    unsigned int offsetInLane = offset%8;
    const unsigned char *curInput = input;
    unsigned char *curOutput = output;
    const UINT64 *statesAsLanes = (const UINT64 *)states;

    if ((sizeLeft > 0) && (offsetInLane != 0)) {
        unsigned int bytesInLane = 8 - offsetInLane;
        UINT64 lane = statesAsLanes[((lanePosition)*4 + instanceIndex)] >> (8 * offsetInLane);
        if (bytesInLane > sizeLeft)
            bytesInLane = sizeLeft;
        sizeLeft -= bytesInLane;
        do {
            *(curOutput++) = *(curInput++) ^ (unsigned char)lane;
            lane >>= 8;
        } while ( --bytesInLane != 0);
        lanePosition++;
    }

    while(sizeLeft >= 8) {
        *((UINT64*)curOutput) = *((UINT64*)curInput) ^ statesAsLanes[((lanePosition)*4 + instanceIndex)];
        sizeLeft -= 8;
        lanePosition++;
        curInput += 8;
        curOutput += 8;
    }

    if (sizeLeft != 0) {
        UINT64 lane = statesAsLanes[((lanePosition)*4 + instanceIndex)];
        do {
            *(curOutput++) = *(curInput++) ^ (unsigned char)lane;
            lane >>= 8;
        } while ( --sizeLeft != 0);
    }
}

void KeccakP1600times4_ExtractAndAddLanesAll(const void *states, const unsigned char *input, unsigned char *output, unsigned int laneCount, unsigned int laneOffset)
{
    const UINT64 *curInput0 = (UINT64 *)input;
    const UINT64 *curInput1 = (UINT64 *)(input+laneOffset*1*8);
    const UINT64 *curInput2 = (UINT64 *)(input+laneOffset*2*8);
    const UINT64 *curInput3 = (UINT64 *)(input+laneOffset*3*8);
    UINT64 *curOutput0 = (UINT64 *)output;
    UINT64 *curOutput1 = (UINT64 *)(output+laneOffset*1*8);
    UINT64 *curOutput2 = (UINT64 *)(output+laneOffset*2*8);
    UINT64 *curOutput3 = (UINT64 *)(output+laneOffset*3*8);

    const V256 *stateAsLanes = (const V256 *)states;
    const UINT64 *stateAsLanes64 = (const UINT64*)states;
    V256 lanes0, lanes1, lanes2, lanes3, lanesL01, lanesL23, lanesH01, lanesH23;
    unsigned int i;
# 421 "keccak4x/KeccakP-1600-times4-SIMD256.c"
    if ( laneCount >= 16 ) {
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[0 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), lanesL01 = _mm256_loadu_si256((const V256 *)&(curInput0[0])), lanesH01 = _mm256_loadu_si256((const V256 *)&(curInput1[0])), lanesL23 = _mm256_loadu_si256((const V256 *)&(curInput2[0])), lanesH23 = _mm256_loadu_si256((const V256 *)&(curInput3[0])), lanes0 = _mm256_xor_si256(lanes0, lanesL01), lanes1 = _mm256_xor_si256(lanes1, lanesH01), lanes2 = _mm256_xor_si256(lanes2, lanesL23), lanes3 = _mm256_xor_si256(lanes3, lanesH23), _mm256_storeu_si256((V256 *)&(curOutput0[0]), lanes0), _mm256_storeu_si256((V256 *)&(curOutput1[0]), lanes1), _mm256_storeu_si256((V256 *)&(curOutput2[0]), lanes2), _mm256_storeu_si256((V256 *)&(curOutput3[0]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[4 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), lanesL01 = _mm256_loadu_si256((const V256 *)&(curInput0[4])), lanesH01 = _mm256_loadu_si256((const V256 *)&(curInput1[4])), lanesL23 = _mm256_loadu_si256((const V256 *)&(curInput2[4])), lanesH23 = _mm256_loadu_si256((const V256 *)&(curInput3[4])), lanes0 = _mm256_xor_si256(lanes0, lanesL01), lanes1 = _mm256_xor_si256(lanes1, lanesH01), lanes2 = _mm256_xor_si256(lanes2, lanesL23), lanes3 = _mm256_xor_si256(lanes3, lanesH23), _mm256_storeu_si256((V256 *)&(curOutput0[4]), lanes0), _mm256_storeu_si256((V256 *)&(curOutput1[4]), lanes1), _mm256_storeu_si256((V256 *)&(curOutput2[4]), lanes2), _mm256_storeu_si256((V256 *)&(curOutput3[4]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[8 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), lanesL01 = _mm256_loadu_si256((const V256 *)&(curInput0[8])), lanesH01 = _mm256_loadu_si256((const V256 *)&(curInput1[8])), lanesL23 = _mm256_loadu_si256((const V256 *)&(curInput2[8])), lanesH23 = _mm256_loadu_si256((const V256 *)&(curInput3[8])), lanes0 = _mm256_xor_si256(lanes0, lanesL01), lanes1 = _mm256_xor_si256(lanes1, lanesH01), lanes2 = _mm256_xor_si256(lanes2, lanesL23), lanes3 = _mm256_xor_si256(lanes3, lanesH23), _mm256_storeu_si256((V256 *)&(curOutput0[8]), lanes0), _mm256_storeu_si256((V256 *)&(curOutput1[8]), lanes1), _mm256_storeu_si256((V256 *)&(curOutput2[8]), lanes2), _mm256_storeu_si256((V256 *)&(curOutput3[8]), lanes3);
        lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[12 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), lanesL01 = _mm256_loadu_si256((const V256 *)&(curInput0[12])), lanesH01 = _mm256_loadu_si256((const V256 *)&(curInput1[12])), lanesL23 = _mm256_loadu_si256((const V256 *)&(curInput2[12])), lanesH23 = _mm256_loadu_si256((const V256 *)&(curInput3[12])), lanes0 = _mm256_xor_si256(lanes0, lanesL01), lanes1 = _mm256_xor_si256(lanes1, lanesH01), lanes2 = _mm256_xor_si256(lanes2, lanesL23), lanes3 = _mm256_xor_si256(lanes3, lanesH23), _mm256_storeu_si256((V256 *)&(curOutput0[12]), lanes0), _mm256_storeu_si256((V256 *)&(curOutput1[12]), lanes1), _mm256_storeu_si256((V256 *)&(curOutput2[12]), lanes2), _mm256_storeu_si256((V256 *)&(curOutput3[12]), lanes3);
        if ( laneCount >= 20 ) {
            lanes0 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +0])), lanes1 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +1])), lanes2 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +2])), lanes3 = _mm256_load_si256((const V256 *)&(stateAsLanes[16 +3])), lanesL01 = _mm256_unpacklo_epi64((lanes0), (lanes1)), lanesH01 = _mm256_unpackhi_epi64((lanes0), (lanes1)), lanesL23 = _mm256_unpacklo_epi64((lanes2), (lanes3)), lanesH23 = _mm256_unpackhi_epi64((lanes2), (lanes3)), lanes0 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x20), lanes2 = (V256)_mm256_permute2f128_ps((__m256)(lanesL01), (__m256)(lanesL23), 0x31), lanes1 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x20), lanes3 = (V256)_mm256_permute2f128_ps((__m256)(lanesH01), (__m256)(lanesH23), 0x31), lanesL01 = _mm256_loadu_si256((const V256 *)&(curInput0[16])), lanesH01 = _mm256_loadu_si256((const V256 *)&(curInput1[16])), lanesL23 = _mm256_loadu_si256((const V256 *)&(curInput2[16])), lanesH23 = _mm256_loadu_si256((const V256 *)&(curInput3[16])), lanes0 = _mm256_xor_si256(lanes0, lanesL01), lanes1 = _mm256_xor_si256(lanes1, lanesH01), lanes2 = _mm256_xor_si256(lanes2, lanesL23), lanes3 = _mm256_xor_si256(lanes3, lanesH23), _mm256_storeu_si256((V256 *)&(curOutput0[16]), lanes0), _mm256_storeu_si256((V256 *)&(curOutput1[16]), lanes1), _mm256_storeu_si256((V256 *)&(curOutput2[16]), lanes2), _mm256_storeu_si256((V256 *)&(curOutput3[16]), lanes3);
            for(i=20; i<laneCount; i++)
                curOutput0[i] = curInput0[i] ^ stateAsLanes64[4*(i)], curOutput1[i] = curInput1[i] ^ stateAsLanes64[4*(i)+1], curOutput2[i] = curInput2[i] ^ stateAsLanes64[4*(i)+2], curOutput3[i] = curInput3[i] ^ stateAsLanes64[4*(i)+3];
        }
        else {
            for(i=16; i<laneCount; i++)
                curOutput0[i] = curInput0[i] ^ stateAsLanes64[4*(i)], curOutput1[i] = curInput1[i] ^ stateAsLanes64[4*(i)+1], curOutput2[i] = curInput2[i] ^ stateAsLanes64[4*(i)+2], curOutput3[i] = curInput3[i] ^ stateAsLanes64[4*(i)+3];
        }
    }
    else {
        for(i=0; i<laneCount; i++)
            curOutput0[i] = curInput0[i] ^ stateAsLanes64[4*(i)], curOutput1[i] = curInput1[i] ^ stateAsLanes64[4*(i)+1], curOutput2[i] = curInput2[i] ^ stateAsLanes64[4*(i)+2], curOutput3[i] = curInput3[i] ^ stateAsLanes64[4*(i)+3];
    }


}
# 688 "keccak4x/KeccakP-1600-times4-SIMD256.c"
static __attribute__ ((aligned(32))) const UINT64 KeccakF1600RoundConstants[24] = {
    0x0000000000000001ULL,
    0x0000000000008082ULL,
    0x800000000000808aULL,
    0x8000000080008000ULL,
    0x000000000000808bULL,
    0x0000000080000001ULL,
    0x8000000080008081ULL,
    0x8000000000008009ULL,
    0x000000000000008aULL,
    0x0000000000000088ULL,
    0x0000000080008009ULL,
    0x000000008000000aULL,
    0x000000008000808bULL,
    0x800000000000008bULL,
    0x8000000000008089ULL,
    0x8000000000008003ULL,
    0x8000000000008002ULL,
    0x8000000000000080ULL,
    0x000000000000800aULL,
    0x800000008000000aULL,
    0x8000000080008081ULL,
    0x8000000000008080ULL,
    0x0000000080000001ULL,
    0x8000000080008008ULL};
# 800 "keccak4x/KeccakP-1600-times4-SIMD256.c"
# 1 "keccak4x/KeccakP-1600-unrolling.macros" 1
# 801 "keccak4x/KeccakP-1600-times4-SIMD256.c" 2

void KeccakP1600times4_PermuteAll_24rounds(void *states)
{
    V256 *statesAsLanes = (V256 *)states;
    V256 Aba, Abe, Abi, Abo, Abu; V256 Aga, Age, Agi, Ago, Agu; V256 Aka, Ake, Aki, Ako, Aku; V256 Ama, Ame, Ami, Amo, Amu; V256 Asa, Ase, Asi, Aso, Asu; V256 Bba, Bbe, Bbi, Bbo, Bbu; V256 Bga, Bge, Bgi, Bgo, Bgu; V256 Bka, Bke, Bki, Bko, Bku; V256 Bma, Bme, Bmi, Bmo, Bmu; V256 Bsa, Bse, Bsi, Bso, Bsu; V256 Ca, Ce, Ci, Co, Cu; V256 Ca1, Ce1, Ci1, Co1, Cu1; V256 Da, De, Di, Do, Du; V256 Eba, Ebe, Ebi, Ebo, Ebu; V256 Ega, Ege, Egi, Ego, Egu; V256 Eka, Eke, Eki, Eko, Eku; V256 Ema, Eme, Emi, Emo, Emu; V256 Esa, Ese, Esi, Eso, Esu;




    Aba = _mm256_load_si256((const V256 *)&(statesAsLanes[ 0])); Abe = _mm256_load_si256((const V256 *)&(statesAsLanes[ 1])); Abi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 2])); Abo = _mm256_load_si256((const V256 *)&(statesAsLanes[ 3])); Abu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 4])); Aga = _mm256_load_si256((const V256 *)&(statesAsLanes[ 5])); Age = _mm256_load_si256((const V256 *)&(statesAsLanes[ 6])); Agi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 7])); Ago = _mm256_load_si256((const V256 *)&(statesAsLanes[ 8])); Agu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 9])); Aka = _mm256_load_si256((const V256 *)&(statesAsLanes[10])); Ake = _mm256_load_si256((const V256 *)&(statesAsLanes[11])); Aki = _mm256_load_si256((const V256 *)&(statesAsLanes[12])); Ako = _mm256_load_si256((const V256 *)&(statesAsLanes[13])); Aku = _mm256_load_si256((const V256 *)&(statesAsLanes[14])); Ama = _mm256_load_si256((const V256 *)&(statesAsLanes[15])); Ame = _mm256_load_si256((const V256 *)&(statesAsLanes[16])); Ami = _mm256_load_si256((const V256 *)&(statesAsLanes[17])); Amo = _mm256_load_si256((const V256 *)&(statesAsLanes[18])); Amu = _mm256_load_si256((const V256 *)&(statesAsLanes[19])); Asa = _mm256_load_si256((const V256 *)&(statesAsLanes[20])); Ase = _mm256_load_si256((const V256 *)&(statesAsLanes[21])); Asi = _mm256_load_si256((const V256 *)&(statesAsLanes[22])); Aso = _mm256_load_si256((const V256 *)&(statesAsLanes[23])); Asu = _mm256_load_si256((const V256 *)&(statesAsLanes[24]));
    Ca = _mm256_xor_si256(Aba, _mm256_xor_si256(Aga, _mm256_xor_si256(Aka, _mm256_xor_si256(Ama, Asa)))); Ce = _mm256_xor_si256(Abe, _mm256_xor_si256(Age, _mm256_xor_si256(Ake, _mm256_xor_si256(Ame, Ase)))); Ci = _mm256_xor_si256(Abi, _mm256_xor_si256(Agi, _mm256_xor_si256(Aki, _mm256_xor_si256(Ami, Asi)))); Co = _mm256_xor_si256(Abo, _mm256_xor_si256(Ago, _mm256_xor_si256(Ako, _mm256_xor_si256(Amo, Aso)))); Cu = _mm256_xor_si256(Abu, _mm256_xor_si256(Agu, _mm256_xor_si256(Aku, _mm256_xor_si256(Amu, Asu)))); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[0]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[1]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[2]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[3]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[4]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[5]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[6]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[7]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[8]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[9]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[10]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[11]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[12]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[13]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[14]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[15]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[16]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[17]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[18]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[19]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[20]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[21]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[22]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[23]))); Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse));
    _mm256_store_si256((V256 *)&(statesAsLanes[ 0]), Aba); _mm256_store_si256((V256 *)&(statesAsLanes[ 1]), Abe); _mm256_store_si256((V256 *)&(statesAsLanes[ 2]), Abi); _mm256_store_si256((V256 *)&(statesAsLanes[ 3]), Abo); _mm256_store_si256((V256 *)&(statesAsLanes[ 4]), Abu); _mm256_store_si256((V256 *)&(statesAsLanes[ 5]), Aga); _mm256_store_si256((V256 *)&(statesAsLanes[ 6]), Age); _mm256_store_si256((V256 *)&(statesAsLanes[ 7]), Agi); _mm256_store_si256((V256 *)&(statesAsLanes[ 8]), Ago); _mm256_store_si256((V256 *)&(statesAsLanes[ 9]), Agu); _mm256_store_si256((V256 *)&(statesAsLanes[10]), Aka); _mm256_store_si256((V256 *)&(statesAsLanes[11]), Ake); _mm256_store_si256((V256 *)&(statesAsLanes[12]), Aki); _mm256_store_si256((V256 *)&(statesAsLanes[13]), Ako); _mm256_store_si256((V256 *)&(statesAsLanes[14]), Aku); _mm256_store_si256((V256 *)&(statesAsLanes[15]), Ama); _mm256_store_si256((V256 *)&(statesAsLanes[16]), Ame); _mm256_store_si256((V256 *)&(statesAsLanes[17]), Ami); _mm256_store_si256((V256 *)&(statesAsLanes[18]), Amo); _mm256_store_si256((V256 *)&(statesAsLanes[19]), Amu); _mm256_store_si256((V256 *)&(statesAsLanes[20]), Asa); _mm256_store_si256((V256 *)&(statesAsLanes[21]), Ase); _mm256_store_si256((V256 *)&(statesAsLanes[22]), Asi); _mm256_store_si256((V256 *)&(statesAsLanes[23]), Aso); _mm256_store_si256((V256 *)&(statesAsLanes[24]), Asu);
}

void KeccakP1600times4_PermuteAll_12rounds(void *states)
{
    V256 *statesAsLanes = (V256 *)states;
    V256 Aba, Abe, Abi, Abo, Abu; V256 Aga, Age, Agi, Ago, Agu; V256 Aka, Ake, Aki, Ako, Aku; V256 Ama, Ame, Ami, Amo, Amu; V256 Asa, Ase, Asi, Aso, Asu; V256 Bba, Bbe, Bbi, Bbo, Bbu; V256 Bga, Bge, Bgi, Bgo, Bgu; V256 Bka, Bke, Bki, Bko, Bku; V256 Bma, Bme, Bmi, Bmo, Bmu; V256 Bsa, Bse, Bsi, Bso, Bsu; V256 Ca, Ce, Ci, Co, Cu; V256 Ca1, Ce1, Ci1, Co1, Cu1; V256 Da, De, Di, Do, Du; V256 Eba, Ebe, Ebi, Ebo, Ebu; V256 Ega, Ege, Egi, Ego, Egu; V256 Eka, Eke, Eki, Eko, Eku; V256 Ema, Eme, Emi, Emo, Emu; V256 Esa, Ese, Esi, Eso, Esu;




    Aba = _mm256_load_si256((const V256 *)&(statesAsLanes[ 0])); Abe = _mm256_load_si256((const V256 *)&(statesAsLanes[ 1])); Abi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 2])); Abo = _mm256_load_si256((const V256 *)&(statesAsLanes[ 3])); Abu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 4])); Aga = _mm256_load_si256((const V256 *)&(statesAsLanes[ 5])); Age = _mm256_load_si256((const V256 *)&(statesAsLanes[ 6])); Agi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 7])); Ago = _mm256_load_si256((const V256 *)&(statesAsLanes[ 8])); Agu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 9])); Aka = _mm256_load_si256((const V256 *)&(statesAsLanes[10])); Ake = _mm256_load_si256((const V256 *)&(statesAsLanes[11])); Aki = _mm256_load_si256((const V256 *)&(statesAsLanes[12])); Ako = _mm256_load_si256((const V256 *)&(statesAsLanes[13])); Aku = _mm256_load_si256((const V256 *)&(statesAsLanes[14])); Ama = _mm256_load_si256((const V256 *)&(statesAsLanes[15])); Ame = _mm256_load_si256((const V256 *)&(statesAsLanes[16])); Ami = _mm256_load_si256((const V256 *)&(statesAsLanes[17])); Amo = _mm256_load_si256((const V256 *)&(statesAsLanes[18])); Amu = _mm256_load_si256((const V256 *)&(statesAsLanes[19])); Asa = _mm256_load_si256((const V256 *)&(statesAsLanes[20])); Ase = _mm256_load_si256((const V256 *)&(statesAsLanes[21])); Asi = _mm256_load_si256((const V256 *)&(statesAsLanes[22])); Aso = _mm256_load_si256((const V256 *)&(statesAsLanes[23])); Asu = _mm256_load_si256((const V256 *)&(statesAsLanes[24]));
    Ca = _mm256_xor_si256(Aba, _mm256_xor_si256(Aga, _mm256_xor_si256(Aka, _mm256_xor_si256(Ama, Asa)))); Ce = _mm256_xor_si256(Abe, _mm256_xor_si256(Age, _mm256_xor_si256(Ake, _mm256_xor_si256(Ame, Ase)))); Ci = _mm256_xor_si256(Abi, _mm256_xor_si256(Agi, _mm256_xor_si256(Aki, _mm256_xor_si256(Ami, Asi)))); Co = _mm256_xor_si256(Abo, _mm256_xor_si256(Ago, _mm256_xor_si256(Ako, _mm256_xor_si256(Amo, Aso)))); Cu = _mm256_xor_si256(Abu, _mm256_xor_si256(Agu, _mm256_xor_si256(Aku, _mm256_xor_si256(Amu, Asu)))); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[12]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[13]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[14]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[15]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[16]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[17]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[18]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[19]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[20]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[21]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[22]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[23]))); Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse));
    _mm256_store_si256((V256 *)&(statesAsLanes[ 0]), Aba); _mm256_store_si256((V256 *)&(statesAsLanes[ 1]), Abe); _mm256_store_si256((V256 *)&(statesAsLanes[ 2]), Abi); _mm256_store_si256((V256 *)&(statesAsLanes[ 3]), Abo); _mm256_store_si256((V256 *)&(statesAsLanes[ 4]), Abu); _mm256_store_si256((V256 *)&(statesAsLanes[ 5]), Aga); _mm256_store_si256((V256 *)&(statesAsLanes[ 6]), Age); _mm256_store_si256((V256 *)&(statesAsLanes[ 7]), Agi); _mm256_store_si256((V256 *)&(statesAsLanes[ 8]), Ago); _mm256_store_si256((V256 *)&(statesAsLanes[ 9]), Agu); _mm256_store_si256((V256 *)&(statesAsLanes[10]), Aka); _mm256_store_si256((V256 *)&(statesAsLanes[11]), Ake); _mm256_store_si256((V256 *)&(statesAsLanes[12]), Aki); _mm256_store_si256((V256 *)&(statesAsLanes[13]), Ako); _mm256_store_si256((V256 *)&(statesAsLanes[14]), Aku); _mm256_store_si256((V256 *)&(statesAsLanes[15]), Ama); _mm256_store_si256((V256 *)&(statesAsLanes[16]), Ame); _mm256_store_si256((V256 *)&(statesAsLanes[17]), Ami); _mm256_store_si256((V256 *)&(statesAsLanes[18]), Amo); _mm256_store_si256((V256 *)&(statesAsLanes[19]), Amu); _mm256_store_si256((V256 *)&(statesAsLanes[20]), Asa); _mm256_store_si256((V256 *)&(statesAsLanes[21]), Ase); _mm256_store_si256((V256 *)&(statesAsLanes[22]), Asi); _mm256_store_si256((V256 *)&(statesAsLanes[23]), Aso); _mm256_store_si256((V256 *)&(statesAsLanes[24]), Asu);
}

size_t KeccakF1600times4_FastLoop_Absorb(void *states, unsigned int laneCount, unsigned int laneOffsetParallel, unsigned int laneOffsetSerial, const unsigned char *data, size_t dataByteLen)
{
    if (laneCount == 21) {
# 871 "keccak4x/KeccakP-1600-times4-SIMD256.c"
        const unsigned char *dataStart = data;
        const UINT64 *curData0 = (const UINT64 *)data;
        const UINT64 *curData1 = (const UINT64 *)(data+laneOffsetParallel*1*8);
        const UINT64 *curData2 = (const UINT64 *)(data+laneOffsetParallel*2*8);
        const UINT64 *curData3 = (const UINT64 *)(data+laneOffsetParallel*3*8);
        V256 *statesAsLanes = (V256 *)states;
        V256 Aba, Abe, Abi, Abo, Abu; V256 Aga, Age, Agi, Ago, Agu; V256 Aka, Ake, Aki, Ako, Aku; V256 Ama, Ame, Ami, Amo, Amu; V256 Asa, Ase, Asi, Aso, Asu; V256 Bba, Bbe, Bbi, Bbo, Bbu; V256 Bga, Bge, Bgi, Bgo, Bgu; V256 Bka, Bke, Bki, Bko, Bku; V256 Bma, Bme, Bmi, Bmo, Bmu; V256 Bsa, Bse, Bsi, Bso, Bsu; V256 Ca, Ce, Ci, Co, Cu; V256 Ca1, Ce1, Ci1, Co1, Cu1; V256 Da, De, Di, Do, Du; V256 Eba, Ebe, Ebi, Ebo, Ebu; V256 Ega, Ege, Egi, Ego, Egu; V256 Eka, Eke, Eki, Eko, Eku; V256 Ema, Eme, Emi, Emo, Emu; V256 Esa, Ese, Esi, Eso, Esu;

        Aba = _mm256_load_si256((const V256 *)&(statesAsLanes[ 0])); Abe = _mm256_load_si256((const V256 *)&(statesAsLanes[ 1])); Abi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 2])); Abo = _mm256_load_si256((const V256 *)&(statesAsLanes[ 3])); Abu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 4])); Aga = _mm256_load_si256((const V256 *)&(statesAsLanes[ 5])); Age = _mm256_load_si256((const V256 *)&(statesAsLanes[ 6])); Agi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 7])); Ago = _mm256_load_si256((const V256 *)&(statesAsLanes[ 8])); Agu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 9])); Aka = _mm256_load_si256((const V256 *)&(statesAsLanes[10])); Ake = _mm256_load_si256((const V256 *)&(statesAsLanes[11])); Aki = _mm256_load_si256((const V256 *)&(statesAsLanes[12])); Ako = _mm256_load_si256((const V256 *)&(statesAsLanes[13])); Aku = _mm256_load_si256((const V256 *)&(statesAsLanes[14])); Ama = _mm256_load_si256((const V256 *)&(statesAsLanes[15])); Ame = _mm256_load_si256((const V256 *)&(statesAsLanes[16])); Ami = _mm256_load_si256((const V256 *)&(statesAsLanes[17])); Amo = _mm256_load_si256((const V256 *)&(statesAsLanes[18])); Amu = _mm256_load_si256((const V256 *)&(statesAsLanes[19])); Asa = _mm256_load_si256((const V256 *)&(statesAsLanes[20])); Ase = _mm256_load_si256((const V256 *)&(statesAsLanes[21])); Asi = _mm256_load_si256((const V256 *)&(statesAsLanes[22])); Aso = _mm256_load_si256((const V256 *)&(statesAsLanes[23])); Asu = _mm256_load_si256((const V256 *)&(statesAsLanes[24]));
        while(dataByteLen >= (laneOffsetParallel*3 + laneCount)*8) {


            Aba = _mm256_xor_si256(Aba, _mm256_set_epi64x((UINT64)(curData3[0]), (UINT64)(curData2[0]), (UINT64)(curData1[0]), (UINT64)(curData0[0])));
            Abe = _mm256_xor_si256(Abe, _mm256_set_epi64x((UINT64)(curData3[1]), (UINT64)(curData2[1]), (UINT64)(curData1[1]), (UINT64)(curData0[1])));
            Abi = _mm256_xor_si256(Abi, _mm256_set_epi64x((UINT64)(curData3[2]), (UINT64)(curData2[2]), (UINT64)(curData1[2]), (UINT64)(curData0[2])));
            Abo = _mm256_xor_si256(Abo, _mm256_set_epi64x((UINT64)(curData3[3]), (UINT64)(curData2[3]), (UINT64)(curData1[3]), (UINT64)(curData0[3])));
            Abu = _mm256_xor_si256(Abu, _mm256_set_epi64x((UINT64)(curData3[4]), (UINT64)(curData2[4]), (UINT64)(curData1[4]), (UINT64)(curData0[4])));
            Aga = _mm256_xor_si256(Aga, _mm256_set_epi64x((UINT64)(curData3[5]), (UINT64)(curData2[5]), (UINT64)(curData1[5]), (UINT64)(curData0[5])));
            Age = _mm256_xor_si256(Age, _mm256_set_epi64x((UINT64)(curData3[6]), (UINT64)(curData2[6]), (UINT64)(curData1[6]), (UINT64)(curData0[6])));
            Agi = _mm256_xor_si256(Agi, _mm256_set_epi64x((UINT64)(curData3[7]), (UINT64)(curData2[7]), (UINT64)(curData1[7]), (UINT64)(curData0[7])));
            Ago = _mm256_xor_si256(Ago, _mm256_set_epi64x((UINT64)(curData3[8]), (UINT64)(curData2[8]), (UINT64)(curData1[8]), (UINT64)(curData0[8])));
            Agu = _mm256_xor_si256(Agu, _mm256_set_epi64x((UINT64)(curData3[9]), (UINT64)(curData2[9]), (UINT64)(curData1[9]), (UINT64)(curData0[9])));
            Aka = _mm256_xor_si256(Aka, _mm256_set_epi64x((UINT64)(curData3[10]), (UINT64)(curData2[10]), (UINT64)(curData1[10]), (UINT64)(curData0[10])));
            Ake = _mm256_xor_si256(Ake, _mm256_set_epi64x((UINT64)(curData3[11]), (UINT64)(curData2[11]), (UINT64)(curData1[11]), (UINT64)(curData0[11])));
            Aki = _mm256_xor_si256(Aki, _mm256_set_epi64x((UINT64)(curData3[12]), (UINT64)(curData2[12]), (UINT64)(curData1[12]), (UINT64)(curData0[12])));
            Ako = _mm256_xor_si256(Ako, _mm256_set_epi64x((UINT64)(curData3[13]), (UINT64)(curData2[13]), (UINT64)(curData1[13]), (UINT64)(curData0[13])));
            Aku = _mm256_xor_si256(Aku, _mm256_set_epi64x((UINT64)(curData3[14]), (UINT64)(curData2[14]), (UINT64)(curData1[14]), (UINT64)(curData0[14])));
            Ama = _mm256_xor_si256(Ama, _mm256_set_epi64x((UINT64)(curData3[15]), (UINT64)(curData2[15]), (UINT64)(curData1[15]), (UINT64)(curData0[15])));
            Ame = _mm256_xor_si256(Ame, _mm256_set_epi64x((UINT64)(curData3[16]), (UINT64)(curData2[16]), (UINT64)(curData1[16]), (UINT64)(curData0[16])));
            Ami = _mm256_xor_si256(Ami, _mm256_set_epi64x((UINT64)(curData3[17]), (UINT64)(curData2[17]), (UINT64)(curData1[17]), (UINT64)(curData0[17])));
            Amo = _mm256_xor_si256(Amo, _mm256_set_epi64x((UINT64)(curData3[18]), (UINT64)(curData2[18]), (UINT64)(curData1[18]), (UINT64)(curData0[18])));
            Amu = _mm256_xor_si256(Amu, _mm256_set_epi64x((UINT64)(curData3[19]), (UINT64)(curData2[19]), (UINT64)(curData1[19]), (UINT64)(curData0[19])));
            Asa = _mm256_xor_si256(Asa, _mm256_set_epi64x((UINT64)(curData3[20]), (UINT64)(curData2[20]), (UINT64)(curData1[20]), (UINT64)(curData0[20])));

            Ca = _mm256_xor_si256(Aba, _mm256_xor_si256(Aga, _mm256_xor_si256(Aka, _mm256_xor_si256(Ama, Asa)))); Ce = _mm256_xor_si256(Abe, _mm256_xor_si256(Age, _mm256_xor_si256(Ake, _mm256_xor_si256(Ame, Ase)))); Ci = _mm256_xor_si256(Abi, _mm256_xor_si256(Agi, _mm256_xor_si256(Aki, _mm256_xor_si256(Ami, Asi)))); Co = _mm256_xor_si256(Abo, _mm256_xor_si256(Ago, _mm256_xor_si256(Ako, _mm256_xor_si256(Amo, Aso)))); Cu = _mm256_xor_si256(Abu, _mm256_xor_si256(Agu, _mm256_xor_si256(Aku, _mm256_xor_si256(Amu, Asu)))); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[0]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[1]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[2]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[3]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[4]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[5]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[6]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[7]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[8]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[9]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[10]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[11]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[12]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[13]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[14]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[15]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[16]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[17]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[18]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[19]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[20]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[21]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[22]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[23]))); Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse));
            curData0 += laneOffsetSerial;
            curData1 += laneOffsetSerial;
            curData2 += laneOffsetSerial;
            curData3 += laneOffsetSerial;
            dataByteLen -= laneOffsetSerial*8;
        }
        _mm256_store_si256((V256 *)&(statesAsLanes[ 0]), Aba); _mm256_store_si256((V256 *)&(statesAsLanes[ 1]), Abe); _mm256_store_si256((V256 *)&(statesAsLanes[ 2]), Abi); _mm256_store_si256((V256 *)&(statesAsLanes[ 3]), Abo); _mm256_store_si256((V256 *)&(statesAsLanes[ 4]), Abu); _mm256_store_si256((V256 *)&(statesAsLanes[ 5]), Aga); _mm256_store_si256((V256 *)&(statesAsLanes[ 6]), Age); _mm256_store_si256((V256 *)&(statesAsLanes[ 7]), Agi); _mm256_store_si256((V256 *)&(statesAsLanes[ 8]), Ago); _mm256_store_si256((V256 *)&(statesAsLanes[ 9]), Agu); _mm256_store_si256((V256 *)&(statesAsLanes[10]), Aka); _mm256_store_si256((V256 *)&(statesAsLanes[11]), Ake); _mm256_store_si256((V256 *)&(statesAsLanes[12]), Aki); _mm256_store_si256((V256 *)&(statesAsLanes[13]), Ako); _mm256_store_si256((V256 *)&(statesAsLanes[14]), Aku); _mm256_store_si256((V256 *)&(statesAsLanes[15]), Ama); _mm256_store_si256((V256 *)&(statesAsLanes[16]), Ame); _mm256_store_si256((V256 *)&(statesAsLanes[17]), Ami); _mm256_store_si256((V256 *)&(statesAsLanes[18]), Amo); _mm256_store_si256((V256 *)&(statesAsLanes[19]), Amu); _mm256_store_si256((V256 *)&(statesAsLanes[20]), Asa); _mm256_store_si256((V256 *)&(statesAsLanes[21]), Ase); _mm256_store_si256((V256 *)&(statesAsLanes[22]), Asi); _mm256_store_si256((V256 *)&(statesAsLanes[23]), Aso); _mm256_store_si256((V256 *)&(statesAsLanes[24]), Asu);
        return (const unsigned char *)curData0 - dataStart;

    }
    else {

        const unsigned char *dataStart = data;

        while(dataByteLen >= (laneOffsetParallel*3 + laneCount)*8) {
            KeccakP1600times4_AddLanesAll(states, data, laneCount, laneOffsetParallel);
            KeccakP1600times4_PermuteAll_24rounds(states);
            data += laneOffsetSerial*8;
            dataByteLen -= laneOffsetSerial*8;
        }
        return data - dataStart;
    }
}

size_t KeccakP1600times4_12rounds_FastLoop_Absorb(void *states, unsigned int laneCount, unsigned int laneOffsetParallel, unsigned int laneOffsetSerial, const unsigned char *data, size_t dataByteLen)
{
    if (laneCount == 21) {
# 973 "keccak4x/KeccakP-1600-times4-SIMD256.c"
        const unsigned char *dataStart = data;
        const UINT64 *curData0 = (const UINT64 *)data;
        const UINT64 *curData1 = (const UINT64 *)(data+laneOffsetParallel*1*8);
        const UINT64 *curData2 = (const UINT64 *)(data+laneOffsetParallel*2*8);
        const UINT64 *curData3 = (const UINT64 *)(data+laneOffsetParallel*3*8);
        V256 *statesAsLanes = (V256*)states;
        V256 Aba, Abe, Abi, Abo, Abu; V256 Aga, Age, Agi, Ago, Agu; V256 Aka, Ake, Aki, Ako, Aku; V256 Ama, Ame, Ami, Amo, Amu; V256 Asa, Ase, Asi, Aso, Asu; V256 Bba, Bbe, Bbi, Bbo, Bbu; V256 Bga, Bge, Bgi, Bgo, Bgu; V256 Bka, Bke, Bki, Bko, Bku; V256 Bma, Bme, Bmi, Bmo, Bmu; V256 Bsa, Bse, Bsi, Bso, Bsu; V256 Ca, Ce, Ci, Co, Cu; V256 Ca1, Ce1, Ci1, Co1, Cu1; V256 Da, De, Di, Do, Du; V256 Eba, Ebe, Ebi, Ebo, Ebu; V256 Ega, Ege, Egi, Ego, Egu; V256 Eka, Eke, Eki, Eko, Eku; V256 Ema, Eme, Emi, Emo, Emu; V256 Esa, Ese, Esi, Eso, Esu;

        Aba = _mm256_load_si256((const V256 *)&(statesAsLanes[ 0])); Abe = _mm256_load_si256((const V256 *)&(statesAsLanes[ 1])); Abi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 2])); Abo = _mm256_load_si256((const V256 *)&(statesAsLanes[ 3])); Abu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 4])); Aga = _mm256_load_si256((const V256 *)&(statesAsLanes[ 5])); Age = _mm256_load_si256((const V256 *)&(statesAsLanes[ 6])); Agi = _mm256_load_si256((const V256 *)&(statesAsLanes[ 7])); Ago = _mm256_load_si256((const V256 *)&(statesAsLanes[ 8])); Agu = _mm256_load_si256((const V256 *)&(statesAsLanes[ 9])); Aka = _mm256_load_si256((const V256 *)&(statesAsLanes[10])); Ake = _mm256_load_si256((const V256 *)&(statesAsLanes[11])); Aki = _mm256_load_si256((const V256 *)&(statesAsLanes[12])); Ako = _mm256_load_si256((const V256 *)&(statesAsLanes[13])); Aku = _mm256_load_si256((const V256 *)&(statesAsLanes[14])); Ama = _mm256_load_si256((const V256 *)&(statesAsLanes[15])); Ame = _mm256_load_si256((const V256 *)&(statesAsLanes[16])); Ami = _mm256_load_si256((const V256 *)&(statesAsLanes[17])); Amo = _mm256_load_si256((const V256 *)&(statesAsLanes[18])); Amu = _mm256_load_si256((const V256 *)&(statesAsLanes[19])); Asa = _mm256_load_si256((const V256 *)&(statesAsLanes[20])); Ase = _mm256_load_si256((const V256 *)&(statesAsLanes[21])); Asi = _mm256_load_si256((const V256 *)&(statesAsLanes[22])); Aso = _mm256_load_si256((const V256 *)&(statesAsLanes[23])); Asu = _mm256_load_si256((const V256 *)&(statesAsLanes[24]));
        while(dataByteLen >= (laneOffsetParallel*3 + laneCount)*8) {


            Aba = _mm256_xor_si256(Aba, _mm256_set_epi64x((UINT64)(curData3[0]), (UINT64)(curData2[0]), (UINT64)(curData1[0]), (UINT64)(curData0[0])));
            Abe = _mm256_xor_si256(Abe, _mm256_set_epi64x((UINT64)(curData3[1]), (UINT64)(curData2[1]), (UINT64)(curData1[1]), (UINT64)(curData0[1])));
            Abi = _mm256_xor_si256(Abi, _mm256_set_epi64x((UINT64)(curData3[2]), (UINT64)(curData2[2]), (UINT64)(curData1[2]), (UINT64)(curData0[2])));
            Abo = _mm256_xor_si256(Abo, _mm256_set_epi64x((UINT64)(curData3[3]), (UINT64)(curData2[3]), (UINT64)(curData1[3]), (UINT64)(curData0[3])));
            Abu = _mm256_xor_si256(Abu, _mm256_set_epi64x((UINT64)(curData3[4]), (UINT64)(curData2[4]), (UINT64)(curData1[4]), (UINT64)(curData0[4])));
            Aga = _mm256_xor_si256(Aga, _mm256_set_epi64x((UINT64)(curData3[5]), (UINT64)(curData2[5]), (UINT64)(curData1[5]), (UINT64)(curData0[5])));
            Age = _mm256_xor_si256(Age, _mm256_set_epi64x((UINT64)(curData3[6]), (UINT64)(curData2[6]), (UINT64)(curData1[6]), (UINT64)(curData0[6])));
            Agi = _mm256_xor_si256(Agi, _mm256_set_epi64x((UINT64)(curData3[7]), (UINT64)(curData2[7]), (UINT64)(curData1[7]), (UINT64)(curData0[7])));
            Ago = _mm256_xor_si256(Ago, _mm256_set_epi64x((UINT64)(curData3[8]), (UINT64)(curData2[8]), (UINT64)(curData1[8]), (UINT64)(curData0[8])));
            Agu = _mm256_xor_si256(Agu, _mm256_set_epi64x((UINT64)(curData3[9]), (UINT64)(curData2[9]), (UINT64)(curData1[9]), (UINT64)(curData0[9])));
            Aka = _mm256_xor_si256(Aka, _mm256_set_epi64x((UINT64)(curData3[10]), (UINT64)(curData2[10]), (UINT64)(curData1[10]), (UINT64)(curData0[10])));
            Ake = _mm256_xor_si256(Ake, _mm256_set_epi64x((UINT64)(curData3[11]), (UINT64)(curData2[11]), (UINT64)(curData1[11]), (UINT64)(curData0[11])));
            Aki = _mm256_xor_si256(Aki, _mm256_set_epi64x((UINT64)(curData3[12]), (UINT64)(curData2[12]), (UINT64)(curData1[12]), (UINT64)(curData0[12])));
            Ako = _mm256_xor_si256(Ako, _mm256_set_epi64x((UINT64)(curData3[13]), (UINT64)(curData2[13]), (UINT64)(curData1[13]), (UINT64)(curData0[13])));
            Aku = _mm256_xor_si256(Aku, _mm256_set_epi64x((UINT64)(curData3[14]), (UINT64)(curData2[14]), (UINT64)(curData1[14]), (UINT64)(curData0[14])));
            Ama = _mm256_xor_si256(Ama, _mm256_set_epi64x((UINT64)(curData3[15]), (UINT64)(curData2[15]), (UINT64)(curData1[15]), (UINT64)(curData0[15])));
            Ame = _mm256_xor_si256(Ame, _mm256_set_epi64x((UINT64)(curData3[16]), (UINT64)(curData2[16]), (UINT64)(curData1[16]), (UINT64)(curData0[16])));
            Ami = _mm256_xor_si256(Ami, _mm256_set_epi64x((UINT64)(curData3[17]), (UINT64)(curData2[17]), (UINT64)(curData1[17]), (UINT64)(curData0[17])));
            Amo = _mm256_xor_si256(Amo, _mm256_set_epi64x((UINT64)(curData3[18]), (UINT64)(curData2[18]), (UINT64)(curData1[18]), (UINT64)(curData0[18])));
            Amu = _mm256_xor_si256(Amu, _mm256_set_epi64x((UINT64)(curData3[19]), (UINT64)(curData2[19]), (UINT64)(curData1[19]), (UINT64)(curData0[19])));
            Asa = _mm256_xor_si256(Asa, _mm256_set_epi64x((UINT64)(curData3[20]), (UINT64)(curData2[20]), (UINT64)(curData1[20]), (UINT64)(curData0[20])));

            Ca = _mm256_xor_si256(Aba, _mm256_xor_si256(Aga, _mm256_xor_si256(Aka, _mm256_xor_si256(Ama, Asa)))); Ce = _mm256_xor_si256(Abe, _mm256_xor_si256(Age, _mm256_xor_si256(Ake, _mm256_xor_si256(Ame, Ase)))); Ci = _mm256_xor_si256(Abi, _mm256_xor_si256(Agi, _mm256_xor_si256(Aki, _mm256_xor_si256(Ami, Asi)))); Co = _mm256_xor_si256(Abo, _mm256_xor_si256(Ago, _mm256_xor_si256(Ako, _mm256_xor_si256(Amo, Aso)))); Cu = _mm256_xor_si256(Abu, _mm256_xor_si256(Agu, _mm256_xor_si256(Aku, _mm256_xor_si256(Amu, Asu)))); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[12]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[13]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[14]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[15]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[16]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[17]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[18]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[19]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[20]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[21]))); Ca = Aba; Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Abe; Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Abi; Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Abo; Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Abu; Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Aga); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Age); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Agi); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ago); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Agu); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Aka); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Ake); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Aki); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Ako); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Aku); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ama); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Ame); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Ami); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Amo); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Amu); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Asa); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ase); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Asi); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Aso); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Asu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Aba = _mm256_xor_si256(Aba, Da); Bba = Aba; Age = _mm256_xor_si256(Age, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Age, 44), _mm256_srli_epi64(Age, 64-(44))); Aki = _mm256_xor_si256(Aki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Aki, 43), _mm256_srli_epi64(Aki, 64-(43))); Eba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Eba = _mm256_xor_si256(Eba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[22]))); Ca = Eba; Amo = _mm256_xor_si256(Amo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Amo, 21), _mm256_srli_epi64(Amo, 64-(21))); Ebe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Ce = Ebe; Asu = _mm256_xor_si256(Asu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Asu, 14), _mm256_srli_epi64(Asu, 64-(14))); Ebi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Ci = Ebi; Ebo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Co = Ebo; Ebu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Cu = Ebu; Abo = _mm256_xor_si256(Abo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Abo, 28), _mm256_srli_epi64(Abo, 64-(28))); Agu = _mm256_xor_si256(Agu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Agu, 20), _mm256_srli_epi64(Agu, 64-(20))); Aka = _mm256_xor_si256(Aka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Aka, 3), _mm256_srli_epi64(Aka, 64-(3))); Ega = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Ca = _mm256_xor_si256(Ca, Ega); Ame = _mm256_xor_si256(Ame, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Ame, 45), _mm256_srli_epi64(Ame, 64-(45))); Ege = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Ce = _mm256_xor_si256(Ce, Ege); Asi = _mm256_xor_si256(Asi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Asi, 61), _mm256_srli_epi64(Asi, 64-(61))); Egi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ci = _mm256_xor_si256(Ci, Egi); Ego = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Co = _mm256_xor_si256(Co, Ego); Egu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Cu = _mm256_xor_si256(Cu, Egu); Abe = _mm256_xor_si256(Abe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Abe, 1), _mm256_srli_epi64(Abe, 64-(1))); Agi = _mm256_xor_si256(Agi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Agi, 6), _mm256_srli_epi64(Agi, 64-(6))); Ako = _mm256_xor_si256(Ako, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Ako, 25), _mm256_srli_epi64(Ako, 64-(25))); Eka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Ca = _mm256_xor_si256(Ca, Eka); Amu = _mm256_xor_si256(Amu, Du); Bko = _mm256_shuffle_epi8(Amu, _mm256_load_si256((const V256 *)&(rho8))); Eke = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Ce = _mm256_xor_si256(Ce, Eke); Asa = _mm256_xor_si256(Asa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Asa, 18), _mm256_srli_epi64(Asa, 64-(18))); Eki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ci = _mm256_xor_si256(Ci, Eki); Eko = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Co = _mm256_xor_si256(Co, Eko); Eku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Cu = _mm256_xor_si256(Cu, Eku); Abu = _mm256_xor_si256(Abu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Abu, 27), _mm256_srli_epi64(Abu, 64-(27))); Aga = _mm256_xor_si256(Aga, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Aga, 36), _mm256_srli_epi64(Aga, 64-(36))); Ake = _mm256_xor_si256(Ake, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Ake, 10), _mm256_srli_epi64(Ake, 64-(10))); Ema = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Ca = _mm256_xor_si256(Ca, Ema); Ami = _mm256_xor_si256(Ami, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Ami, 15), _mm256_srli_epi64(Ami, 64-(15))); Eme = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Ce = _mm256_xor_si256(Ce, Eme); Aso = _mm256_xor_si256(Aso, Do); Bmu = _mm256_shuffle_epi8(Aso, _mm256_load_si256((const V256 *)&(rho56))); Emi = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Ci = _mm256_xor_si256(Ci, Emi); Emo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Co = _mm256_xor_si256(Co, Emo); Emu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Cu = _mm256_xor_si256(Cu, Emu); Abi = _mm256_xor_si256(Abi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Abi, 62), _mm256_srli_epi64(Abi, 64-(62))); Ago = _mm256_xor_si256(Ago, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ago, 55), _mm256_srli_epi64(Ago, 64-(55))); Aku = _mm256_xor_si256(Aku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Aku, 39), _mm256_srli_epi64(Aku, 64-(39))); Esa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ca = _mm256_xor_si256(Ca, Esa); Ama = _mm256_xor_si256(Ama, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ama, 41), _mm256_srli_epi64(Ama, 64-(41))); Ese = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ce = _mm256_xor_si256(Ce, Ese); Ase = _mm256_xor_si256(Ase, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ase, 2), _mm256_srli_epi64(Ase, 64-(2))); Esi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Ci = _mm256_xor_si256(Ci, Esi); Eso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Co = _mm256_xor_si256(Co, Eso); Esu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse)); Cu = _mm256_xor_si256(Cu, Esu); Ce1 = _mm256_or_si256(_mm256_slli_epi64(Ce, 1), _mm256_srli_epi64(Ce, 64-(1))); Da = _mm256_xor_si256(Cu, Ce1); Ci1 = _mm256_or_si256(_mm256_slli_epi64(Ci, 1), _mm256_srli_epi64(Ci, 64-(1))); De = _mm256_xor_si256(Ca, Ci1); Co1 = _mm256_or_si256(_mm256_slli_epi64(Co, 1), _mm256_srli_epi64(Co, 64-(1))); Di = _mm256_xor_si256(Ce, Co1); Cu1 = _mm256_or_si256(_mm256_slli_epi64(Cu, 1), _mm256_srli_epi64(Cu, 64-(1))); Do = _mm256_xor_si256(Ci, Cu1); Ca1 = _mm256_or_si256(_mm256_slli_epi64(Ca, 1), _mm256_srli_epi64(Ca, 64-(1))); Du = _mm256_xor_si256(Co, Ca1); Eba = _mm256_xor_si256(Eba, Da); Bba = Eba; Ege = _mm256_xor_si256(Ege, De); Bbe = _mm256_or_si256(_mm256_slli_epi64(Ege, 44), _mm256_srli_epi64(Ege, 64-(44))); Eki = _mm256_xor_si256(Eki, Di); Bbi = _mm256_or_si256(_mm256_slli_epi64(Eki, 43), _mm256_srli_epi64(Eki, 64-(43))); Aba = _mm256_xor_si256(Bba, _mm256_andnot_si256(Bbe, Bbi)); Aba = _mm256_xor_si256(Aba, (V256)_mm256_broadcast_sd((const double*)(&KeccakF1600RoundConstants[23]))); Emo = _mm256_xor_si256(Emo, Do); Bbo = _mm256_or_si256(_mm256_slli_epi64(Emo, 21), _mm256_srli_epi64(Emo, 64-(21))); Abe = _mm256_xor_si256(Bbe, _mm256_andnot_si256(Bbi, Bbo)); Esu = _mm256_xor_si256(Esu, Du); Bbu = _mm256_or_si256(_mm256_slli_epi64(Esu, 14), _mm256_srli_epi64(Esu, 64-(14))); Abi = _mm256_xor_si256(Bbi, _mm256_andnot_si256(Bbo, Bbu)); Abo = _mm256_xor_si256(Bbo, _mm256_andnot_si256(Bbu, Bba)); Abu = _mm256_xor_si256(Bbu, _mm256_andnot_si256(Bba, Bbe)); Ebo = _mm256_xor_si256(Ebo, Do); Bga = _mm256_or_si256(_mm256_slli_epi64(Ebo, 28), _mm256_srli_epi64(Ebo, 64-(28))); Egu = _mm256_xor_si256(Egu, Du); Bge = _mm256_or_si256(_mm256_slli_epi64(Egu, 20), _mm256_srli_epi64(Egu, 64-(20))); Eka = _mm256_xor_si256(Eka, Da); Bgi = _mm256_or_si256(_mm256_slli_epi64(Eka, 3), _mm256_srli_epi64(Eka, 64-(3))); Aga = _mm256_xor_si256(Bga, _mm256_andnot_si256(Bge, Bgi)); Eme = _mm256_xor_si256(Eme, De); Bgo = _mm256_or_si256(_mm256_slli_epi64(Eme, 45), _mm256_srli_epi64(Eme, 64-(45))); Age = _mm256_xor_si256(Bge, _mm256_andnot_si256(Bgi, Bgo)); Esi = _mm256_xor_si256(Esi, Di); Bgu = _mm256_or_si256(_mm256_slli_epi64(Esi, 61), _mm256_srli_epi64(Esi, 64-(61))); Agi = _mm256_xor_si256(Bgi, _mm256_andnot_si256(Bgo, Bgu)); Ago = _mm256_xor_si256(Bgo, _mm256_andnot_si256(Bgu, Bga)); Agu = _mm256_xor_si256(Bgu, _mm256_andnot_si256(Bga, Bge)); Ebe = _mm256_xor_si256(Ebe, De); Bka = _mm256_or_si256(_mm256_slli_epi64(Ebe, 1), _mm256_srli_epi64(Ebe, 64-(1))); Egi = _mm256_xor_si256(Egi, Di); Bke = _mm256_or_si256(_mm256_slli_epi64(Egi, 6), _mm256_srli_epi64(Egi, 64-(6))); Eko = _mm256_xor_si256(Eko, Do); Bki = _mm256_or_si256(_mm256_slli_epi64(Eko, 25), _mm256_srli_epi64(Eko, 64-(25))); Aka = _mm256_xor_si256(Bka, _mm256_andnot_si256(Bke, Bki)); Emu = _mm256_xor_si256(Emu, Du); Bko = _mm256_shuffle_epi8(Emu, _mm256_load_si256((const V256 *)&(rho8))); Ake = _mm256_xor_si256(Bke, _mm256_andnot_si256(Bki, Bko)); Esa = _mm256_xor_si256(Esa, Da); Bku = _mm256_or_si256(_mm256_slli_epi64(Esa, 18), _mm256_srli_epi64(Esa, 64-(18))); Aki = _mm256_xor_si256(Bki, _mm256_andnot_si256(Bko, Bku)); Ako = _mm256_xor_si256(Bko, _mm256_andnot_si256(Bku, Bka)); Aku = _mm256_xor_si256(Bku, _mm256_andnot_si256(Bka, Bke)); Ebu = _mm256_xor_si256(Ebu, Du); Bma = _mm256_or_si256(_mm256_slli_epi64(Ebu, 27), _mm256_srli_epi64(Ebu, 64-(27))); Ega = _mm256_xor_si256(Ega, Da); Bme = _mm256_or_si256(_mm256_slli_epi64(Ega, 36), _mm256_srli_epi64(Ega, 64-(36))); Eke = _mm256_xor_si256(Eke, De); Bmi = _mm256_or_si256(_mm256_slli_epi64(Eke, 10), _mm256_srli_epi64(Eke, 64-(10))); Ama = _mm256_xor_si256(Bma, _mm256_andnot_si256(Bme, Bmi)); Emi = _mm256_xor_si256(Emi, Di); Bmo = _mm256_or_si256(_mm256_slli_epi64(Emi, 15), _mm256_srli_epi64(Emi, 64-(15))); Ame = _mm256_xor_si256(Bme, _mm256_andnot_si256(Bmi, Bmo)); Eso = _mm256_xor_si256(Eso, Do); Bmu = _mm256_shuffle_epi8(Eso, _mm256_load_si256((const V256 *)&(rho56))); Ami = _mm256_xor_si256(Bmi, _mm256_andnot_si256(Bmo, Bmu)); Amo = _mm256_xor_si256(Bmo, _mm256_andnot_si256(Bmu, Bma)); Amu = _mm256_xor_si256(Bmu, _mm256_andnot_si256(Bma, Bme)); Ebi = _mm256_xor_si256(Ebi, Di); Bsa = _mm256_or_si256(_mm256_slli_epi64(Ebi, 62), _mm256_srli_epi64(Ebi, 64-(62))); Ego = _mm256_xor_si256(Ego, Do); Bse = _mm256_or_si256(_mm256_slli_epi64(Ego, 55), _mm256_srli_epi64(Ego, 64-(55))); Eku = _mm256_xor_si256(Eku, Du); Bsi = _mm256_or_si256(_mm256_slli_epi64(Eku, 39), _mm256_srli_epi64(Eku, 64-(39))); Asa = _mm256_xor_si256(Bsa, _mm256_andnot_si256(Bse, Bsi)); Ema = _mm256_xor_si256(Ema, Da); Bso = _mm256_or_si256(_mm256_slli_epi64(Ema, 41), _mm256_srli_epi64(Ema, 64-(41))); Ase = _mm256_xor_si256(Bse, _mm256_andnot_si256(Bsi, Bso)); Ese = _mm256_xor_si256(Ese, De); Bsu = _mm256_or_si256(_mm256_slli_epi64(Ese, 2), _mm256_srli_epi64(Ese, 64-(2))); Asi = _mm256_xor_si256(Bsi, _mm256_andnot_si256(Bso, Bsu)); Aso = _mm256_xor_si256(Bso, _mm256_andnot_si256(Bsu, Bsa)); Asu = _mm256_xor_si256(Bsu, _mm256_andnot_si256(Bsa, Bse));
            curData0 += laneOffsetSerial;
            curData1 += laneOffsetSerial;
            curData2 += laneOffsetSerial;
            curData3 += laneOffsetSerial;
            dataByteLen -= laneOffsetSerial*8;
        }
        _mm256_store_si256((V256 *)&(statesAsLanes[ 0]), Aba); _mm256_store_si256((V256 *)&(statesAsLanes[ 1]), Abe); _mm256_store_si256((V256 *)&(statesAsLanes[ 2]), Abi); _mm256_store_si256((V256 *)&(statesAsLanes[ 3]), Abo); _mm256_store_si256((V256 *)&(statesAsLanes[ 4]), Abu); _mm256_store_si256((V256 *)&(statesAsLanes[ 5]), Aga); _mm256_store_si256((V256 *)&(statesAsLanes[ 6]), Age); _mm256_store_si256((V256 *)&(statesAsLanes[ 7]), Agi); _mm256_store_si256((V256 *)&(statesAsLanes[ 8]), Ago); _mm256_store_si256((V256 *)&(statesAsLanes[ 9]), Agu); _mm256_store_si256((V256 *)&(statesAsLanes[10]), Aka); _mm256_store_si256((V256 *)&(statesAsLanes[11]), Ake); _mm256_store_si256((V256 *)&(statesAsLanes[12]), Aki); _mm256_store_si256((V256 *)&(statesAsLanes[13]), Ako); _mm256_store_si256((V256 *)&(statesAsLanes[14]), Aku); _mm256_store_si256((V256 *)&(statesAsLanes[15]), Ama); _mm256_store_si256((V256 *)&(statesAsLanes[16]), Ame); _mm256_store_si256((V256 *)&(statesAsLanes[17]), Ami); _mm256_store_si256((V256 *)&(statesAsLanes[18]), Amo); _mm256_store_si256((V256 *)&(statesAsLanes[19]), Amu); _mm256_store_si256((V256 *)&(statesAsLanes[20]), Asa); _mm256_store_si256((V256 *)&(statesAsLanes[21]), Ase); _mm256_store_si256((V256 *)&(statesAsLanes[22]), Asi); _mm256_store_si256((V256 *)&(statesAsLanes[23]), Aso); _mm256_store_si256((V256 *)&(statesAsLanes[24]), Asu);
        return (const unsigned char *)curData0 - dataStart;

    }
    else {

        const unsigned char *dataStart = data;

        while(dataByteLen >= (laneOffsetParallel*3 + laneCount)*8) {
            KeccakP1600times4_AddLanesAll(states, data, laneCount, laneOffsetParallel);
            KeccakP1600times4_PermuteAll_12rounds(states);
            data += laneOffsetSerial*8;
            dataByteLen -= laneOffsetSerial*8;
        }
        return data - dataStart;
    }
}
